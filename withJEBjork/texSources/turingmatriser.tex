\documentclass{amsart}
\usepackage[applemac]{inputenc}
\addtolength{\hoffset}{-12mm}
\addtolength{\textwidth}{22mm}
\addtolength{\voffset}{-10mm}
\addtolength{\textheight}{20mm}

\def\uuu{_}

\def\vvv{-}

\begin{document}

johan 0761 383940  ring måndag-tisdag ev. onsfad

marika lagercrantz

070 6588598

söndag klockan 11 i bagarmossen


maria högnerg

070 71888012
\newpage


VÄNTA VÄNTA

Flytta detta !!!

\bigskip

\bigskip

\centerline{\bf{Differentialekvationer.}}
\medskip


\noindent
Linjära system av
ordinära differentialekvationer
kan med fördel lösas på med hjälp av 
matrisvärda funktioner.
Till att börja med betraktas en oberoende  $t$-variabel  som 
i de flesta tillämpningar uppfattas
uppfattas som
tidsparameter.
Låt nu $n$ vara ett heltal $\geq 1$.
En reellvärd funktion $y(t)$ definierad på ett begränsat och 
slutet intervall $[0,T]$
är $n$ gånger kontinuerligt deriverbar om
dess derivator upp till
ordning
$n$ existerar och ger kontinuerliga funktioner på $[0,T]$.
Klassen av sådana funktioner betecknas med $C^n[0,T]$, och för varje 
sådan funktion betecknas dess derivator med
\[
 y^{(k)}(t)= \frac{d^ky}{dt^k}\quad 1\leq k\leq b
\]
Givet ett par av positiva  heltal $n$ och $N$
söker man $N$  funktioner $y_1(t),\ldots,y_N(t)$
i $C^n[0,T]$
som tillsammans uppfyller systemet
\[
y_p^{(n)}(t)= \sum_{k=0}^{n-1}\sum_{\mu=1}^{\mu=N}\,
a_{p,\nu,k}(t)\cdot y_\nu^{(k)}(t)\quad\, 1\leq p\leq N\tag{*}
\]
för alla $0\leq t\leq T$ där de trippel-indicerade $a$-funktionerna  är
reellvärda och kontinuerliga på $[0,T]$.
Detta system kan
skrivas om till
ett system av första ordningen, dvs. med $n=1$ där 
antalet $y$-funktioner
ökar från $N$ till $n\cdot N$.
Dessutom kan ett 
system av första ordningen
skrivas på  matrisform
där man
söker lösningar som består av
vektorvärda funktioner
$\mathbf y(t)=(y_1(t),\ldots,y_N(t)$ vilka utgör kolonner hos en tidsberoende \emph{matrisvärd funktion}. 
Den centrala uppgiften är därför att
lösa en differentialekvation
\[
\frac{d\Phi(t)}{dt}=A(t)\cdot \Phi(t)\tag{**}
\]
där 
$M\geq 2$ och $A(t)$ en
en $(M,M)$-matris vars element $\{a_{pq}(t)\}$
är kontinuerliga funktioner på $[0,T]$, varefter man
söker
den tidsberoende
$(M.M)$-matriser
$\Phi(t)$ som löser (*) med startvillkor 
\[
\Phi(0)= E_M\tag{i}
\]
dvs. vid tiden $t=0$ har man enhetsmatrisen.
Tidsderivatan av $\Phi(t)$
som uppträder  i
vänsterledet hos (**) 
ges helt enkelt genom att
ta $t$-derivatan av
matrisens $M^2$ många element $\{\phi_{pq}(t)$.
Så utskrivet  betyder (**) att följande system av första ordningen
är uppfyllt:
\[
\dot \phi_{pq}(t)= \sum_{\nu=1}^{\nu=N}\, a_{p\nu}(t)\cdot \phi_{\nu,q}(t)
\quad\, 1\leq p,q\leq M\tag{ii}
\]
Startvillkoret (i) betyder att
$\phi_{pq}(0)$ är Kroneckers delta-funktion.
med beteckningar enligt ovan kan vi  annonsera satsen nedan som
i sin fulla generalitet bevisades av Emile Picard 1880.
\medskip


\noindent
{\bf{Picards Teorem.}}\emph{Ekvationen (*) har en entydig
lösning
$\Phi(t)$ och  dessutom gäller att}
\[
det\,\Phi(t)=e^{Tr(A(t)}
\quad 0\leq t\leq T
\]
\emph{där $Tr(A(t))$ betecknar spåret hos matrisen $A(t)$}.

\bigskip


\noindent
{\bf{Inhomogena system.}}
Låt $\mathbf Z(t)$ vara en 
tidsberende $(M,M)$-matris, dvs. vars
$M^2$ många element $\{z_{pq}(t)\}$
är kontinuerliga på $[0,T]$.
Nu kan vi definiera den matrisvärda funktionen

\[ 
\Psi(t)= \Phi(t)\cdot \int_0^t\, \Phi(s)^{-1}\cdot \mathbf Z(s)\, ds\tag{1}
\]
Under integraltecknet
har vi alltså multiplicerat
de två matriserna $\Phi^{-1}(s)$ och $\mathbf Z(s)$. Elementen i denna
matrisprodukt är nu kontinuerliga funktioner
som var för sig integreras över intervall $[0,t]$ och
den resulterande matrisen multipliceras sedan med $\Phi(t)$.
Låt oss nu bilda   tidsderivator i vänster- och högerledet.
Vi får då
\[
\dot \Psi(t)=
\dot \Phi(t)\cdot \int_0^t\, \Phi(s)^{-1}\cdot \mathbf Z(s)\, ds
+\Phi(t)\cdot \Phi^{-1}(t)\cdot \mathbf Z(t)=
A(t)\cdot \Psi(t)+\bold Z(t)
\]
Således har vi också funnit en lösning till
ett inhomogent system där 
$\bold Z$-matrisen kan väljas godtyckligt !
\bigskip


\noindent
{\bf{Om startvillkor.}}
Notera att konstruktionen
av $\Psi$-matrisen innebär att
\[
\Psi(0)= \mathbf O_M= \text{nollmatrisen}
\]
Om man önskar en lösning
till den inhomogena ekvationen
\[
\dot \chi(t)= A(t)\cdot \chi(t)+\mathbf Z(t)
\] 
med startvillkoret
\[
 \chi(0)=\bold B
\] 
där $B$  är en $M,M)$-matris vars element är komplexa tal
så väljer vi helt enkelt
\[ 
\chi(t)= \Psi(t)+ B\cdot \Phi(t)
\]

\medskip

\centerline{\bf{Euler-Picards iterativa lösning.}}


\medskip


\noindent
Ekvationen (**) kan lösas via en iterativ metod.
I första steget definieras
\[ 
\Phi_0(t)=E_M\quad\&\quad
\Phi_1(t)= E_n+\int_0^t\, A(s)\, ds\tag{i}
\]
och då $n\geq 1$ sätter vi
\[
\Phi_{n+1}(t)= E_M+\int_0^t\, A(s)\cdot \Phi_n(s)\, ds
\]
\medskip


\noindent
Det visar sig att den rekursiva
sviten $\{\Phi_n(t)\}$
konvergerar då $n\to+\infty$ och
gränsvärdet ger
den söka matrisvärda funktionen
$\bold \Phi(t)$ i Teorem XX.
Metoden
är att konstruera  en
\emph{majorantserie}
där vi utnyttjar \emph{normer} hos matriser från  XXXX.
\medskip


\noindent
{\bf{Övning.}}
Välj en norm på
$(M,M)$-matriser
och sätt
\[
||A||^*= \max_{0\leq t\leq T}\, ||A(t)||\tag{1}
\]
För varje $n\geq 1$ sätter
vi
\[
\rho_n(t)= \max_{0\leq s\leq t}\, ||\Phi_n(t)-\Phi_{n-1}(t)||
\]
där vi börjat med 
\[
\phi_0(t)=E_M\quad\&\quad
\rho_0(t)=1\quad 0\leq t\leq T
\]
Nu kan läsarten visa följande olikhet för varje $n\geq 0$:
\[
\rho_{n+1}(t)\leq t\cdot ||A||^*\cdot \int_0^t\, \rho_n(s)\,ds \tag{ii}
\]




\bigskip


\noindent
{\bf{Icke linjära system.}}
Picards rekursion ovan
kan också användas för att läsa
differentialekvationer av formen
\[ 
\dot y(t)= f(t,y(t))
\]
där $f(t,y)$ är en kontinuerlig funktion av de två variablerna $t$ och $y$.
Här år dock  entydighhet inte  säkerställd. Ett
tillräckligt villkor för entydighet är att $f$ är \emph{Lipschitzkontinuerlig} med avseende på
$y$-variabeln. Läsaren hänvisas till
Lars Gårdings
lärobok [Gask]
för
beviset.
En kraftfullare  text
som också ägnas åt
ordinära differentialekvationer ges i det inledande kapitlet
från Lars Hörmanders bok [Hö]. Bland annat visar denne eminente författare
på ett synnerligen detaljerat och
koncist vis
att
(*) har en unik maximal och en unik minimal lösning
under den enda förutsättningen att $f$ är kontinuerlig !













\centerline{\bf{Differentialekvationer.}}
\medskip


\noindent
Linjära system av
ordinära differentialekvationer
kan med fördel lösas på med hjälp av 
matrisvärda funktioner.
Till att börja med betraktas en oberoende  $t$-variabel  som 
i de flesta tillämpningar uppfattas
uppfattas som
tidsparameter.
Låt nu $n$ vara ett heltal $\geq 1$.
En reellvärd funktion $y(t)$ definierad på ett begränsat och 
slutet intervall $[0,T]$
är $n$ gånger kontinuerligt deriverbar om
dess derivator upp till
ordning
$n$ existerar och ger kontinuerliga funktioner på $[0,T]$.
Klassen av sådana funktioner betecknas med $C^n[0,T]$, och för varje 
sådan funktion betecknas dess derivator med
\[
 y^{(k)}(t)= \frac{d^ky}{dt^k}\quad 1\leq k\leq b
\]
Givet ett par av positiva  heltal $n$ och $N$
söker man $N$  funktioner $y_1(t),\ldots,y_N(t)$
i $C^n[0,T]$
som tillsammans uppfyller systemet
\[
y_p^{(n)}(t)= \sum_{k=0}^{n-1}\sum_{\mu=1}^{\mu=N}\,
a_{p,\nu,k}(t)\cdot y_\nu^{(k)}(t)\quad\, 1\leq p\leq N\tag{*}
\]
för alla $0\leq t\leq T$ där de trippel-indicerade $a$-funktionerna  är
reellvärda och kontinuerliga på $[0,T]$.
Detta system kan
skrivas om till
ett system av första ordningen, dvs. med $n=1$ där 
antalet $y$-funktioner
ökar från $N$ till $n\cdot N$.
Dessutom kan ett 
system av första ordningen
skrivas på  matrisform
där man
söker lösningar som består av
vektorvärda funktioner
$\bold y(t)=(y_1(t),\ldots,y_N(t)$ vilka utgör kolonner hos en tidsberoende \emph{matrisvärd funktion}. 
Den centrala uppgiften är därför att
lösa en differentialekvation
\[
\frac{d\Phi(t)}{dt}=A(t)\cdot \Phi(t)\tag{**}
\]
där 
$M\geq 2$ och $A(t)$ en
en $(M,M)$-matris vars element $\{a_{pq}(t)\}$
är kontinuerliga funktioner på $[0,T]$, varefter man
söker
den tidsberoende
$(M.M)$-matriser
$\Phi(t)$ som löser (*) med startvillkor 
\[
\Phi(0)= E_M\tag{i}
\]
dvs. vid tiden $t=0$ har man enhetsmatrisen.
Tidsderivatan av $\Phi(t)$
som uppträder  i
vänsterledet hos (**) 
ges helt enkelt genom att
ta $t$-derivatan av
matrisens $M^2$ många element $\{\phi_{pq}(t)$.
Så utskrivet  betyder (**) att följande system av första ordningen
är uppfyllt:
\[
\dot \phi_{pq}(t)= \sum_{\nu=1}^{\nu=N}\, a_{p\nu}(t)\cdot \phi_{\nu,q}(t)
\quad\, 1\leq p,q\leq M\tag{ii}
\]
Startvillkoret (i) betyder att
$\phi_{pq}(0)$ är Kroneckers delta-funktion.
med beteckningar enligt ovan kan vi  annonsera satsen nedan som
i sin fulla generalitet bevisades av Emile Picard 1880.
\medskip


\noindent
{\bf{Picards Teorem.}}\emph{Ekvationen (*) har en entydig
lösning
$\Phi(t)$ och  dessutom gäller att}
\[
det\,\Phi(t)=e^{Tr(A(t)}
\quad 0\leq t\leq T
\]
\emph{där $Tr(A(t))$ betecknar spåret hos matrisen $A(t)$}.

\bigskip


\noindent
{\bf{Inhomogena system.}}
Låt $\bold Z(t)$ vara en 
tidsberende $(M,M)$-matris, dvs. vars
$M^2$ många element $\{z_{pq}(t)\}$
är kontinuerliga på $[0,T]$.
Nu kan vi definiera den matrisvärda funktionen

\[ 
\Psi(t)= \Phi(t)\cdot \int_0^t\, \Phi(s)^{-1}\cdot \bold Z(s)\, ds\tag{1}
\]
Under integraltecknet
har vi alltså multiplicerat
de två matriserna $\Phi^{-1}(s)$ och $\bold (s)$. Elementen i denna
matrisprodukt är nu kontinuerliga funktioner
som var för sig integreras över intervall $[0,t]$ och
den resulterande matrisen multipliceras sedan med $\Phi(t)$.
Låt oss nu bilda   tidsderivator i vänster- och högerledet.
Vi får då
\[
\dot \Psi(t)=
\dot \Phi(t)\cdot \int_0^t\, \Phi(s)^{-1}\cdot \bold Z(s)\, ds
+\Phi(t)\cdot \Phi^{-1}(t)\cdot \bold Z(t)=
A(t)\cdot \Psi(t)+\bold Z(t)
\]
Således har vi också funnit en lösning till
ett inhomogent system där 
$\bold Z$-matrisen kan väljas godtyckligt !
\bigskip


\noindent
{\bf{Om startvillkor.}}
Notera att konstruktionen
av $\Psi$-matrisen innebär att
\[
\Psi(0)= \bold O_M= \text{nollmatrisen}
\]
Om man önskar en lösning
till den inhomogena ekvationen
\[
\dot \chi(t)= A(t)\cdot \chi(t)+\bold Z(t)
\] 
med startvillkoret
\[
 \chi(0)=\bold B
\] 
där $B$  är en $M,M)$-matris vars element är komplexa tal
så väljer vi helt enkelt
\[ 
\chi(t)= \Psi(t)+ B\cdot \Phi(t)
\]

\medskip

\centerline{\bf{Euler-Picards iterativa lösning.}}


\medskip


\noindent
Ekvationen (**) kan lösas via en iterativ metod.
I första steget definieras
\[ 
\Phi_0(t)=E_M\quad\&\quad
\Phi_1(t)= E_n+\int_0^t\, A(s)\, ds\tag{i}
\]
och då $n\geq 1$ sätter vi
\[
\Phi_{n+1}(t)= E_M+\int_0^t\, A(s)\cdot \Phi_n(s)\, ds
\]
\medskip


\noindent
Det visar sig att den rekursiva
sviten $\{\Phi_n(t)\}$
konvergerar då $n\to+\infty$ och
gränsvärdet ger
den söka matrisvärda funktionen
$\bold \Phi(t)$ i Teorem XX.
Metoden
är att konstruera  en
\emph{majorantserie}
där vi utnyttjar \emph{normer} hos matriser från  XXXX.
\medskip


\noindent
{\bf{Övning.}}
Välj en norm på
$(M,M)$-matriser
och sätt
\[
||A||^*= \max_{0\leq t\leq T}\, ||A(t)||\tag{1}
\]
För varje $n\geq 1$ sätter
vi
\[
\rho_n(t)= \max_{0\leq s\leq t}\, ||\Phi_n(t)-\Phi_{n-1}(t)||
\]
där vi börjat med 
\[
\phi_0(t)=E_M\quad\&\quad
\rho_0(t)=1\quad 0\leq t\leq T
\]
Nu kan läsarten visa följande olikhet för varje $n\geq 0$:
\[
\rho_{n+1}(t)\leq t\cdot ||A||^*\cdot \int_0^t\, \rho_n(s)\,ds \tag{ii}
\]




\bigskip


\noindent
{\bf{Icke linjära system.}}
Picards rekursion ovan
kan också användas för att läsa
differentialekvationer av formen
\[ 
\dot y(t)= f(t,y(t))
\]
där $f(t,y)$ är en kontinuerlig funktion av de två variablerna $t$ och $y$.
Här år dock  entydighhet inte  säkerställd. Ett
tillräckligt villkor för entydighet är att $f$ är \emph{Lipschitzkontinuerlig} med avseende på
$y$-variabeln. Läsaren hänvisas till
Lars Gårdings
lärobok [Gask]
för
beviset.
En kraftfullare  text
som också ägnas åt
ordinära differentialekvationer ges i det inledande kapitlet
från Lars Hörmanders bok [Hö]. Bland annat visar denne eminente författare
på ett synnerligen detaljerat och
koncist vis
att
(*) har en unik maximal och en unik minimal lösning
under den enda förutsättningen att $f$ är kontinuerlig !













\noindent
{\bf{Matrisalgebran.}}
För vajre $n\geq 2$ ehålles
mängden av alla kvadratiska matriser
som framställs av en dubbel-indicerad mängd på formen

\medskip

\[
A=
\begin{pmatrix}
a_{11}&a_{12}&\ldots  &a_{1n}\\
a_{21}&a_{22}&\ldots &a_{2n}\\
\ldots&\ldots&\ldots&\ldots\\
\ldots&\ldots&\ldots&\ldots\\
a_{n1}&a_{n2}&\ldots &a_{nn}\\
\end{pmatrix}
\]
De dubbelindicerade reella talen $\{a_{pq}\}$
utgör matrisens element.
För varje $1\leq k\leq n$
svarar $n$-tupeln $(a_{k1},a_{k2}\ldots ,a_{kn})$
mot matrisens radvektor. På samma sätt erhålles 
kolonnvektorer där 
$(a_{11},a_{21},\ldots ,a_{n1})$
är matrisens  första kolonn.
\medskip


\noindent
Om $B$ är en annan $(n,n)$-matris med element $\{b_{pq}\}$
definieras matrisprodukten
\[
AB=C\quad\colon\,
c_{pq}= \sum_{\nu=1}^{\nu=n}\, a_{p\nu}\cdot b_{\nu q}\tag{1}
\]
Läsaren bör konfirmera att
den associativa lagen för produkter av reella tal
medför att om $A,B,C$ är tre matriser så gäller
likheten
\[ 
A(BC)= (AB)C
\]
Vidare finns enhetsmatrisen
\[
E_n=
\begin{pmatrix}
1&0&\ldots  &0\\
0&1&\ldots &0\\
\ldots&\ldots&\ldots&\ldots\\
\ldots&\ldots&\ldots&\ldots\\
0&0&\ldots &1\\
\end{pmatrix}
\]
och här gäller att
\[ AE_n=E_nA=A
\]
för alla $A$.
\medskip

\noindent
\emph{Addition} av två matriser utföres elementvis, dvs. om$A$ och $B$ är två matriser så erhålles $A+B$ med element
$\{a_{pq}+b_{pq}\}$.
Till skillnad från produkter av reella tal är
matrismultiplikation \emph{inte} kommutativ.
med $n=2$ har man t.ex.
\{medskip


\noindent
Notera att förutom brist på kommuterande produkt kan även
en produkt av två nollskilda matriser bli den trivial nollmatrisen.
Slutsatsen är att $M_n({\bf{R}}$
är en associativ algebra som inte är kommutativ
och där det dessutom kan förekomma nolldelare.
I avsnitt § XX diskuteras rent algebraiska egenskaper hos
$M_n({\bf{R}})$ och vi kommer även att studera
matriser vars element är komplexa tal vilket ger
algebran $M_n({\bf{C}}$
där en komplex matris uppstår då
de dubbel-indicerade $a$-talen ovan nu får vara komplexa tal.
\medskip


\noindent
Även i situationer då man utgår från en reell matris $A$
är det ofta
användbart att betrakta $A$ i den större algebra
$M_n({\bf{C}})$.
Ett skäl för detta är \emph{Algebrans Fundamentalsats}
som säger att varje polynom $q(x)$
av någpn grad $n$ kan faktoriseras till formen
\[
q(x)= \prod_{\nu=1}^{\nu=n}\, (x-\alpha_\nu)
\]
där $\{\alpha_\nu\}$
i allmännhet är komplexa tal. Här kan i allmännhet multipla rötter
förekomma, dvs.
$n$-tupeln $\{\alpha_\nu\}$
består inte alltid av $n$ olika komplexa tal.
I Avsnitt § xx stideras komplexa matriser
med fokus på
deras tillhörande
\emph{resolventer} som per defintion ges av invera matriser
\[
R(\lambda)= (\lambda\cdot E_n-A)^{-1}\tag{*}
\] 
där $\lambda$ är ett komplext tal som ligger utanför
$A$-matrisen \emph{spektrum} som består av
rötter till
dess \emph{karakteristiska polynom}
\[ 
\Delta(\lambda)= det(\lambda\cdot E_n-A)
\]
Här har vi "gått händeleserna i förväg" eftersom
då vi talar om determinanten hos en matris vars konstruktion görs i § xx.
Ett fundamentalt result är att
resolventmatrisen i (*) som berpr av
den komplexa variablen
$\lambda$ är
en
rationell funktion
vars koefficienter nu istället för komplex tal utgöres av matriser.
Mer precist gäller i analogi med
partialbråksuppdelningen av polynom att
man har
\[ 
R(\lambda)= 
\sum_{\nu=1}^{\nu=k}\, [\sum_{j=1}^{j=e_\nu}\, \frac{C(\nu,j)}{(\lambda-\alpha_\nu)^j}]\tag{*}
\]
där $\{C(\nu,j)\}$ är $(n,n)$-matriser och
uppdelningen bygger på att
\[ 
\Delta(\lambda)= \prod_{\nu=1}^{\nu=k}\, (\lambda-\alpha_\nu)^{e_\nu}
\]
Speciellt gäller att om $k=n$, dvs. om 
$A$-matrisen karakteristiska polynom
$\Delta(\lambda)$ har $n$ enkla rötter så är
\[
R(\lambda)= 
\sum_{\nu=1}^{\nu=n}\,\frac{C_\nu}{\lambda-\alpha_\nu}\tag{**}
\]









FLYTTAS !!!

\bigskip

Men Håll kvar ev. material !!! xxxx

\noindent
Matriser uppträder i  kalkyler som ingår i såväl
algebra som   analys även om undertecknad anser
att det inte finns någpn som helst boskillnad melllan dessa
discipliner inom
vad
som med rätta endast bär beskrivas som Matematik !
Ett område  där
alla slags metoder hämtade från matematik
är 
differentialgeometri där 
resultat om 
differentialformer på
mångfalder  baseras 
kalkyler som utgår från matriser
där ett typiskt exempel är den kaniska symplektiska strukturen
hos  $T^*(M)$ där $M$ är en $C^\infty$  mångfald av någon dimension $n\geq 2$
och $T^*(M)$ betecknar dess kotangentrum där den symplektiska
och $d$-exakta
$2$-formen $\sigma$ uppträder. Resultat som nyttjar denna kanoniskt definierade
sympletiska
struktur på $T^*(M)$ baseras på förberedande resultat om
symplektiska matriser.
\medskip


\noindent
Vi kommer främst att behandla matriser vars element
är reella - och eller - komplexa tal.
Men många grundläggande konstruktioner som
matrisers determinanter, yttre produkter med mera
kan utvidgas  på allmänna vektorrum där
skalärerna hör till
en godtycklig kropp.
Studiet av matriser med pionjärinsatser av bland andra
Hamilton, Cayley och Sylvester har banat väg för 
funktionalanalysens operatorteori
där
matriskalkyl  utvidgas för att 
studera  
linjära operatorer på Hilbertrum och även  mer generella topologiska vektorrum.
Så det följande
materialet  är i hög grad "nyttigt" eftersom matriser
ger ett behändigt redskap inom en rad olika discipliner !
\medskip


\noindent
Läsaren
förväntas vara förtrogen med
grundläggande euklidisk geometri
och åskådliga exempel som uppträder när matriser
av ordning (2,2) eller (3,3)
utnyttjas för
att klarlägga 
geometriska satser i planet eller det
3-dimensionella euklidiska rummet.
Men  avsnitt § XX
ägnas åt $(2,2)$-matriser där några av resultaten visar
att man redan här möter en  intrikata kalkyler som
samtidigt
illustrerar fenomen i högre dimensioner.
Betrakta t.ex. en $(2,2)$-matris
\[
A=
\]
där $a>0$ är ett godtyckligt postivt reellt tal.
Matrisen ger nu en linjära avbildning som sänder
en punkt $(x,y)$ i det euklidiska planet till
\[
A(x,y)= (x+ay,y)
\]
I § zz visar vi
att det existerar ett  par av vektorer
\[
\bold u=x_1\bold e_x+y_1\bold e_y\,\&\,
\bold v=x_2\bold e_x+y_2\bold e_y
\]
där 
\[ 
x_1^2+y_1^2=x_2^2+y_2^2=1
\]
och vektorerna 
$A(\bold u)$ och $A(\bold v)$ är vinkelräta mot varandra
vilket uttrycks av ekvationen
\[
\langle A(\bold u),A(\bold v)\rangle =
(x_1+ay_1)(x_2+ay_2)+y_1y_2=0
\]
Även om det är "fullt möjligt" att visa detta med en direkt kalkyl så
är det från början inte uppenbart hur man bör gå tillväga
för att
finna $\bold u$ och $\bold v)$.
Så detta ger ett exempel  på nyttan av att bli förtrogen 
med linjära algebra som många  situationer
tillhandahåller
"kanoniska recept", t.ex. hur man ovan kan finna 
$\bold u$ och $\bold v$.
Fallet $n=2$ kan sedan utvidgas till
godtycklig dimension $n\geq 3$ där man startar med en 
$(n,n)$-matris $A$  som antas vara inverterbar, dvs. den tillhörande
linjära avbildningen på
${\bf{R}}^n$ är bijektiv.
I § xx visas  existensen 
av $n$  vektorer $\bold u_1,\cdots,\bold u_n$
som var och en har längd ett, dvs. vektorernas spetsar  ligger på
enhetssfären $S^{n-1}$ och deras  
bildvektorer är  ortogonala, dvs.
\[
p\neq q\implies\langle\bold u_p,\bold u_q\rangle=0
\]
\medskip


\noindent
Låt oss också
erinra om att redan studuet av $(2,2)$-matriser leder
till
mycket intrikata resultat.
Ett exempel ges i Ref [xxx]
där artikeln
\emph{A stochastic web and diffusion particles
in a magnetic field}
naseras på följnade analys:
Låt $p$ och $q$ mvara tvp åosyoova heltal utan gemensam primtalsdelare.
Med $\phi= \frac{p}{q}$ bildas den ortogonala 
matrisen
\[ 
A=
\]
Betrakta också ett litet positivt tal $\epsilon>0$
och matrisen
\[
B= \epsilon = \epsilon
\]
Sätt 
\[
T= AB = xxxx= A+\epsilon xxxx\tag{xx}
\]
Notera att valet hos vinkeln  $\phi$ ger att
$A^q=E_2$ och (xx) ger
\[
T^q=(A+\epsilon\cdot B)^q
\]
med ett mycket litet $\epsilon$ ignorerar 
$\epsilon ^2$-termer och övriga högre
$\epsilon$-potenser som ger att
\[ 
T^q\simeq A+ \epsilon(A^{q-1}B+ A^{q-2}BA+\ldots+ BA^{q-1})+ Ordo(\epsilon^2)
\]
där läsaren bor observera att matriserna $A$ och $B$ \emph{inte kommuterar}
varför
summan ovan inte ges av den "kommuativa formeln" $qAB$.
Matrien $T$ har determinant lika med ett och
ger därför en \emph{areabevarande} avbildiing
och speicellt betraktas $T$ -bilen av ernhetscirkel som
blir en ellips
$\mathcal E$.
Med ett mycket litet $\epsilon$ - som 
kan man fortsätta till högre postens $T^{Nq}\, N=1,2,\ldots$
och
tolka
dem som tidsberoende förflyttningar av punkter på enhhetscirkeln.
Resukltatet av detta är att
man får en approximativt  av \emph{kvasiperiodisk}
process vars
betydelse bland annat är framlyft av V.I. Arnold i hans synnerligen  läsvärda bok 
bok \emph{Huyghens, Barrow, Newton \& Hooke}.














\bigskip





Ett exempel på kraften via matriskakyl är följande:
\medskip

\noindent
\emph{Till varje Varje triangel $\Delta$ i $x,y)$-planet
hör en inskriven ellips
$E$ som tangerar var och en av triangelns tre sidor i deras
respektive mittpunkt.
Mer precist gäller
att det för varje
tal $0<a<1$
finns en  skara av trianglar som omskriver
ellipsen}
\[
E(a)= \{x,y)\colon \frac{x^2}{a^2}+y^2=1
\]
Upp till likformighet
ger detta en
parameterframställning
av  trianglar där den liksida är exceptionell eftersom
dess inskrivna ellips är en cirkel.
Att  kartlägga det med
renodlade
geometriska metoder är rätt besvärligt medan
matriskalkyler ger ett behändigt svar som bygger på studiet av
(2,2)-matriser. Betrakta t.ex. en  matris
\[
A= xxxx
\]
där det ansas att dess determinant
\[ 
det (A) =ad-bc=1
\]
En speciell klass utgörs av orttgonala matriser
\[ 
S(\phi=xxxx
\] 
som helt enkelt avbildar planet på sig själv  via en vridning med vinkeln $\phi$.
När $A$ inte är ortogonal avbildar den enhetscirkel på en ellips $\mathcal E$ 
vars ekvation i $(x,y)$planet ges på parameterrormen:
\[ 
x(\phi= a\cdot \cos\,\phi+c\cdot \sin(\phi)\quad
\colon\, y((\phi)=
\]
Positionen hos denna ellips samt förhållandet mellkan längderna hos dess stor- respektive lill-axel
kan beräknas via matriskalkyl där ett  centralt resultat
säöger att  en
matris $A$ som ovan
kan skrivas som en produkt
\[ 
A= S(\psi)\cdot D\cdot S(\phi)
\]
där de två ortogonala matriserna är entydigt bestämda av $A$ och
$D$ är en diagonalmatris
\[ 
=xxx
\]
varvid $d>1$ svarar mot längden hos storaxeln hos $\mathcal E$ medan
$1/d$ är längden hos lill-axeln.
OBS ! Mera detaljer i ett separat avsnitt !

\newpage


\noindent
Det är förstås vägledande att vara förtrogen med matriskalkyl i det två-och det tre-dimensionella
fallen där åskådliga geometriska bilder
hjälper till för 
att klarlägga
resultat
i högre dimensioner.
I § xx  kommer vi att 
redogöra för  \emph{volymformler}
hos 
$k$-dimensionella simplex som är inbäddade i ${\bf{R}}^n$ där
$n\geq 4$ och $2\leq k\leq n-1$.
Låt oss även understryka
att mångas resultat  härleds  via de 
fundamentala \emph{additionsteoremen} för
de två trigonometriska sinus- och cosinusfunktionerna. Ett exempel ges i
§ XX där ett result av Arne Beurling
ger ett kriterium för att en \emph{kvadratisk form}
är positivt definit:
\medskip

\noindent
{\bf{Teorem.}}
\emph{Låt $m\geq 3$ och 
$\bold v_1,\ldots\bold v_N$
vara en $N$ olika vektorer i
${\bf{R}}^m$ där $N$ är ett godtyckligt heltal $\geq 2$.
Sätt}
\[
b_{ik}= ||\bold v_i||+||\bold v_k||-
||\bold v_i-\bold v_k||\quad\, 1\leq i,k\leq N
\]
\emph{Då är nedanstående  kvadratiska form positivt definit.}
\[
H(\xi_1,\ldots,\xi_N)= \sum\sum\, b_{ik}\cdot \xi_i\xi_k
\]






\medskip

\noindent
{\bf{Algebraiska formler}}.
Inom  "renodlad algebra" förekommer många
identiteter  där existensen  av "formella uttryck" ofta kan
inses direkt medan det däremot 
kan fordras  omfattande kalkyler som utnyttjar matriser
för att
räkna fram vilka koefficienter som uppträder i slutformlerna.
Ett exempel som 
är \emph{Utveckling i partialbråk}. Betrakta ett polynomi $q(\lambda)$ som i allmännhet har komplexa koefficienter och
via algebrans fundamentalsats är given av en produkt:
\[ 
q(\lambda)= \prod_{nu=1}^{\nu=k}\, (\lambda-\alpha_\nu)^{e_\nu}
\]
där $\alpha$-rötterna är olika men ett enskilt nollställe kan ha en rot med muliplicitet $e_\nu\geq 2$.
Gradtalet hos polynomet blir alltså
\[ 
N= e_1+\ldots+e_k
\]
Med dessa beteckningar gäller följden:
\medskip


\noindent
{\bf{Teorem}}. \emph{För  varje heltal $0\leq m\leq N-1$ existerar
konstanter $\{C_m(\nu,j)\}$
så att
den rationella funktion}
\[
\frac{\lambda^m}{q(\lambda)}=
\sum_{\nu=1}^{\nu=k}\, [\sum_{j=1}^{j=e_\nu}\, \frac{C_m(\nu,j)}{(\lambda-\alpha_\nu)^j}]\tag{*}
\]
\medskip


\noindent
{\bf{Kommentar}}.
Beviset kanm genomföras med induktion över
$N$. Se exempelvis Kapitel II i Torsten Carlemans eminenta lärobok
\emph{Differential- och Integralkalkyl}.
De två matematiska "giganterna"
Euler och Lagrange genomförde
en stor mängd av explicita kalkyler.
där (*) utnbyttjades för en rad olika syften.
Låt oss påpeka att  \emph{existensen} av
uppdelningen (*) omedelbart följer av  att
vänsterledet är en meromorf funktion med nollställen
i $\alpha$-punkterna där dess negativa potensserie - dvs. dess ändliga Laurent serie - 
bestäms enligt högerledet och differensen blir då en
\emph{hel} analytisk funtktion som
går mot noll då $\lambda|\to +\infty$ och ger  likheten (*).


\medskip


\noindent
I specialfallet då alla rötter är enkla, dvs. när alla $e_\nu=1$ så att $k=N$
kan man via induktion härleda formeln
\[
\frac{\lambda^m}{q(\lambda)}=
\sum_{\nu=1}^{\nu=N}\, 
\frac{\alpha_\nu^m}{\lambda-\alpha_\nu}\quad\, 1\leq m\leq k-1\tag{*}
\]
\medskip

\newpage











\centerline{\bf{The Schur-Weierstrass inequality,}}

\bigskip

\noindent
Let $A$ be an $n\times n$-matrix with operator norm
$\leq 1$, i.e.,$A$ is a contraction.
For each
polynomial $p(z)= a_0+a_1z+\ldots+a_Nz^N$
with complex coefficients we get the matrix $p(A)$.

\medskip

\noindent
{\bf{1. Theorem.}} \emph{One has the inequality}
\[
\text{Norm}(p(A))\leq \max_{z\in D}|\, |p(z)|
\]
\medskip

\noindent
To prove this we need a preliminary
result whjoich goes as follows.
Let $\alpha$ bne a point in the open unit disc. 
Since $A$ is a contraction yje following hold for every vector $\bold y$:
one has
\[
(1-|\alpha|^2)\cdot \langle (E_m-A^*A)(\bold y),\bold y\rangle\geq 0
\]
Expanding this inequality we get
\[
||\alpha E_m-A)(\bold y)||^2\leq ||\bold y-\bar\alpha\cdot A(\bold y)||^2\tag{i}
\]
Next, there
exits the matrix
\[
g_\alpha(A)= (\alpha E_m-A)\cdot (E_m-\bar\alpha A)^{-1}
\]
Consider a vector $\bold x$ of unit norm and set
\[
\bold y=(E_m-\bar\alpha A)^{-1}(\bold x)
\]
Then
\[ 
||g_\alpha(A)(\bold x)||^2=||\alpha E-A)(\bold y)||^2\leq ||y-\bar\alpha\cdot A(\bold y)||^2\tag{ii}
\]
where the last inequality used (i).
Now
$\bold y-\bar\alpha\cdot A(\bold y)=\bold x$ and hence (ii) gives
\[
||g_\alpha(A)(\bold x)||^2||\leq||\bold x||^2
\]
Since the vector $\bold x$ was arbitrary we conclude that
\[ 
\text{Norm}(g_\alpha(A))\leq 1\tag{3.1}
\]

\medskip


\noindent
Returning to the polynomial $p(z)$
we consider
its eventual zeros in the open 
unit disc and
if $\{\alpha_\nu\}$
are these zeros - where eventual multiplke zeros are repeated we
construct the \emph{Blaschke product}
\[
\mathcal B(z)=\prod_{alpha_\nu} \, \frac{z-\alpha_\nu}{1-\bar\alpha_\nu\cdot z}
\]
Since $A$  is a conyrasction there exist
the matrices
For every $\alpha_\nu$ we get the
matrix
\[
(1-|\alpha|^2)\cdot \langle (E-A^*A)(y),y\rangle\geq 0
\]
hold for every vector $y$. Expanding this inequality we get
\[
||\alpha E-A)(y)||^2\leq ||y-\bar\alpha\cdot A(y)||^2\tag{i}
\]
Next, there
exists the matrix
\[
g_\alpha(A)= (\alpha E-A)\cdot (E-\bar\alpha A)^{-1}
\]
Consider a vector $x$ of unit norm and set
\[
y=(E-\bar\alpha A)^{-1}(x)
\]
Then
\[ 
||g_\alpha(A)(x)||^2=||\alpha E-A)(y)||^2\leq ||y-\bar\alpha\cdot A(y)||^2\tag{ii}
\]
where the last inequality used (i).
Now
$y-\bar\alpha\cdot A(y)=x$ and hence (ii) gives
\[
||g_\alpha(A)(x)||^2||\leq||x||^2
\]






To prove this we first
establish a general  inequality which goes back to Schur.
Let  $g(z)$ be an analytic function
in the unit disc which extends continuously to the boundary
and  $A$ is  some  $n\times n$-matrix whose spectrum is 
contained in the open unit disc.
If $g(z)$ has the series expansion $\sum\, c_kz^k$
we know from § xx that the matrix-valued series
$\sum\, c_kA^k$ converges and gives a matrix $g(A)$.
There exists also the exponential matrix
\[
B= e^{g(A)}
\]
If $g^*(z)=\sum\, \overline{c_k}z^k$ then
\[
B^*=e^{g^*(A^*)}
\]
Put
\[
C=e^{g^*(A^*+g(A)}=B^*B
\]
The result in § xx gives
\medskip

\noindent
{\bf{10.2 The Schur-Weierstrass inequality.}}
\emph{For each pair $A$ and $g$ as above one has}
\[
\text{Norm}(e^{g(A)})=
\max_{\lambda\in\sigma(A)}\, e^{\mathfrak{Re}(g(\lambda))}\tag{*}
\]
\medskip

\noindent
Notice that (*) holds under the sole assumption that
$\sigma(A)\subset D$, i.e.  $A$ need not be a contraction.

\medskip

\noindent
{\bf{10.3.  Another norm inequality.}}
Let $\alpha$ be a point in the open unit disc and suppose that
$A$ is a contraction. It follows that
\[
(1-|\alpha|^2)\cdot \langle (E-A^*A)(y),y\rangle\geq 0
\]
hold for every vector $y$. Expanding this inequality we get
\[
||\alpha E-A)(y)||^2\leq ||y-\bar\alpha\cdot A(y)||^2\tag{i}
\]
Next, there
exists the matrix
\[
g_\alpha(A)= (\alpha E-A)\cdot (E-\bar\alpha A)^{-1}
\]
Consider a vector $x$ of unit norm and set
\[
y=(E-\bar\alpha A)^{-1}(x)
\]
Then
\[ 
||g_\alpha(A)(x)||^2=||\alpha E-A)(y)||^2\leq ||y-\bar\alpha\cdot A(y)||^2\tag{ii}
\]
where the last inequality used (i).
Now
$y-\bar\alpha\cdot A(y)=x$ and hence (ii) gives
\[
||g_\alpha(A)(x)||^2||\leq||x||^2
\]
Since the vector $x$ was arbitrary we conclude that
\[ 
\text{Norm}(g_\alpha(A))\leq 1\tag{3.1}
\]
\bigskip

\noindent
\emph{Proof of Theorem 1.}
By scaling we can assume that the maximum norm
$|p|_D=1$.
We construct the Blaschke product taken over the zeros of $p$ in the open unit disc
and get a factorisation
\[
p(z)=B(z)\cdot e^{g(z)}
\]
where the zero-free analytic function $e^{g(z)}$ has maximum norm one which gives
$\mathfrak{Re}(g)(z)\leq 0$ for all $z\in D$.
Now
\[
p(A)= B(A)\cdot e^{g(A)}
\]
Here $B(A)$ is the product of operators of the form $-g_\alpha(A)$
where $\alpha$ are zeros of $p$ in $D$. By (3.1)
each of these operators have norm $\leq 1$ and (*) in (2) entails that
the same holds for $e^{g(A)}$.
So $p(A)$ is the product of operators of norm $\leq 1$ and
Theorem 1 follows.
\medskip


\noindent
{\bf{Remark.}}
The proof given by von Neumann in [1951]
is carried out for operators on Hobert spaces and we remark only that
the present version for matrices easilly extends to contractions on Hilbert spaces.
Abve we empoyed Blaschke's factorisation which was used by Schur, while 
the proof by von Neumann in [1951] avoid 
Blasche products
via  certain constructions of unitary operators 
arising from   contractions. The interested
reader should consult the text-book
[Davies] for this proof as well as further extensions 
of Theorem 10.1 which
appear in [ibid: Chapter 10].



































\centerline{\bf{10. von Neumann's inequality.}}

\bigskip

\noindent
Let $A$ be an $n\times n$-matrix with operator norm
$\leq 1$, i.e., $A$ is a contraction.
For each
polynomisl $p(z)= a_0+a_1z+\ldots+a_Nz^N$
with complex coefficients we get the matrix $p(A)$.

\medskip

\noindent
{\bf{10.1 Theorem.}} \emph{One has the inequality}
\[
\text{Norm}(p(A))\leq \max_{z\in D}|, |p(z)|
\]
\medskip

\noindent
To prove this we first
establish a general  inequality which goes back to Schur.
Let  $g(z)$ be an analytic function
in the unit disc which extends continuously to the boundary
and  $A$ is  some  $n\times n$-matrix whose spectrum is 
contained in the open unit disc.
If $g(z)$ has the series expansion $\sum\, c_kz^k$
we know from § xx that the matrix-valued series
$\sum\, c_kA^k$ converges and gives a matrix $g(A)$.
There exists also the exponential matrix
\[
B= e^{g(A)}
\]
If $g^*(z)=\sum\, \overline{c_k}z^k$ then
\[
B^*=e^{g^*(A^*)}
\]
Put
\[
C=e^{g^*(A^*+g(A)}=B^*B
\]
The result in § xx gives
\medskip

\noindent
{\bf{10.2 The Schur-Weierstrass inequality.}}
\emph{For each pair $A$ and $g$ as above one has}
\[
\text{Norm}(e^{g(A)})=
\max_{\lambda\in\sigma(A)}\, e^{\mathfrak{Re}(g(\lambda))}\tag{*}
\]
\medskip

\noindent
Notice that (*) holds under the sole assumption that
$\sigma(A)\subset D$, i.e.  $A$ need not be a contraction.

\medskip

\noindent
{\bf{10.3.  Another norm inequality.}}
Let $\alpha$ be a point in the open unit disc and suppose that
$A$ is a contraction. It follows that
\[
(1-|\alpha|^2)\cdot \langle (E-A^*A)(y),y\rangle\geq 0
\]
hold for every vector $y$. Expanding this inequality we get
\[
||\alpha E-A)(y)||^2\leq ||y-\bar\alpha\cdot A(y)||^2\tag{i}
\]
Next, there
exists the matrix
\[
g_\alpha(A)= (\alpha E-A)\cdot (E-\bar\alpha A)^{-1}
\]
Consider a vector $x$ of unit norm and set
\[
y=(E-\bar\alpha A)^{-1}(x)
\]
Then
\[ 
||g_\alpha(A)(x)||^2=||\alpha E-A)(y)||^2\leq ||y-\bar\alpha\cdot A(y)||^2\tag{ii}
\]
where the last inequality used (i).
Now
$y-\bar\alpha\cdot A(y)=x$ and hence (ii) gives
\[
||g_\alpha(A)(x)||^2||\leq||x||^2
\]
Since the vector $x$ was arbitrary we conclude that
\[ 
\text{Norm}(g_\alpha(A))\leq 1\tag{3.1}
\]
\bigskip

\noindent
\emph{Proof of Theorem 1.}
By scaling we can assume that the maximum norm
$|p|_D=1$.
We construct the Blaschke product taken over the zeros of $p$ in the open unit disc
and get a factorisation
\[
p(z)=B(z)\cdot e^{g(z)}
\]
where the zero-free analytic function $e^{g(z)}$ has maximum norm one which gives
$\mathfrak{Re}(g)(z)\leq 0$ for all $z\in D$.
Now
\[
p(A)= B(A)\cdot e^{g(A)}
\]
Here $B(A)$ is the product of operators of the form $-g_\alpha(A)$
where $\alpha$ are zeros of $p$ in $D$. By (3.1)
each of these operators have norm $\leq 1$ and (*) in (2) entails that
the same holds for $e^{g(A)}$.
So $p(A)$ is the product of operators of norm $\leq 1$ and
Theorem 1 follows.
\medskip


\noindent
{\bf{Remark.}}
The proof given by von Neumann in [1951]
is carried out for operators on Hobert spaces and we remark only that
the present version for matrices easilly extends to contractions on Hilbert spaces.
Abve we empoyed Blaschke's factorisation which was used by Schur, while 
the proof by von Neumann in [1951] avoid 
Blasche products
via  certain constructions of unitary operators 
arising from   contractions. The interested
reader should consult the text-book
[Davies] for this proof as well as further extensions 
of Theorem 10.1 which
appear in [ibid: Chapter 10].


































Från sida 22 !!

\newpage



\noindent
I detta avsnitt
visas ett resultat av Arne Beurling som
som framaställer en klass av positivt definita
bilinjära former.
Låt 
$\bold x_1,\ldots,\bold x_N$
vara $N$ många \emph{distinkta} vektorer i
${\bf{R}}^m$ där $m\geq 2$ där $N$ i allmännhet är betydligt större än $m$.
För varje par  $1\leq j,k\leq N$
sätter vi
\[
 b_{jk}=||\bold x_j||+||\bold x_k||-||\bold x_j-\bold x_k||
\]
där vi använder den euklidiska  normen i
${\bf{R}}^m$.
Vi erhåller  den kvadratiska formen
\[
H(\xi_1,\ldots,\xi_N)=
\sum\sum\, b_{jk}\cdot_j\cdot \xi_k
\]
\medskip


\noindent
{\bf{Teorem.}} \emph{$H(\xi_\bullet)$
är positivt definit.}
\medskip


\noindent
Becbset baseras på följande  formel för att uttrycka längden hos vektorer i
${\bf{R}}^m$.
\medskip


\noindent
{\bf{Teorem.}}\emph{Det existerar en konstant $C_m$ så att fgölk´jande likhet gäller för varje $m$-vektor $\bold x$}
\[
||\bold x||= C_m\cdot \int_{{\bf{R}}^m}\,\frac{1-\cos\langle\bold x,\bold y\rangle }{||\bold y||^{m+1}}\,dy
\]
\medskip


\noindent
\emph{Bevis.}
Vi använder polära koordinater i ${\bf{R}}^m$  så att integralen i (*)
kan skrivas på formen
\[
\int_0^\infty\, [\int_{S^{m-1}}\, (1-\cos\langle X,r\cdot \omega\rangle \, dA(\omega)]\cdot \frac{dr}{r^2}
\]
där $S^{m-1}$ är enhetssfären och i den inre integralen integreras med avseende på
dess $m-1)$-dimensionella  areamått.
Uppenbart beror integralen ovan endast av normen hos $bold x$-vektorn
och med $||\bold x||=M$ kan vi utan inskränkning anta att
$\bold x=(A,0,\ldots,0)$. Den inre integrtalen blir då
\[
\int_{S^{m-1}}\, (1-\cos( Mr\cdot \omega_1) \, dA(\omega)]\cdot \frac{dr}{r^2}
\]

























\newpage





\centerline{\bf{Infinite systems of linear equations}}

\bigskip

\centerline{\bf{Ivar Fredholms integralekvationer.}}



\medskip


\noindent
I banbrytande arbeten från 1890-talet introducerade Ivar Fredholm
generella determinanter
för att analysera resolventer till
integraloperatorer.
Ett inledande resultat är följande:
\medskip


\noindent
Låt $\phi_1,\ldots,\phi_p$
och $\psi_1,\ldots,\psi_p$ vara $2p$ många kontinuerliga
och i allmännhet komplexvärda funktioner på
det kompakta enhetsintervallet $[0,1]$. De ger en $(p,p)$ matris $A$
med element


\[
a_{\nu k}= \int_ 0^1\, \phi_\nu(x)\psi_k(x)\cdot dx
\]
Vidare införs följande
två funktioner av $p$ många variabler som  båda definieras
på $[0,1]^p$:

\[ 
\Phi(x_1,\ldots,x_p)
=\text{det}
\begin{pmatrix}
\phi_1(x_1)&\cdots&\phi_1(x_p)\\
\cdots &\cdots&\cdots \\
\cdots &\cdots&\cdots\\
\phi_p(x_1)&\cdots &\phi_1(x_ p)\\
\end{pmatrix}\quad\colon\quad
\Psi(x_1,\ldots,x_p)
=\text{det}
\begin{pmatrix}
\psi_1(x_1)&\cdots&\psi_1(x_p)\\
\cdots &\cdots&\cdots \\
\cdots &\cdots&\cdots\\
\psi_p(x_1)&\cdots &\psi_1(x_ p)\\
\end{pmatrix}
\]

\medskip

\noindent
Produktregler för determinanter ger då Gram-Fredholms ekvation:

\[
\text{det}(A)=
\frac{1}{p\,!}\int_{[0,1]^p}\, 
\Phi(x_1,\ldots,x_p)\cdot
\Psi(x_1,\ldots,x_p)\cdot
dx_1\ldots dx_p\tag{*}
\]
\medskip

\noindent
{\bf{Övning.}} Visa (*) och vid behov kan literastur konsulkteras där
man särskilt kan nämna den utmärkta läroboken
[Bocher]
"which contains a detailed account about Fredholm
determinants and
their role for solutions to integral equations".





 
\bigskip

\noindent
\centerline
{\bf{0.D Resolvents of integral operators.}}
\medskip

\noindent
Fredholm studied integral equations of the form
\[
\phi(x)\vvv \lambda\cdot 
\int\uuu\Omega\,
K(x,y)\cdot \phi(y)\cdot dy=
f(x)\tag{*}
\]
where $\Omega$ is a bounded domain in some euclidian space
and the kernel function $K$ is complex\vvv valued. In general no
symmetry condition is imposed.
Various regularity conditions can be imposed upon the kernel. The simplest is when
$K(x,y)$ is a continuous function in
$\Omega\times\Omega$.
The situation becomes more involved when singularities occur, for example when
$K$ is $+\infty$ on the diagonal, i.e. $|K(x,x)|=+\infty$.
This occurs for example when
$K$ is derived from Green's functions
which yield fundamental solutions to
elliptic PDE\vvv equations
where corresponding boundary value problems are solved
via integral equations. 
To obtain square integrable solutions in (*) for less regular kernel
functions, 
the original determinants used by Fredholm were modified by
Hilbert which avoid the singularities
and lead to quite general 
formulas for resolvents of the integral operator $\mathcal K$ defined by
\[
\mathcal K(\phi)(x)=\int\uuu\Omega\,
K(x,y)\cdot \phi(y)\cdot dy
\]
One studies foremost the case when $K$ is square integrable,  i.e.
when
\[
\iint\uuu{\Omega\times\Omega}\, |K(x,y)|^2\, dxdy<\infty\tag{*}
\]
An eigenvalue is a complex number
$\lambda\neq 0$ for which there exists a non\vvv zero function $\phi$ such that
\[
\mathcal K(\phi)= \lambda\cdot \phi
\]
It is not difficult to show that (*) entails  that 
the set of eigenvalues form a discrete set $\{\lambda\uuu n\}$.
In the article [Schur: 1909] Schur proved 
the inequality
\[
\sum\, \frac{1}{|\lambda\uuu n|^2}\leq
\iint\uuu{\Omega\times\Omega}\, |K(x,y)|^2\, dxdy<\infty\tag{**}
\]

\noindent
Notice that  one does not assume that
the kernel function is symmetric, i.e. in general $K(x,y)\neq K(y,x)$.

\bigskip

\noindent
{\bf{0.D.1 Hilbert's determinants.}}
Let $K$ be a kernel function for which the integral (*) is finite.
A typical case is that
$K$ is singular on the diagonal subset
of $\Omega\times\Omega$.
To each positive integer $m$ one associates a pair of matrices of size
$(m+1)\times m(+1)$ whose elements depend upon a pair 
$(\xi,\eta)
\in\Omega\times\Omega$ and an $m$\vvv tuple of distinct points
$x\uuu 1,\ldots,x\uuu m$ in $\Omega$: 

\[
C^*\uuu m=
\begin{pmatrix}
0&K(\xi,x\uuu 1)&K(\xi,x\uuu 2)&\ldots&\ldots &K(\xi, x\uuu m)\\
K(x\uuu 1,\eta)&0&K(x\uuu 1,x\uuu 2)&\ldots&\ldots &K(x\uuu 1,x\uuu m)\\
K(x\uuu 2,\eta)&K(x\uuu 2,x\uuu 1)&0
&\ldots&\ldots&0\\
\ldots&
\ldots&
\ldots&
\ldots&\ldots&\ldots
\\
\ldots&
\ldots&
\ldots&
\ldots&\ldots&\ldots
\\
K(x\uuu m,\eta)&K(x\uuu m,x\uuu 1)&K(x\uuu m,x\uuu 2)&\ldots &\ldots&0\\
\end{pmatrix}
\]

\bigskip


\[
C\uuu m=\begin{pmatrix}
0&K(x\uuu 1,x\uuu 2)&&\ldots&0&K(x\uuu 1, x\uuu m)\\
K(x\uuu 2,x\uuu 3)&0&K(x\uuu 2,x\uuu 3)
&\ldots&&K(x\uuu 2,x\uuu m)\\
\ldots&\ldots &\ldots&\ldots&\ldots&\ldots\\
\ldots&
\ldots&
\ldots&
\ldots&\ldots&\ldots
\\
K(x\uuu m,x\uuu 1)&K(x\uuu m,x\uuu 2)
&K(x\uuu m,x\uuu 3) &\ldots&K(x\uuu m,x\uuu{m\vvv 1})&0\\
\end{pmatrix}
\]


\noindent
Put:
\[
D^*\uuu m(\xi,\eta)=
\int\uuu{\Omega^m}\, 
C^*\uuu m(\xi,\eta: x\uuu 1,\ldots,x\uuu m)\cdot dx\uuu 1\cdots dx\uuu m\tag{i}
\]
\[
D\uuu m=
\int\uuu{\Omega^m}\, 
C\uuu m(x\uuu 1,\ldots,x\uuu m)\cdot dx\uuu 1\cdots dx\uuu m\tag{ii}
\]
Thus, we take the integral over the $m$\vvv fold product of
$\Omega$.
Next, let $\lambda$ be a new complex parameter and set
\[ 
\mathcal D^*(\xi,\eta,\lambda)=
\sum\uuu{m=1}^\infty\, \frac{(\vvv\lambda)^m}{m!}
\cdot D^{**}\uuu m(\xi,\eta)
\]

\[ 
\mathcal D(\lambda)=1+
\sum\uuu{m=1}^\infty\, \frac{(\vvv\lambda)^m}{m!}
\cdot D_m
\]

\noindent


\medskip

\centerline{\bf{Some results by Carleman}}
\medskip


\noindent
Using the Fredholm-Hilbert determinants 
some
conclusive facts about integral operators
were established
by Carleman in the article \emph{Zur Theorie der Integralgleichungen}
from 1921  when the kernel
$K$ is of the Hilbert-Schmidt type, i.e. below we assume that
\[ 
\iint\, |K(x,y]|^2\, dxdy<\infty\tag{*}
\]


\medskip

\noindent
{\bf{D.2.1 Theorem.}} \emph{The kernel of the resolvent associated to the integral operator
$\mathcal K$ is for each complex $\lambda$ outside the spectrum 
given by}
\[
\Gamma(\xi,\eta;\lambda)=
K(\xi,\eta)+\frac{\mathcal D^*(\xi,\eta,\lambda)}{
\mathcal D(\lambda)}
\]


\noindent
{\bf{D.2.2  Remark.}}
Let  $\{\lambda\uuu\nu\}$ be the discrete spectrum of $\mathcal K$ 
where multiple
eigenvalues are repeated when the corresponding 
eigenspaces have dimension $\geq 2$. 
This spectrum constitutes the zeros of the entire function
$\mathcal D(\lambda)$. So when
$\lambda$ is outside this zero set
the inverse operator $(\lambda\cdot E-\mathcal K)^{-1}$
is the integral operator
defined by
\[
f\mapsto \int_\Omega\, \Gamma(\xi,\eta;\lambda)\cdot f(\eta)\, d\eta
\]
where $\xi$ and $\eta$ denote variable points in
$\Omega$.
Using
inequalities of Fredholm\vvv Hadamard type for determinants, it
is also proved in [ibid] that:
\[
\int\uuu{\Omega}\, \Gamma (\xi,\xi; \lambda)\cdot d\xi=
\vvv \lambda\cdot \sum\uuu{\nu=1}^\infty\,
\frac{1}{\lambda\uuu\nu(\lambda\vvv\lambda\uuu\nu)}\tag{D.2.3}
\]
\medskip


\noindent
Another major result in [ibid]
deals with
the function $\mathcal D(\lambda)$.

\medskip

\noindent
{\bf{D.2.4 Theorem.}}
\emph{$\mathcal D(\lambda)$ is an entire function of the complex parameter
$\lambda$ given by a Hadamard product}
\[
\mathcal D(\lambda)=
\prod\,(1-\frac{\lambda}{\lambda_n})\cdot e^{\frac{\lambda}{\lambda_n}}\tag{1}
\]
\emph{where $\{\lambda_n\}$ satisfy}
\[
\sum\, |\lambda_n|^{-2}<\infty\tag{2}
\]
\medskip


\noindent
{\bf{Remark.}}
Prior to this Schur had proved a
representation 
for $\mathcal D(\lambda)$ as above adding
a factor $e^{b\lambda^2}$.
So the novelty
in Carleman's
work is that
$b=0$ always hold.
Apart from Schur's result that (2) above is convergent, a
crucial
step in Carleman's  proof of (1) 
was to use  an  inequality for
determinants which goes as follows:
Let $q>p\geq 1$ be a pair of integers
and
$\{a\uuu{k,\nu}\}$ a doubly\vvv indexed sequence of complex numbers
which appear as elements in a $p+q$\vvv matrix of the form:






\[
\begin{pmatrix}
0&\ldots&0&a\uuu{1,p+1}&\ldots &a\uuu{1,p+q}\\
0&\ldots&0&a\uuu{2,p+1}&\ldots &a\uuu{1,p+q}\\
\ldots&
\ldots&\ldots&\ldots&\ldots&\ldots \\
0&\ldots&0&a\uuu{p,p+1}&\ldots &a\uuu{p,p+q}\\
a\uuu{p+1,1}&\ldots &a\uuu{p+1,p}&a\uuu{p+1,p+1}&\ldots&
a\uuu{p+1,p+q}\\
\ldots&
\ldots&\ldots&\ldots&\ldots&\ldots
\\
a\uuu{p+q,1}&\ldots &a\uuu{p+q,p}&a\uuu{p+q,p+1}&\ldots
&a\uuu{p+q,p+q}
\\
\end{pmatrix}\tag{*}
\]
\medskip



\noindent
For each pair $1\leq m\leq p$ we put
\[
L\uuu m=\sum\uuu{\nu=1}^{\nu=q}\, |a\uuu{m,p+\nu}|^2
\quad\colon\quad 
S\uuu m=\sum\uuu{\nu=1}^{\nu=q}\, |a\uuu{p+\nu,m}|^2
\quad\colon\quad 
N=\sum\uuu{j=1}^{j=q} \sum\uuu{\nu=1}^{\nu=q}\, |a\uuu{p+j,p+\nu}|^2
\]





\noindent
{\bf{D.2.5 Theorem.}}\emph{
Let $D$ be the determinant of the matrix  (*). Then}
\[
|D|\leq (L\uuu 1\cdots L\uuu p) ^ {\frac{1}{p}}\cdot
\sqrt{M\uuu 1\cdots M\uuu p}\cdot
\frac{N^{\frac{q\vvv p}{2}}}{(q\vvv p)^{\frac{q\vvv p}{2}}}
\]


\noindent
\emph{Proof.}
After unitary transformations of the last $q$ rows and 
the last $q$ columns respectively, the proof is reduced to the case
when $a\uuu{jk}=0$ for pairs $(j.k)$ with  $j\leq p$ and $k>p+j$ or with
$k\leq p$ and $j>p+k$.
Here $L\uuu m, S\uuu m$ and $N$  are unchanged and we get
\[ 
D=(\vvv 1)^p\cdot \prod\uuu{j=1}^{j=p}\,
a\uuu{j,p+j}\cdot \prod\uuu{k=1}^{k=p}\,a\uuu {p+k,k}
\cdot 
\det \begin{pmatrix}
a\uuu{p+1,2p+1}&\ldots &a\uuu{p+1,p+q}\\
\ldots&
\ldots&\ldots
\\
a\uuu{p+q,p+1}&\ldots &a\uuu{p+q,p+q}\\
\end{pmatrix}
\]
\medskip


\noindent
The absolute value of the last determinant is
majorized by   Hadamard's inequality in § F.XX   and 
the requested inequality in Theorem D.2.5 follows.















\newpage









\centerline{\bf{The Stieltjes measure in ${\bf{R}}^2$.}}

\medskip

\noindent 
Modern integration theory
started in 1890
when Stieltjes introduced
measures on the closed real interval $[0,1]$.
More precisely, let $s(x)$ be a non-decreasing continuous function
on the unit  interval where 
$s(0)=0$ and $s(1)=1$.
Recall that every open subset $U$
of $[0,1]$ is a disjoint union of at most denumerably many
open intervals $\{\alpha_\nu,\beta_\nu)\}$. Among these we include
half-open intervals $[0,\beta)$ or $[\alpha,1]$.
Following Stieltjes we assign
the measure
\[
\mu_s(U) =\sum\, (s(\beta_\nu)-s(\alpha_\nu))
\]
For a closed, i.e. a compact subset
$K$ of $[0,1]$ we define
\[
\mu_s(K)= 1-\mu_s([0,1]\setminus K)
\]
The continuity of $s$ entails that
\[ 
\mu_s(K)= \lim_{\delta\to 0}\, \mu_s(K_\delta)\, \quad\colon K_\delta=\{x\colon\, dist(x,K)<\delta\}
\]
From this it follows that $\mu_s$ extends to a $\sigma$-additive measure
defined on the $\sigma$-algebra $\mathcal B$ of Borel sets, i.e. the smallest $\sigma$-algebra 
of subsets of $[0,1]$ generated by intervals.
To each continuous function $f(x)$
on $[0.1]$
there exists the Stieltjes integral
\[
\int_0^1\, f(x)d\mu_s\quad\colon f\in C^0[0,1]\tag{*}
\]
Using the \emph{uniform continuity} of $f$ one can compute (*) in a similar fashion as for the ordinary
Riemann integral where we use $s(x)= x$ so that
$\mu_s=dx$. More precisely if
$0=a_o<a_1<\ldots<a_N=1$
is a partion of the unit interval
one dedines the upper- respekctively the lower
Riemann sums
\[
I^*(a_\bullet)= \sum_{\nu=0}^{N-1} \, (a_{\nu+1}-a_\nu)\cdot
\max_{a_\nu\leq x\leq a_{\nu+1}}\, f(x)
\]
\[
I_*(a_\bullet)= \sum_{\nu=0}^{N-1} \, (a_{\nu+1}-a_\nu)\cdot
\min_{a_\nu\leq x\leq a_{\nu+1}}\, f(x)
\]
Then , if $\delta=\max (a_{\{nu+1}-a_\nu)$, it is  clear  that
\[
I^*(a_\bullet)- I_*(a_\bullet)\leq \omega_f(\delta)\tag{**}
\]
where we have put
\[ 
\omega_f(\delta)= \max\, |f(a)-f(b)\quad\colon \,
|a-b\leq \delta
\]
Since $\lim_{\delta\to 0}\, \omega_f(\delta)=0$
we conclude that the Riemann sums
which are constructed from partitions of $(0,1]$
converge to a limit
and gives the value of the integral in (').
\medskip


\noindent
{\bf{Another evaluation of (*)}}.
Suppose that the range of $f$ is
confined to
an interval $[0,M)$, i.e.
it is non-negative with  maximum norm $M$.
Now we define a non-decreasing function
\[
\rho_f(t)= \mu_s(\{f\leq t\})\quad\colon |,0\leq t\leq M
\]
Following Stieltjes we
define for every integer $k\geq 1$:
\[
\mathbf S_k(f)=
\sum_{\nu=0}^{\nu=xx}\,
\frac{\nu\cdot M}{k}\, \rho_f(\nu+1)-\rho_f(\nu)
\]














More generally there exist well-defined integrals
for every \emph{bounded Borel-function} $\phi$, i.e.
those real-valued functions for which
the sets
\[
\{\phi<a\}\in \mathcal B\quad\colon a\in {\bf{R}}
\]
A \emph{null set} with respect to $\mu_s$ is a set
such that
\[ 
0=\inf\, \mu_s(U)\colon\, A\subset U
\] 
where the infimum is taken over open sets
$U$ which contain $A$. This family of sets which are negligable with respect to $\mu_s$ is denoted by $\mathcal N(\mu_s)$.
Next, a
set $A$ is called  $\mu_s$-measurable if
its inner and outer measures are equal, i.e. if
\[ 
\sup\, \mu_s(K)=\inf\,\mu_s(U)\quad\colon K\subset A\subset U
\]
where $K$ denote compact sets and $U$ open sets.
One proves easily that
if $A$ is measurable then it can be expressed by a union:

\[ 
A= F^*\cup\,N\quad\colon\,  N\in \mathcal N(\mu_s)\quad\&\quad
F^*= \cup\, K_\nu
\]
where $F^*$ as indicated
is a denumerable union of an increasing sequence of
compact sets
$\{K_\nu\}$.
In a similar fashion one can  express $A$ by
\[
G_*\setminus N\quad \colon
N\in \mathcal N(\mu_s)\quad\&\quad
G_*= \cap\, U_\nu
\]
where $\{U_\nu\}$ is a decreasing sequence of open sets.



\medskip

\noindent
{\bf{A notation.}}
The family of all $\mu_s$-measurable sets is denoted by
$\mathfrak{M}(\mu_s)$.


\medskip

\noindent
\centerline {\bf{Vitali's differential theorem.}}
\medskip


\noindent
Let $A$ be a $\mu_s$-measurable set.
For each point $p\in A$ and every $\epsilon>0$ we put
\[
\delta_*(\epsilon,p)
=\inf\, \frac{\mu_s(A\cap\, (a,b))}{s(b)-s(a)}\quad\colon
a<p<b\quad\&\quad a-b<\epsilon
\]
\medskip

\noindent
Notice that above we compete with
\emph{all} open intervals which contain $p$
while we seek a minimum,  i.e. $p$
need not be the mid-point of $(a,b)$.
\medskip


\noindent
{\bf{Definition.}}
\emph{A point $p\in A$ is a point of density in the sense of Vitali if}
\[
\lim_{\epsilon\to 0}\, \delta_*(\epsilon,p)=1\tag{*}
\]
The set of points in $A$ for which (*) hold is denoted by
$\mathcal V(A)$ and called Vitali-points of $A$.


\medskip

\noindent
Using his famous \emph{Covering Lemma}, Vitali proved the following:
\medskip

\noindent
{\bf{Theorem.}}
\emph{For every $A\in \mathfrak{M}(\mu_s)$ it follows that}
\[ 
A\setminus \mathcal V(A)\in \mathcal N(\mu_s)
\]

\medskip


\noindent
The proof requires several steps.
Following Vitali we first give:
\medskip

\noindent
{\bf{Definition.}}
\emph{A family $\mathcal J$
of closed intervals is a Vitali-covering of $A$
if there for every point $a\in A$ and every $\epsilon>0$ exists some interval
$J\in \mathcal J$ which contains $a$ and has length}
$<\epsilon$.
\medskip


\noindent
{\bf{Vitali's covering theorem.}}
\emph{For every pair $A;\mathcal J$ as above there exists
a disjoint subfamily $J_1,J_2,\ldots$ from $\mathcal J$ such that
for every $N\geq 1$ 
one has the inclusion}
\[
A\subset\, \bigcup_{\nu=1}^{\nu=N}\, J_v\,\bigcup_{\nu>N} \, 3\cdot J_\nu
\]

\medskip

\noindent 
\emph{Proof.}
Let $\delta_1$ be the maximal length of intervals from $\mathcal J$. Pick $J_1$ so that
\[
|J_1|>\frac{2\cdot \delta_1}{3}
\]
If 
\[
A\setminus 3\cdot J_1=\emptyset\tag{i}
\]
 we are done, i.e. above we
just keep $J_1$ and need not further intervals.
If (i) does not hold we  let $\delta_2$ be the maximum length of intervals $\mathcal J$
for which $J_1\cap J=\emptyset$ and pick an interval $J_2$ for which
\[
J_1\cap J_2=\emptyset\quad\and\, |J_2>\frac{2\delta_2}{3}
\]
We can continue in this way and obtain a sequence of disjoint intervals $J_1,J_2,\ldots$
in $\mathcal J$ where
\[ 
|J_{n+1}|> \frac{2\delta_n}{3}\quad\colon\, n=1,2,\ldots
\]
while $\delta_n$ is the maximal length of intervals from $\mathcal J$
which are disjoint from the union $J_1\cup\ldots\cup J_n$.
\medskip

\noindent
There remains only to prove that
the family $\{J_n\}$
gives the requested inclusions in (*).
To see this we consider some fixed $N\geq 1$
and regard a point $p\in A$.
If $p$ already belongs to $J_1\cup\,\ldots\cup\, J_N$ we are done.
Otherwise we
find - by the property of a Vitali-covering - a
sufficently small interval $J_*\in \mathcal J$
such that
\[
p\in J_*\quad\&\quad J\cap\,m j_1\cap \ldots\cap J_N=\emptyset
\]
Next, we have the positive number
$|J_*|=\delta$
and from (xx) above
there exists some $M>N$ such that
$\delta_M<\delta$ from which we conclude that
the interval $J$ is  disjoint from the union 
\[
J_1\cup \dots \cup\, J_M\tag{i}
\]
Shrinking $M$ if necessary 
we may assume that (i) holds while
$J$ is disjoint from the smaller union 
\[
J_1\cup \dots \cup\, J_{M-1}\tag{ii}
\]
Now (ii) means that
\[ 
\delta<\delta_M
\]
and at the same time (i-ii) gives a point
\[ 
q\in J\cap\, J_M\tag{i-iii}
\]
Since $|J_M|> \frac{\delta_M}{3}$
it is seen by drawing a figure 
that (i-iii) togeher give
\[ 
J\subset 3\cdot J_M
\]
which proves that the arbitrarily chosen point $p\in A$ indeed belongs to
the union from (*) in Vitali's covbering theorem.













\medskip

\noindent
\centerline{\bf{A specific example.}}
\medskip


\noindent
A family of non-decreasing and continous functions was
constructed by Torsten C in his
monograph \emph{Sur les équations intégrales singulièrs a noyau réel et symétrique}.
It reveals while certain arithmetic properties can entail that
a continous non-decreasing function $s$ on $[0,1]$
fails to be
\emph{absolutely continuous} which
means that
$\mu_s$ contains a  called singular part, i.e. there exists
a  set $A$ such that 
$\mu_s(A)>0$ while $A$ has
Lebesgue measure zero.


\medskip

\noindent
{\bf{A general construction.}}
Let $N\geq 2$ be an integer. We find the denumerable $\mathcal D(N)$
 real numbers $y>0$ for which
there exists some integer $k\geq 1$ such that
\[
N^k\cdot y\in\mathbf Z_+
\]
When $y>0$ is oursude this set 
there exists a \emph{unique}
series expansion
\[ 
y= \sum_{nu=1}^\infty\, \frac{\sigma_y(\nu)}{N^\nu}\quad\colon 0\leq \sigma_y(\nu)
\]
where the set of all $\nu$ for which
$\sigma_y(\nu)\neq 0$ is infinite.
Let us now choose a sequence of positive real numbers
$\{\alpha_\nu\}$ such that
\[ 
\sum_{\nu=1}^\infty\, \alpha_\nu<+\infty\quad \& \quad
\alpha_n\geq (N-1)\cdot\sum_{\nu=n+1}^\infty\, \alpha_\nu\quad\colon n=1,2,\ldots
\]
Then we define the funmction $\psi(y)$
where $\psi(y)=0$ and
\[
\psi(y)=\sum_{\nu=1}^\infty\, \sigma_y(\nu)\quad \colon y\in {\bf{R}}_+\setminus \mathcal D(N)
\]
It is easily seen that the function
\[
y\mapsto  \psi(y)
\]
is pointwise continuous outside the set $\mathcal D(N)$.
Next, let us enumerate $\mathcal D(N)$, i.e.
we get a sequence of distinct numbers
$\eta_1,\eta_2,\ldots$. One can for example employ
the obvious
lexiographic order of the  rational numbers which belong to
$\mathcal D(N)$.
In the $(x,y)$-plane we can construct a continuous function
$\psi^*(y)$ by adding the horisontal  line
\[
y=\eta_p\quad\&\quad \psi(\eta_p-0)\leq x\leq \psi(\eta_p)
\]
and in this way get  a  curve
$\mathcal C$
 in the $(x,y)$-plane defined by the equation
 \[ 
 y=\phi(x)
 \]
 where $\phi(x)$ is continuous over the interval
 \[
 0\leq x\leq (N-1)\cdot \sum_{\nu=1}^\infty=b
 \]
 \medskip
 
 \noindent 
 It turns out that
 this $\phi$-function fails to be
 \emph{absolutely continuous}
 ad hence the distribution derivative
 $\phi'(x)$
 contain a singular part, i.e. a measure of positive mass carried by
 a null-set in the snse of Lebesgue.
 To see this we regard the Fourier transform
\[
 f(\xi)= \int_0^b\, e^{ix\xi}\, \frac{d\phi}{dx}(x)\, dx
\]
A straifgfirward calcykatiuioin wgiuch is left to the reader gives the formnula
\[
f(\xi)= \prod_{\nu=1}^\infty\, \frac{1-e^ {Ni\xi\alpha_\nu}}{N\cdot(1- e^{i\xi\alpha_\nu})}
\]

 
 
 
 














\newpage

\centerline{\bf{The passage to ${\bf{R}}^2$.}}


\noindent
Consider the closed square
\[
\square=\{(x,y)\colon\, 0\leq x,y\leq 1\}
\]
Let $s(x,y)\in C^0(\square)$ be doubly non-decreasing, i.e. for 
every freezed $y$ the function
$x\mapsto s(x,y)$is non-decreasing, and vice versa
$y\mapsto s(x,y)$ is non-decreasing while $x$ is freezed.
In addition 
\[ 
s(0,0)=0\quad\&\quad s(1,1)=1
\]
In order to construct the measaure $\mu_s$ we employ
\emph{dyadic grids},
Thus, to a positive integer $N$
we divide $\square$ into $2^{2N}$
many  squares
\[
\delta_N(p,q)= \{2^{-N}\cdot p\leq x\leq 2^{-N}(p+1)\,\&\,
2^{-N}\cdot q\leq y\leq 2^{-N}(q+1)\}
\]
where  $p,q$ run over integers from zero  to $2^N-1$.
\medskip


\noindent
Next, for every square with sides parallell to the coordinate axes
we define
\[ 
\mu_s(\square) =s(a^*,b^*)+s(a_*,b_*)- s(a^*,b_*)-s(a_*,b^*)
\]
where the square has its upper corner point at $(a^*,b^*)$ and its lower corner point at $(a_*.b_*)$.
\medskip

\noindent
Let us now consider an - arbitrary - closed subset $K$ of $\square$.
When $N\geq 1$ we denote by
$\mathcal D_N(K)$ the family of squares
$\delta_N(p,q)$ for which
\[
 \delta_N(p,q)\cap\, K\neq \emptyset
\]
Next, put 
\[
\rho_N(K)=
\sum^*\, \mu_s(\delta_N(p,q))\quad\,\quad \delta_N(p,q)\in \mathcal D_N(K)
\]
One checks that these $\rho$-numbers
\emph{decrease} as $N$ increases. Passing to the limit we put
\[ 
\mu_s(K) = \lim_{N\to \infty}\,\rho_N(K)\tag{1}
\]
\medskip

\noindent In a similar fashion we construct $\mu_s(U)$ for open sets $U$, i.e.
here
we put
\[
\rho_N(U) =\sum\, \mu_s(\delta_N(p,q))\quad\colon \,\quad \delta_N(p,q)\subset U
\] 
Thus, we add measures from  those cubes which
are contained in $U$.
It is clear that
$N\mapsto \rho_N(U)$ increase with $N$ and we put
\[
 \mu_s(U)=\lim\, \rho^*_N(U)\tag{2}
\]
\medskip

\noindent
Via (1-2) one easily verifies that
the measure $\mu_s$ extends to be $\sigma$-additive on $\mathcal B(\square)$.
and we also remark that the Vitali theorem from § 1 remains valid where
one now regard a point $p$ in a $\mu_s$-measurable set $A$
and take  limits of quotients
\[
\frac{\mu_s(\delta\,\cap A)}{\mu_s(\delta)}
\] 
where $\delta$ are small squares tending to the singleton set $\{p\}$.
\medskip

\noindent
{\bf{Remark.}}
 The construction of $\mu_s$ entails that when
 $f(x,y)$ is twice continously differentiable then
 \[ 
 \iint\, f\cdot d\mu_s=
 \iint \, \frac{\partial^2f}{\partial x\partial y}\cdot s(x,y)\,dxdy
\]
In other words, $d\mu_s$ is the second order mixed distribution derivative
$\partial_x\partial_y(s)$.
\medskip


\noindent
It goes withput saying that the construction for $n=2$ extends verbatim to
every $n\geq 3$.
Concerning the measures which
are found above we notice that
since $\{s(x,y)\}$ is continuous, it follows that
every vertical line $\{x=a\}$ or horiszontal line
$\{y=b\}$ are nullsets.
\emph{Conversely} every
$\sigma$-additive and non-negative measure $\gamma$ on
$\square$ for which this family of lines are null-sets
is equal to $\mu_s$ for a unique $s(x,y)$.
So  one has - at least essentially - recovered all probability measures on $\square$ via
the constructions perfomed above.
\medskip

\noindent
The ordinary planar Lebesgue measure arises when we take $s(x,y)=xy$.
Examples of singular measures arise if one for  regards
a pair of linear functions
\[
\ell_1(x,y)= ax+by\quad\, \ell_2(x,y)= a_2+b_2y
\]
where $a,b,c,d$ are positive constants
and
\[ 
s(x,y)=\max \ell_1(x,y),\ell_2(x,y)
\]
Then $\mu_s$ is supported by the intersection of $\square$ and the line
\[
ax+by-cx-dy=0
\]



\newpage


\centerline{\bf{Stokes integral formula.}}

\medskip


\noindent
For a given $n\geq 2$ we consider the family
$\mathcal D^1(n)$ of bounded and connected open
subsets $\Omega$ of ${\bf{R}}^n$
for which
the boundary
$\partial\Omega $ is of class $C^1$.
Thius means that for eevey boundary point $p$
there exists a small open ball $B$ cventered at $p$
and a diffentiable function $f(x)= f(x_1,\ldots,x_N)$
which is defined in $B$ and where
\[
 \Omega\,\cap B= \{f<0\}\,\quad \&\quad
\partial\Omega\,\cap B= \{f=0\} 
\]
Finally the gradient vector
\[ 
\nabla(f)(x)= (\partial f/\partial x_1,\ldots,\partial f/\partial x_n)\neq 0\tag{*}
\]
for all $x\in B$.
\medskip

\noindent
To simplify notations
a partial derivative of a $C^1$-function $u$
with respect to a chosen $x$-ccorinaste we use the notation 
\[ 
\partial_j(u)= \partial u/\partial x_j
\]
Next, from the local descriptionm of $\partial\Omega$
it follows that
this compact sdets is a - in general finite disjoint union - of
connected embedded hypersurfaces in ${\bf{R}}^n$.
Alobg every such
cinnected
component of $\partial \Omega$ we find
the \emph{outward normal vector}.
For example, locally it is found
for a pair $(B,f)$ as above, i.e
 then
 \[ 
 \bf{n}(p)= \frac{\nabla f(p)}{||\nabla f(p)||}\quad\colon\, p\in B\cap \partial\Omega
 \]
 Here we should recall that
 the right hand side is intrinsic, i.e.
 if $f$ is replaced by a similar funcvtyion $g$ in
 the ball $B$, then
 calculus teaches that one has the equality
 \[
  \frac{\nabla f(p)}{||\nabla f(p)||}=
 \frac{\nabla g(p)}{||\nabla g(p)||}\quad\, p\in B\cap \partial\Omega
 \]
 \medskip
 
\noindent
With notastyions as above we can now announce

\medskip

\noindent
{\bf{Theorem.}}
\emph{For each $C^1$-function $u(x)$
and every $1\leq j\leq n$ one has the equality:}
\[
\int_\Omega\, \partial_j(u)\cdot dX=
\int_{\partial\Omega}\, u\cdot \mathbf n_j\cdot dA
\]
\emph{where $dA$ is the area measure on $\partial \Omega$
and $\mathbf n_j$ the j:th componentt of the outer normal vector.}
\medskip


\noindent
The proof goes back - at least essentially - to
the device by Arkimedes who estaöbihed this theorem
for
piecewise linear bodies in ${\bf{R}}^3$.
The proof has two ingredients.
First we establish Stokes Therem for \emph{graphic domains}
which are of the form
\[ 
\Omega=\{x\,\colon\,0 < x_n<\phi(x')\quad\, \&\,x'\in \square\}
\]
where $\square$ is the open unit square in the
$(n-1)$-dimensional $x'=(x_1,\ldots,x_{n-1})$-space
and $\phi(x')$ is a $C^1$-function of the $x'$-variables.
Put
\[
\partial\Omega(+)=\{[x',x_n)\colon x'\in \square\,\&\quad x_n=\phi(x')\}
\] 
With these notatyions we shall prove that
\[ 
\int_\Omega\, \partial_j(f)\,dX=\int_{\partial\Omega(+)}\, f\cdot \mathbf n_j\cdot dA
\]
for every $C^1$-function $f(x',x_n)$ such that
\[ 
f(x',0)=0\quad\& \quad f(x',x_n)=xxx
\]






 
 
 
 
 












\noindent
{\bf{How the Arkimedes' - Stokes formula yields
a class of singular measures.}}
Consider a function $\phi(x)$ of class $C^1$
defined on the closed interval $0\leq x\leq 1$
where it takes positive values except at the end-points
when $\phi(0)=\phi(1)=1$.
In this situation one has
the following formula for every
$C^1$-function $f(x,y)$ in
$\bold R^2$:
\medskip

\[
\iint\, f'_x(x,y)\, dxdy= \int_\gamma\, f\cdot \bold n_x\, ds\tag{*}
\]
where $\gamma$ is the simple arc
defined by the equation
$y=\phi(x)$ having end-points at $0,0)$ and $1,0)$
and in the right hand side
$ds$ is the arc-lenght along $\gamma$ while
$\bold n_x$ is the $x$-component of the outer normal.
Recall the proof carried out by elementary calculus:
Namely, 
consider the function
\[ 
F(x)= \int_0^{\phi(x)}\, f(x,y)\, dy
\]
whose derivative becomes
\[
F'(x)=\int_0^{\phi(x)}\, f'_x(x,y)\, dy+ f(x,\phi(x))\cdot \phi'(x)
\]
Next, by the fundamental theorem of Calculus one has
\[
0=\int_0^1\, F(x)\, dx
\] 
where we used that 
$\phi(0)=\phi(1)=0$. Hence  the double integral in the
right hand side of (*) is equal to
\[
\int_0^1\, f(x,\phi(x))\cdot \phi'(x)\, dx
\]
and there rermains only to observe - via a drawn figure  ! -  that
\[
\phi'(x)\cdot dx= \bold n_x\cdot ds
\] 
hold along the $\gamma$-curve.
\medskip



\noindent
Via the -trivial use - of smooth partitions of the unity
the same proof as above extends to every $n\geq 2$. More precisely, let
$\Omega$ be a bounded open set in ${\bf{R^n}}$.


 

\newpage



\noindent
We shall review   a section in
Torsten Carleman's \emph{Grosse Vortrag}
at the IMU-congress in Zürich 1932 entitled
\emph{Sur la théorie des équation intégrales et ses applications.}
\medskip


\noindent
Consider a doubly indexed sequence of real numbers
$\{c_{pq}\colon 1\leq p,q<+\infty\}$.
For each fixed $p$ we
get the   linear operator $L_p$
which sends an infinite  vector
$\mathbf x=(x_1,x_2,\ldots)$ to the infinite vector 
$\bold y=(y_1,y_2,\ldots)$
where
\[ 
y_p=\sum_{q=1}^{\infty}\, c_{pq}x_q\colon\, p=1,2,\ldots\tag{*}
\]
The domain of definition for $L_p$
consists of vectors $\bold x$ for which
the series in the right hand side of (*) converges for every $p$.
In other words, for each $p$ there exists
\[
\lim_{N\to \infty}\, \sum_{q=1}^{q=N}\, c_{pq}x_q
\]
Notice that no extra conditions are imposed. We only require
convergence  of an additive series which need not be absolutely  convergent, i.e.
may occur that
\[
\sum_{q=1}^{\infty}\, |c_{pq}x_q|=+\infty
\]
Following Carleman we shall describe a
\emph{necessary and sufficient condition} 
on a vector $\bold y$
in order
that the system (*) has
solution $\bold x$ where each right hand side converges, i.e. 
\[
\bold x\in \mathcal D(L_p)\colon\, p=1,2,\ldots
\]

\medskip

\noindent
{\bf{An extra hypothesis.}}
From the start we assume that
$L_1,L_2,\ldots$ are linearly independent. This means that for
every integer $N\geq 1$
one cannot find
a non-zero $N$-tuple $(\xi_1,\ldots,\xi_N)$ such that
\[
\xi_1\cdot c_{1q}+\ldots +\xi_N\cdot c_{Nq}=0\quad\forall \, q=1,2,3,\ldots
\]

\medskip
$\mathcal R(m_1,\ldots,m_n)$
\noindent
To obtain  at Carleman's criterion for the solvability of (*)
we shall study
families of finitely many inequalities.
Given an infinite vector $\bold y$
and $n\geq 2$ we regard an $n$-tuple 
of strictly  increasing integers
$1\leq m_1<\ldots<m_n$ and put
\[
M= m_1+\ldots+m_n
\]
Now we seek
a finite vector $\bold x=(x_1,\ldots,x_M)$
for which the following
inequalities hold:

\[
\big|\sum_{q=1}^{\nu}\, c_{1q}\cdot x_q-y_1\big|\leq 1\colon \,m_1\leq \nu\leq m_1+m_2
\]
and for each $n\geq 3$ and every $1\leq p\leq n-1$
\[
\big|\sum_{q=1}^{\nu}\, c_{pq}\cdot x_q-y_p\big|\leq \frac{1}{n-1}\colon \,m_1+m_2+\ldots+m_{n-1}\leq \nu\leq m_1+m_2+\ldots+m_n
\]
\medskip


\noindent
Keeping $\bold y$ fixed and 
using the linear independence  of $\{L_p\}$ one can show that there
exists an integer $m^*$ such that 
when $m_1\geq m^*$
then there exists  at least one 
vector $(x_1,\ldots,x_M)$
for which the the inequalites above hold
and denote this non-empty family 
of $M$-vectors  by $\mathcal L(m_1,\ldots,m_n)$.
Now we put

\[
\mathcal R(m_1,\ldots,m_n)=
\min\, \sum_{\nu=1}^{m_1}\, x_\nu^2\tag{*}
\]
where the minmium is taken over $M$-vectors $\bold x\in \mathcal L(m_1,\ldots,m_n)$.

\medskip


\noindent
Thus, 
by elementary algebraic manipulations we obtain a denumerable family of quantities
\[ 
\mathcal R(m_1,\ldots,m_n)\quad\colon n=2,3,\ldots
\]
where the sole condition is that $m^*\leq m_1<\ldots <m_n$ 
hold for every $n$-tuple.
With these notations we
announce Carleman's criterion:
\medskip

\noindent
{\bf{Theorem.}}
\emph{The necessary and sufficient condition in order
that the system $L(\bold x)=\bold y$ has a solution $\bold x$
is that one can find an infinite  sequence of
strictly increasing intgers
$m^*\leq \mu_1<\mu_2<\ldots$
and a constant $C$ such that}
\[
\mathcal R(\mu_1,\ldots,\mu_r)\leq C\colon\,r=1,2,\ldots
\]

\bigskip


\noindent
{\bf{Remark.}}
Of course, even in quite
specific situations it is not
easy to check this  criterion and notice that
one is not concerned with eventual \emph{uniqueness}
of a solution. Conditions  on
the infinite matrix $\{c_{pq}\}$ in order that
there exists a vector $\bold X\in \mathcal D(L)$ for which
$L\bold x$ is the zero-vector appears to be
unknown.
\medskip


\noindent
\centerline{\bf{The Dagerholm series.}}
\medskip


\noindent
Let us give  an example to llustrate
that new phenomena in contrast to finite systems
can occur. At the same time
the specific system below illustrates
how subtle it can be to find a solution and to establish
uniqueness.
Consider the matrix 
\[ 
c_{pq}= \frac{1}{p-q}\quad \, p\neq q
\]
In contrast to the finite-dimensional  case it turns out that the homogenous system
\[
\sum_{p\neq q}\, \frac{x_q}{p-q}=\quad\colon p=1,2,\ldots\tag{*}
\]
has a unique solution $\bold x$ for which
the series
\[ 
\sum\,\frac{x_p}{p}=1
\]
\medskip

\noindent
The construction of the unique solution $\bold x$
was achieved by Carleman's formed student Karl Dagerholm
and published 1968
in [Dag].
The main burden in this construction relies  - as one should expect ! - upon analytic function theory.
and as pointed out by Carleman in his IMU-lecture
one may pay attention to
inifinite systems which arise via
a rational function of two variables 
\[
f(x,y)=\frac{a_0(x)+a_1(x)y+\ldots+a_n(x)y^n}
{b_0(x)+b_1(x)y+\ldots b_m(x)y^m}= \frac{A(x,y)}{B(x,y)}
\]
where $\{a_\nu(x)\}$ and $\{b_\nu(x)\}$ are polynomials
and consider 
the case when the $\bold c$-matrix is given
by
\[ 
c_{pq}= f(p,q)
\]
In this situation  solutions to
the corresponding
inifinite system of equations boils down to
find a meromorphic function
with a prescribed set of poles and
linear relations between
their residues.
To be precise, this relies upon 
the following:

\medskip

\noindent
Let 
$\xi\in \bold C$ 
be such that 
\[
a_n(\xi)\neq 0\,\&\, B(\xi,q)\neq 0\quad\colon q=1,2,\ldots\tag{1}
\]
\medskip

\noindent
{\bf{Theorem.}} \emph{Assume (1). Then, if $\bold x$ is an inifinite vector such that
the series}
\[
\Phi(\xi)= \sum_{q=1}^\infty\, f(\xi,q)\cdot x_q\tag{2}
\]
\emph{converges
there exists  a meromorpohic function in
${\bf{C}}$ which outside its set of poles is given by
the convergent seres}

\[ 
\Phi(z)= \sum_{q=1}^\infty\, f(\xi,q)\cdot x_q\tag{i}
\]
\medskip
For the special Dagerholm series
one has $m=$ and $a_0=1$.
Under the condition that the series 
\[ 
\sum\, \frac{x_q}{q}
\] 
converges it follows that 
\[
\Phi(z)= \sum\, \frac{x_q}{z-q}
\] 
yields a meromorphic function.
To
find a non-trivial homogenous
solution to (*)
therefore amounts to construct a sequence
$\{x_q\}$ such that the resulting 
meromorphic 
function $\Phi$ has poles confined to
the set of positive integers and at with prescribed 
residues 
$\{x_q\}$ so that (*) above hold.
\medskip

\noindent
{\bf{Remark.}}
An example where non-zero homogenous solutions
do not exist arises when one  regards the system
\[
\sum_{q=1}^\infty\, \frac{x_q}{p+aq}=0\quad\, p=1,2,\ldots
\]
where $a$ is  a positive  real number
or more generally a complex number
outside the negative real axis $(-\infty,0]$.
The reader is invited to find the proof using analytic function theory, or if
necessary consult [Dag].






\newpage


\centerline{\bf{Algebrans fundamentalsats}}

\bigskip


\noindent
Den säger att om
\[ 
P(z)= z^n+c_1z^{n-1}+\ldots+c_{n-1}z+c_n
\]
är ett polynom i av någon grad $n\geq 2$ där $\{c_\nu\}$ är komplexa tal så 
existerar $n$ många rötter där eventuella rötter kan uppträda.
 Det betyder att det existerar en faktorisering
\[
P(z)=\prod_{\nu=1}^{\nu=k}\,(z-\alpha_\nu)^{e_\nu}\quad \colon\, e_1+\ldots+e_k=n
\]
där $\alpha_1,\ldots,\alpha_k$ är polynomets distinkta rötter.
I fallet $k=n$ förekommer alltså $n$ enkla rötter.
Beviset fordrar
en "analytisk metodik" 
eftersom godtyckliga komplexa tal - precis som de reella - uppstår
via gränsprocesser från de
rationella talen.
Notera dock att om man lyckats visa att ett polynom av godtyckligt stort gradtal har
\emph{minst en rot}
så följer satsen via induktion över $n$,
\medskip


\noindent
Vi skall ge  ett bevis hämtat från Bernhard Riemanns doktorsavhandling
från 185x.
Till att börja med
påminner vi om
några elementöra formel från  differentialkalkyl.
I $(x,y)$-planet betraktas för ett givet $R>0$
den öppna cirkelskivan
\[
D(R)= \{ x^2+y^2<R^2\}
\]
vars rand är  cirkeln  $T(R)$ och innehåller punkter av formen
\[ 
x=R\cos\phi\,\&\, y=R\sin(\phi)\quad\colon 0\leq\phi\leq 2\pi
\]
Låt nu $u(x,y)$ vara en reellvärd funktion som är kontinuerligt deriverbar, dvs.
de två partiella derivatorna
\[ 
u'_x=\frac{\partial u}{\partial x}\,\quad\,
u'_=\frac{\partial u}{\partial y}
\] 
existerar som kontinuerliga funktioner.
Med dessa beteckningar gäller de två integralformlerna
\[
\iint_{D(R)}\, u'_x dxdy=
R\cdot \int_0^{2\pi}\, u(R\cos\phi,R\sin\,\phi)\cdot \cos(\phi)\, d\phi\tag{i}
\]
\[
\iint_{D(R)}\, u'_y dxdy=
R\cdot \int_0^{2\pi}\, u(R\cos\phi,R\sin\,\phi)\cdot \sin(\phi)\, d\phi\tag{ii}
\]
\medskip


\noindent
{\bf{Övning.}} Med polära koordinater $(r,\phi$ i $(x,y)$-planet betraktas funktionerna

\[
C_{m,k}(r,\phi)= r^m\cdot \cos\,k\phi\,\,\&\,\,
S_{m,k}(r,\phi)=  r^m\cdot sin\,k\phi\tag{iii}
\]
för alla par av positiva  heltal.
Via additionsformler för de trigonmetriska funktionerna följer att varje $u$ och $v$ som ovan kan 
approximeras likformigt - inklusive deras derivata av
ändliga linjära kombinationer av $C$-och $S$-funktionerna.
På så vis reduceras beviset av (i-ii) till
en av funktionerna från (iii)
och beviset för (i-ii) i detta specialfall lämnas tilll läsaren.
\medskip




\noindent
Betrakta nu
ett par av $C^1$-funktioner $u,v)$
och bilda den komplexvärda funktionen
\[ 
f=u+iv
\]
Antag  att man har likheten
\[
u'_x=v'_y\tag {*}
\]
Med (i) använd på $u$ och (ii) på $v$ följer att 
\[
R\cdot \int_0^{2\pi}\, \, u(R\cos\phi,R\sin\,\phi)\cdot \cos(\phi)\,d\phi=
R\cdot \int_0^{2\pi}\, \, v(R\cos\phi,R\sin\,\phi)\cdot \sin(\phi)
\, d\phi\tag{iv}
\]
\medskip


\noindent
Inför nu den välkända komplexa formeln
\[ 
e^{i\phi}=\cos\,\phi+ i\cdot \sin\,\phi
\]
och betrakta integralen
\[
R\cdot \int_0^{2\pi}\, f(Re^{i\phi})\cdot e^{i\phi}\,d\phi\tag{*}
\]
\medskip

\noindent
{\bf{Övning.}}
Visa att likheten (iv) medför att \emph{realdelen} hos
integralen (*) blir lika med noll.
Antag också att
paret $(u,v)$ är ett par där   både (*) samt ekvationen
\[
u'_y=-v'_x
\]
gäller. Visa att denna likhet medför att
\emph{imaginärdelen} hos (*) också blir noll.
\medskip


\noindent
{\bf{Slutsats.}} De två Cauchy-Riemann ekvationerna
\[
u'_x=v'_y\quad\, u'_y=-v'_x\implies
\int_0^{2\pi}\, f(Re^{i\phi})\cdot e^{i\phi}\,d\phi=0\tag{**}
\]

\medskip

\noindent
{\bf{Fallet då $f=\frac{p'(z)}{p(z)}$}}.
Betrakta ett polynom $p(z)$ av grad $n$ och antag att det saknar nollställen, dvs. att
$p(z)\neq 0$ för alla $z\in\bold C$
\medskip

\noindent
{\bf{Övning.}}
Visa med stöd av
vanliga regler för derivator i $x$ och $y$-variablerna att
om vi skriver
\[ 
u+iv= \frac{p'(z)}{p(z)}\implies u'_x=v'_y\,\quad \& \quad u'_y=-v'_x
\]
Alltså ger (**) att
för varje $R>0$ gäller 
\[
0=R\cdot \int_0^{2\pi}\, \frac{p'(Re^{i\phi})}{p(Re^{i\phi})}\, d\phi=0\tag {1}
\]
Detta  leder nu till
en motsägelse.
Ty läsaren kan konfirmera att man har gränsvärdeslikheten
\[
\lim_{R\to +\infty}\,\frac{R\cdot p'(Re^{i\phi})}{p(Re^{i\phi})}=n\quad\forall\,\, 0\leq\phi\leq 2\pi
\]
och därför kan integralen (1)inte förbli lika med noll för stora $R$.

























 












\newpage


\centerline{\bf{Camille Jordan's
normalform.}}
\bigskip


\noindent
I ett arbete från 1850 visade Camille
Jordan att när en linjär operator $T$ verkar
på ett ändligt dimensionellt
vektorum  $V$ där
en potens $T^N=0$, dvs. för varje vektor $\bold v\in \bold V$ gäller att
\[
T^N(\bold v)=0\tag{*}
\]
så följer att
$V$ är en direkt summa av cykliska $T$-invarianta delrum.
Mer precist följer av (*) att när $0\neq\bold v\in V$ så existerar
ett minsta heltal $ord(\bold v)$ så att
\[
T^{ord(\bold v)}(\bold v)=0\, \quad\&\,
T^{ord(\bold v-1)}(\bold v)\neq 0
\]
Speciellt är 
$ord(\bold v)=0$ om och endast om $T(\bold v)=0$.

\medskip

\noindent
{\bf{Övning.}}
Visa att
varje vektor  $\bold v$ genererar  ett delrum $\mathcal C(v)$ av $\mathbf{V}$
vars  dimension är 
$ord(\bold v)$ där  vektorerna
$\{\bold v,\ldots,T^{ord(\bold v)-1}$ ger en bas.

\medskip

\noindent
Med beteckningar som ovan skall vi nu bevisa:

\medskip


\noindent
{\bf{Teorem.}}
Vektorrummet emph{$\mathbf V$ är en direkt summa av cykliska delrum, dvs.
det existerar
en ändlig familj av vektorer
$\bold v_1,\ldots,\bold v_m$ så att}
\[ 
\mathbf  V=\bigoplus_{i=1}^{i=m}\, \mathcal C(\bold v_i)
\]
\medskip


\noindent
\emph{Bevis}.
Välj först en vektor
$\bold w^*$ där $ord(\bold w^*)$ är maximal, dvs. man har
\[
ord(\bold v)\leq ord(\bold w^*)
\,\quad \forall \,  \bold v\in V\tag{1}
\]
Sätt $M=ord(\bold w^*)$.
Om $\mathcal C(\bold w^*)=V$ är vi klara. Om strikt inklusion gäller
definieras för varje $\bold v\in V\setminus \mathcal C(\bold w^*)$
det reducerade talet
\[
ord_{\bold w^*}(\bold v)=
\min_k\, T^k(\bold v)\in \mathcal C(\bold w^*)
\]
Välj nu en vektor $\bold w_1$
vars reducerade ordningstal är maximalt.
Så om $m=ord_{\bold w^*}(\bold w_1)$ gäller
\[
T^m(\bold w_1)\in \mathcal C(\bold w^*)\quad\&\,
T^{m-1}(\bold w_1)\in V\setminus \mathcal C(\bold w^*)
\tag{1}
\]
Samtidigt har vi talet $ord(\bold w_1)$ som enligt valet av
$\bold w_1^*$ uppfyller
\[ 
ord(\bold w_1)\leq M\tag{2}
\]
I fallet då
\[ 
ord(\bold w_1)= ord_{\bold w^*}(\bold w_1)\tag{3}
\]
ser vi att
$\mathcal C(\bold w_1)\cap\,\mathcal C(\bold w^*)=\{0\}$
som ger en direkt summa
\[
\mathcal C(\bold w^*)\,\oplus\, \mathcal C(\bold w_1)
\]
Då strikt olikhet gäller i (2) sätter vi
\[ 
j=ord(\bold w_1)-m\geq 1
\]
ochj noterar att olikheten $ord(\bold w_1)\leq M$
ger
\[
j\leq M-m\tag{4}
\]
Enligt (1) kan vi  skriva
\[
T^m(\bold w_1)=
c_0\cdot \bold w^*+c_1\cdot T(\bold w^*)+\ldots+c_{M-1}\cdot T^{M-1}(\bold w^*)\tag{3}
\]
Nu är $T^{j+m}(\bold w_1)=0$
vilket ger
\[
c_0\cdot T^j(\bold w^*)+c_1\cdot T^{j+1}(\bold w^*)+\ldots+c_{M-1}\cdot T^{M-1+j}(\bold w^*)=0\tag{4}
\]
och olikheten (4) medför att 
\[
c_0=\ldots=c_{m-1}=0
\]
Det följer att 
\[ 
T^m(\bold w_1)\in T^m(\mathcal C(\bold w^*))
\]
Vi får alltså en   vektor
$\bold u\in \mathcal C(\bold w^*)$
där vi nu har likheten
\[
ord_{\bold w^*}(\bold w_1-\bold u)=m
\]
som ger 
en direkt summa
\[
\mathcal C(\bold w^*)\oplus\,\mathcal C(\bold w_1-\bold u)
\]
Notera också  att
detta delrum sammanfaller med delrummet som spänns av
$\mathcal C(\bold w^*)$ och $\mathcal C(\bold w_1)$.
Sätt 
\[
\mathbf w_2^*=\mathbf w_2-\mathbf u
\]
Vi kan fortsätta med samma metod och
induktivt  erhålles
en vektorer
$\bold w^*,\bold w^*_1,\ldots,\bold w^*_k$
så att vi med beteckningen
$ \bold w_0=\bold w^*$ har en 
direkt summa
\[
\mathbf V= \bigoplus_{j=0}^{j=k}\, \mathcal C(\bold w_j)
\]
där talen $ord(\bold w_j)$
avtar med växande $j$.
\medskip


\noindent
{\bf{Övning.}}
Uppdelningen av $\mathbf V$ i en direkt summa av cykliska delrum är
inte entydig.
Däremot
är dimensionerna hos de cykliska delrummen som
uppträder i Teorem xx
entydiga. Mer precist
gööler fölajde. Lät
\[ 
M= \dim(\mathbf V)
\]
och för varje $1\leq k\leq m$ låter vi
$\delta(k)$ beteckna antalet cykliska delrum i (*) som har dimensionen $k$.
Eftersom  (*) är en direkt summa gäller
likheten
\[
 \sum_{k=1}^{k=M}\, k\cdot \delta(k)=M
\]
Ett specialfall är att $\delta(1)= M$ vilket svarar mot att
ordningen hos varje nollskild vektor i$\mathbf V$
har ordning ett, dvs. $T$ är den triviala nolloperatorn.
Ett annat extremfall är då $delta(M)=1$ vilket
svarar mot att det finns en vektor $\mathbf v$
där
\[ 
\mathbf V= \mathcal C(\mathbf v)\quad\colon ord(\mathbf v)= M
\]
I det allmänna fallet
gäller antalet $\delta(1),\ldots,\delta(M)$ är entydigt fastlagda vid 
en uppdelning
(*) i Teorem xx.
\medskip


\noindent
För att bevisa entydighet betraktas
följande svit av delrum :
\[
\mathcal F_k= \mathcal N(T^k)\quad\, \colon\quad k=1,\ldots,M
\]
Uppenbart gäller
\[ 
\mathcal F(1)\subset\ldots \subset\mathcal F(M)=\mathfrak V
\]
där den icke avtagande sviten
av dessa delrum
existerar från start, dvs. innan $\mathfrak V$
delats upp i cykliska delrum.
Läsaren kan nu vidimera
att entydigheten hos heltalssviten  $\{\delta(k)\}$ följer av
likheterna
\[
\delta(1)= \dim\, \mathcal F(1)\quad\&\quad
2\cdot \delta(2)= \dim\mathcal F(2)-\dim\mathcal F(1)
\]
och så vidare. Med andra ord, för varje $3\leq k\leq M$
gäller
\[ 
k\cdot \delta(k)= 
\dim\mathcal F(k)-\dim\mathcal F(k-1)
\]




















\newpage















\medskip

Forts s. 40 !!!!
 till s. 86

RÄTTA TEXTEN till förordet !!!!
\medskip


\centerline{\bf{Förord}}


\bigskip

\noindent
Utvecklingen av datorer
har bidragit till 
att många problem
kan attackeras
via linearisering.  
Ett exempel är numeriska  lösningar till 
differentialekvationer
 som i allmännhet är   partiella, dvs, innehålla högre ordningens blandade derivator, som dessutom kan vara icke-linjära,
 Att "räkna med matriser"
 intar därför en central plats
 inom matematiken.
 De uppstår  för att beskriva linjära avbildningar mellan \emph{vektorrum} där vi 
 påminner om att ett vektorrum $\mathbf V$
 är en mängd vara element kallas vektorer
som bildar en  kommutativ grupp. Mer precist kan
 vektorer adderas där den kommutativa och den associativa lagen gäller, dvs.
man har likheterna:
 \[
 \mathbf u+\mathbf v= \mathbf v+\mathbf v\quad\&\quad
 (\mathbf u+\mathbf v )+\mathbf w=
 \mathbf u+
 (\mathbf v+\mathbf w)
 \]
 för alla val  av tre vektorer.
 Dessutom existerar en kommutativ kropp $K$ som kallas $\mathbf V$:s skalärkropp 
där varje $\alpha\in K$
 avbildar en vektor
 \[
  \mathbf v\mapsto \alpha\cdot\mathbf v
 \]
och följande räknelag gäller för varje vektorpar och varje par $\alpha,\beta$ i $K$:
\[ 
(\alpha+\beta)(\mathbf u+\mathbf v)=
\alpha\cdot \mathbf u+ \beta\cdot \mathbf u+
\alpha\cdot \mathbf v+ \beta\cdot \mathbf v
\]
Vidare finns en nollvektor $\mathbf o$ i $V$
och här gäller att
\[
\mathbf v+(-1)\cdot\mathbf v= \mathbf o
\]
\medskip


\noindent
Vi kommer främst att studera vektorrum vars
skalärkropp är de reella talen
$\mathbf R$ eller de komplexa talen 
$\mathbf C$.
Fallet då $K$ är kroppen $Q$ av rationella tal
har naturligtvis också intresse, främst för tillämpningar
inom
talteori där aritmetik står i förgrunden.
Teorin för vektorrum då $K$ är en kommutativ kropp med karakteristik $\neq 0$, dvs. där det finns ett primtal $p\geq 2$ så att
\[
p\alpha=0\quad\forall\, \alpha\in K
\]
kommer inte att diskuteras eftersom
många resultat för vektorrum över
$\mathbf R$ inte 
längre är giltiga varför det fordras andra metoder för att
behandla linjära ekvationer i positiv karakteristik.
\medskip


\noindent
Man kan
redovisa teorin för lösningar av linjära ekvationssystem utan
att
hänvisa till åskådliga 
geometriska bilder. Men
historiskt utvecklades matriskalkyl via stöd
av
analytisk geometri.
Så många resultat i de följande avsnitten
varvas med sitt geometriska innehåll.
Grundläggande resultat inom linjär algebra fullbordades under 1800-talet. Bland
dem som bidragit kan nämnas
Cauchy, Gauss, Hamilton, Cayley, Sylvester,
Jacobi, Kronecker samt  Frobenius, där den sistnämnde
 i arbeten under åren 1880 fram till 1910-talet
knöt samman
matriskalkyl med gruppteori genom
att till varje ändlig grupp införa
införa dess \emph{gruppalgebra}.
Till sist bör  Hermann Grassmann (1809-1877)
lyftas fram.
\medskip

\noindent
Hans banbrytande
arbete \emph{Ausdehnungslehre} från 1844
lade grunden till
många resultat som numera intar en central roll inom såväl
differentia - som algebraisk geometri och  även inspirerat 
utvecklingen inom algebraisk topologi.
Så några centrala avsnitt 
kommer att ägnas åt
vektorrummen




\noindent
We shall review   a section in
Torsten Carleman's \emph{Grosse Vortrag}
at the IMU-congress in Zürich 1932 entitled
\emph{Sur la théorie des équation intégrales et ses applications.}
\medskip


\noindent
Consider a doubly indexed sequence of real numbers
$\{c_{pq}\colon 1\leq p,q<+\infty\}$.
For each fixed $p$ we
get the   linear operator $L_p$
which sends an infinite  vector
$\mathbf x=(x_1,x_2,\ldots)$ to the infinite vector 
$\bold y=(y_1,y_2,\ldots)$
where
\[ 
y_p=\sum_{q=1}^{\infty}\, c_{pq}x_q\colon\, p=1,2,\ldots\tag{*}
\]
The domain of definition for $L_p$
consists of vectors $\bold x$ for which
the series in the right hand side of (*) converges for every $p$.
In other words, for each $p$ there exists
\[
\lim_{N\to \infty}\, \sum_{q=1}^{q=N}\, c_{pq}x_q
\]
Notice that no extra conditions are imposed. We only require
convergence  of an additive series which need not be absolutely  convergent, i.e.
may occur that
\[
\sum_{q=1}^{\infty}\, |c_{pq}x_q|=+\infty
\]
Following Carleman we shall describe a
\emph{necessary and sufficient condition} 
on a vector $\bold y$
in order
that the system (*) has
solution $\bold x$ where each right hand side converges, i.e. 
\[
\bold x\in \mathcal D(L_p)\colon\, p=1,2,\ldots
\]

\medskip

\noindent
{\bf{An extra hypothesis.}}
From the start we assume that
$L_1,L_2,\ldots$ are linearly independent. This means that for
every integer $N\geq 1$
one cannot find
a non-zero $N$-tuple $(\xi_1,\ldots,\xi_N)$ such that
\[
\xi_1\cdot c_{1q}+\ldots +\xi_N\cdot c_{Nq}=0\quad\forall \, q=1,2,3,\ldots
\]

\medskip
$\mathcal R(m_1,\ldots,m_n)$
\noindent
To obtain  at Carleman's criterion for the solvability of (*)
we shall study
families of finitely many inequalities.
Given an infinite vector $\bold y$
and $n\geq 2$ we regard an $n$-tuple 
of strictly  increasing integers
$1\leq m_1<\ldots<m_n$ and put
\[
M= m_1+\ldots+m_n
\]
Now we seek
a finite vector $\bold x=(x_1,\ldots,x_M)$
for which the following
inequalities hold:

\[
\big|\sum_{q=1}^{\nu}\, c_{1q}\cdot x_q-y_1\big|\leq 1\colon \,m_1\leq \nu\leq m_1+m_2
\]
and for each $n\geq 3$ and every $1\leq p\leq n-1$
\[
\big|\sum_{q=1}^{\nu}\, c_{pq}\cdot x_q-y_p\big|\leq \frac{1}{n-1}\colon \,m_1+m_2+\ldots+m_{n-1}\leq \nu\leq m_1+m_2+\ldots+m_n
\]
\medskip


\noindent
Keeping $\bold y$ fixed and 
using the linear independence  of $\{L_p\}$ one can show that there
exists an integer $m^*$ such that 
when $m_1\geq m^*$
then there exists  at least one 
vector $(x_1,\ldots,x_M)$
for which the the inequalites above hold
and denote this non-empty family 
of $M$-vectors  by $\mathcal L(m_1,\ldots,m_n)$.
Now we put

\[
\mathcal R(m_1,\ldots,m_n)=
\min\, \sum_{\nu=1}^{m_1}\, x_\nu^2\tag{*}
\]
where the minmium is taken over $M$-vectors $\bold x\in \mathcal L(m_1,\ldots,m_n)$.

\medskip


\noindent
Thus, 
by elementary algebraic manipulations we obtain a denumerable family of quantities
\[ 
\mathcal R(m_1,\ldots,m_n)\quad\colon n=2,3,\ldots
\]
where the sole condition is that $m^*\leq m_1<\ldots <m_n$ 
hold for every $n$-tuple.
With these notations we
announce Carleman's criterion:
\medskip

\noindent
{\bf{Theorem.}}
\emph{The necessary and sufficient condition in order
that the system $L(\bold x)=\bold y$ has a solution $\bold x$
is that one can find an infinite  sequence of
strictly increasing intgers
$m^*\leq \mu_1<\mu_2<\ldots$
and a constant $C$ such that}
\[
\mathcal R(\mu_1,\ldots,\mu_r)\leq C\colon\,r=1,2,\ldots
\]

\bigskip


\noindent
{\bf{Remark.}}
Of course, even in quite
specific situations it is not
easy to check this  criterion and notice that
one is not concerned with eventual \emph{uniqueness}
of a solution. Conditions  on
the infinite matrix $\{c_{pq}\}$ in order that
there exists a vector $\bold X\in \mathcal D(L)$ for which
$L\bold x$ is the zero-vector appears to be
unknown.
\medskip


\noindent
\centerline{\bf{The Dagerholm series.}}
\medskip


\noindent
Let us give  an example to llustrate
that new phenomena in contrast to finite systems
can occur. At the same time
the specific system below illustrates
how subtle it can be to find a solution and to establish
uniqueness.
Consider the matrix 
\[ 
c_{pq}= \frac{1}{p-q}\quad \, p\neq q
\]
In contrast to the finite-dimensional  case it turns out that the homogenous system
\[
\sum_{p\neq q}\, \frac{x_q}{p-q}=\quad\colon p=1,2,\ldots\tag{*}
\]
has a unique solution $\bold x$ for which
the series
\[ 
\sum\,\frac{x_p}{p}=1
\]
\medskip

\noindent
The construction of the unique solution $\bold x$
was achieved by Carleman's formed student Karl Dagerholm
and published 1968
in [Dag].
The main burden in this construction relies  - as one should expect ! - upon analytic function theory.
and as pointed out by Carleman in his IMU-lecture
one may pay attention to
inifinite systems which arise via
a rational function of two variables 
\[
f(x,y)=\frac{a_0(x)+a_1(x)y+\ldots+a_n(x)y^n}
{b_0(x)+b_1(x)y+\ldots b_m(x)y^m}= \frac{A(x,y)}{B(x,y)}
\]
where $\{a_\nu(x)\}$ and $\{b_\nu(x)\}$ are polynomials
and consider 
the case when the $\bold c$-matrix is given
by
\[ 
c_{pq}= f(p,q)
\]
In this situation  solutions to
the corresponding
inifinite system of equations boils down to
find a meromorphic function
with a prescribed set of poles and
linear relations between
their residues.
To be precise, this relies upon 
the following:

\medskip

\noindent
Let 
$\xi\in \bold C$ 
be such that 
\[
a_n(\xi)\neq 0\,\&\, B(\xi,q)\neq 0\quad\colon q=1,2,\ldots\tag{1}
\]
\medskip

\noindent
{\bf{Theorem.}} \emph{Assume (1). Then, if $\bold x$ is an inifinite vector such that
the series}
\[
\Phi(\xi)= \sum_{q=1}^\infty\, f(\xi,q)\cdot x_q\tag{2}
\]
\emph{converges
there exists  a meromorpohic function in
${\bf{C}}$ which outside its set of poles is given by
the convergent seres}

\[ 
\Phi(z)= \sum_{q=1}^\infty\, f(\xi,q)\cdot x_q\tag{i}
\]
\medskip
For the special Dagerholm series
one has $m=$ and $a_0=1$.
Under the condition that the series 
\[ 
\sum\, \frac{x_q}{q}
\] 
converges it follows that 
\[
\Phi(z)= \sum\, \frac{x_q}{z-q}
\] 
yields a meromorphic function.
To
find a non-trivial homogenous
solution to (*)
therefore amounts to construct a sequence
$\{x_q\}$ such that the resulting 
meromorphic 
function $\Phi$ has poles confined to
the set of positive integers and at with prescribed 
residues 
$\{x_q\}$ so that (*) above hold.
\medskip

\noindent
{\bf{Remark.}}
An example where non-zero homogenous solutions
do not exist arises when one  regards the system
\[
\sum_{q=1}^\infty\, \frac{x_q}{p+aq}=0\quad\, p=1,2,\ldots
\]
where $a$ is  a positive  real number
or more generally a complex number
outside the negative real axis $(-\infty,0]$.
The reader is invited to find the proof using analytic function theory, or if
necessary consult [Dag].






\newpage


\centerline{\bf{Algebrans fundamentalsats}}

\bigskip


\noindent
Den säger att om
\[ 
P(z)= z^n+c_1z^{n-1}+\ldots+c_{n-1}z+c_n
\]
är ett polynom i av någon grad $n\geq 2$ där $\{c_\nu\}$ är komplexa tal så 
existerar $n$ många rötter där eventuella rötter kan uppträda.
 Det betyder att det existerar en faktorisering
\[
P(z)=\prod_{\nu=1}^{\nu=k}\,(z-\alpha_\nu)^{e_\nu}\quad \colon\, e_1+\ldots+e_k=n
\]
där $\alpha_1,\ldots,\alpha_k$ är polynomets distinkta rötter.
I fallet $k=n$ förekommer alltså $n$ enkla rötter.
Beviset fordrar
en "analytisk metodik" 
eftersom godtyckliga komplexa tal - precis som de reella - uppstår
via gränsprocesser från de
rationella talen.
Notera dock att om man lyckats visa att ett polynom av godtyckligt stort gradtal har
\emph{minst en rot}
så följer satsen via induktion över $n$,
\medskip


\noindent
Vi skall ge  ett bevis hämtat från Bernhard Riemanns doktorsavhandling
från 185x.
Till att börja med
påminner vi om
några elementöra formel från  differentialkalkyl.
I $(x,y)$-planet betraktas för ett givet $R>0$
den öppna cirkelskivan
\[
D(R)= \{ x^2+y^2<R^2\}
\]
vars rand är  cirkeln  $T(R)$ och innehåller punkter av formen
\[ 
x=R\cos\phi\,\&\, y=R\sin(\phi)\quad\colon 0\leq\phi\leq 2\pi
\]
Låt nu $u(x,y)$ vara en reellvärd funktion som är kontinuerligt deriverbar, dvs.
de två partiella derivatorna
\[ 
u'_x=\frac{\partial u}{\partial x}\,\quad\,
u'_=\frac{\partial u}{\partial y}
\] 
existerar som kontinuerliga funktioner.
Med dessa beteckningar gäller de två integralformlerna
\[
\iint_{D(R)}\, u'_x dxdy=
R\cdot \int_0^{2\pi}\, u(R\cos\phi,R\sin\,\phi)\cdot \cos(\phi)\, d\phi\tag{i}
\]
\[
\iint_{D(R)}\, u'_y dxdy=
R\cdot \int_0^{2\pi}\, u(R\cos\phi,R\sin\,\phi)\cdot \sin(\phi)\, d\phi\tag{ii}
\]
\medskip


\noindent
{\bf{Övning.}} Med polära koordinater $(r,\phi$ i $(x,y)$-planet betraktas funktionerna

\[
C_{m,k}(r,\phi)= r^m\cdot \cos\,k\phi\,\,\&\,\,
S_{m,k}(r,\phi)=  r^m\cdot sin\,k\phi\tag{iii}
\]
för alla par av positiva  heltal.
Via additionsformler för de trigonmetriska funktionerna följer att varje $u$ och $v$ som ovan kan 
approximeras likformigt - inklusive deras derivata av
ändliga linjära kombinationer av $C$-och $S$-funktionerna.
På så vis reduceras beviset av (i-ii) till
en av funktionerna från (iii)
och beviset för (i-ii) i detta specialfall lämnas tilll läsaren.
\medskip




\noindent
Betrakta nu
ett par av $C^1$-funktioner $u,v)$
och bilda den komplexvärda funktionen
\[ 
f=u+iv
\]
Antag  att man har likheten
\[
u'_x=v'_y\tag {*}
\]
Med (i) använd på $u$ och (ii) på $v$ följer att 
\[
R\cdot \int_0^{2\pi}\, \, u(R\cos\phi,R\sin\,\phi)\cdot \cos(\phi)\,d\phi=
R\cdot \int_0^{2\pi}\, \, v(R\cos\phi,R\sin\,\phi)\cdot \sin(\phi)
\, d\phi\tag{iv}
\]
\medskip


\noindent
Inför nu den välkända komplexa formeln
\[ 
e^{i\phi}=\cos\,\phi+ i\cdot \sin\,\phi
\]
och betrakta integralen
\[
R\cdot \int_0^{2\pi}\, f(Re^{i\phi})\cdot e^{i\phi}\,d\phi\tag{*}
\]
\medskip

\noindent
{\bf{Övning.}}
Visa att likheten (iv) medför att \emph{realdelen} hos
integralen (*) blir lika med noll.
Antag också att
paret $(u,v)$ är ett par där   både (*) samt ekvationen
\[
u'_y=-v'_x
\]
gäller. Visa att denna likhet medför att
\emph{imaginärdelen} hos (*) också blir noll.
\medskip


\noindent
{\bf{Slutsats.}} De två Cauchy-Riemann ekvationerna
\[
u'_x=v'_y\quad\, u'_y=-v'_x\implies
\int_0^{2\pi}\, f(Re^{i\phi})\cdot e^{i\phi}\,d\phi=0\tag{**}
\]

\medskip

\noindent
{\bf{Fallet då $f=\frac{p'(z)}{p(z)}$}}.
Betrakta ett polynom $p(z)$ av grad $n$ och antag att det saknar nollställen, dvs. att
$p(z)\neq 0$ för alla $z\in\bold C$
\medskip

\noindent
{\bf{Övning.}}
Visa med stöd av
vanliga regler för derivator i $x$ och $y$-variablerna att
om vi skriver
\[ 
u+iv= \frac{p'(z)}{p(z)}\implies u'_x=v'_y\,\quad \& \quad u'_y=-v'_x
\]
Alltså ger (**) att
för varje $R>0$ gäller 
\[
0=R\cdot \int_0^{2\pi}\, \frac{p'(Re^{i\phi})}{p(Re^{i\phi})}\, d\phi=0\tag {1}
\]
Detta  leder nu till
en motsägelse.
Ty läsaren kan konfirmera att man har gränsvärdeslikheten
\[
\lim_{R\to +\infty}\,\frac{R\cdot p'(Re^{i\phi})}{p(Re^{i\phi})}=n\quad\forall\,\, 0\leq\phi\leq 2\pi
\]
och därför kan integralen (1)inte förbli lika med noll för stora $R$.

























 





\newpage










\newpage


\centerline{\bf{Camille Jordan's
normalform.}}
\bigskip


\noindent
I ett arbete från 1850 visade Camille
Jordan att när en linjär operator $T$ verkar
på ett ändligt dimensionellt
vektorum  $V$ där
en potens $T^N=0$, dvs. för varje vektor $\bold v\in \bold V$ gäller att
\[
T^N(\bold v)=0\tag{*}
\]
så följer att
$V$ är en direkt summa av cykliska $T$-invarianta delrum.
Mer precist följer av (*) att när $0\neq\bold v\in V$ så existerar
ett minsta heltal $ord(\bold v)$ så att
\[
T^{ord(\bold v)}(\bold v)=0\, \quad\&\,
T^{ord(\bold v-1)}(\bold v)\neq 0
\]
Speciellt är 
$ord(\bold v)=0$ om och endast om $T(\bold v)=0$.

\medskip

\noindent
{\bf{Övning.}}
Visa att
varje vektor  $\bold v$ genererar  ett delrum $\mathcal C(v)$ av $\mathbf{V}$
vars  dimension är 
$ord(\bold v)$ där  vektorerna
$\{\bold v,\ldots,T^{ord(\bold v)-1}$ ger en bas.

\medskip

\noindent
Med beteckningar som ovan skall vi nu bevisa:

\medskip


\noindent
{\bf{Teorem.}}
Vektorrummet emph{$\mathbf V$ är en direkt summa av cykliska delrum, dvs.
det existerar
en ändlig familj av vektorer
$\bold v_1,\ldots,\bold v_m$ så att}
\[ 
\mathbf  V=\bigoplus_{i=1}^{i=m}\, \mathcal C(\bold v_i)
\]
\medskip


\noindent
\emph{Bevis}.
Välj först en vektor
$\bold w^*$ där $ord(\bold w^*)$ är maximal, dvs. man har
\[
ord(\bold v)\leq ord(\bold w^*)
\,\quad \forall \,  \bold v\in V\tag{1}
\]
Sätt $M=ord(\bold w^*)$.
Om $\mathcal C(\bold w^*)=V$ är vi klara. Om strikt inklusion gäller
definieras för varje $\bold v\in V\setminus \mathcal C(\bold w^*)$
det reducerade talet
\[
ord_{\bold w^*}(\bold v)=
\min_k\, T^k(\bold v)\in \mathcal C(\bold w^*)
\]
Välj nu en vektor $\bold w_1$
vars reducerade ordningstal är maximalt.
Så om $m=ord_{\bold w^*}(\bold w_1)$ gäller
\[
T^m(\bold w_1)\in \mathcal C(\bold w^*)\quad\&\,
T^{m-1}(\bold w_1)\in V\setminus \mathcal C(\bold w^*)
\tag{1}
\]
Samtidigt har vi talet $ord(\bold w_1)$ som enligt valet av
$\bold w_1^*$ uppfyller
\[ 
ord(\bold w_1)\leq M\tag{2}
\]
I fallet då
\[ 
ord(\bold w_1)= ord_{\bold w^*}(\bold w_1)\tag{3}
\]
ser vi att
$\mathcal C(\bold w_1)\cap\,\mathcal C(\bold w^*)=\{0\}$
som ger en direkt summa
\[
\mathcal C(\bold w^*)\,\oplus\, \mathcal C(\bold w_1)
\]
Då strikt olikhet gäller i (2) sätter vi
\[ 
j=ord(\bold w_1)-m\geq 1
\]
ochj noterar att olikheten $ord(\bold w_1)\leq M$
ger
\[
j\leq M-m\tag{4}
\]
Enligt (1) kan vi  skriva
\[
T^m(\bold w_1)=
c_0\cdot \bold w^*+c_1\cdot T(\bold w^*)+\ldots+c_{M-1}\cdot T^{M-1}(\bold w^*)\tag{3}
\]
Nu är $T^{j+m}(\bold w_1)=0$
vilket ger
\[
c_0\cdot T^j(\bold w^*)+c_1\cdot T^{j+1}(\bold w^*)+\ldots+c_{M-1}\cdot T^{M-1+j}(\bold w^*)=0\tag{4}
\]
och olikheten (4) medför att 
\[
c_0=\ldots=c_{m-1}=0
\]
Det följer att 
\[ 
T^m(\bold w_1)\in T^m(\mathcal C(\bold w^*))
\]
Vi får alltså en   vektor
$\bold u\in \mathcal C(\bold w^*)$
där vi nu har likheten
\[
ord_{\bold w^*}(\bold w_1-\bold u)=m
\]
som ger 
en direkt summa
\[
\mathcal C(\bold w^*)\oplus\,\mathcal C(\bold w_1-\bold u)
\]
Notera också  att
detta delrum sammanfaller med delrummet som spänns av
$\mathcal C(\bold w^*)$ och $\mathcal C(\bold w_1)$.
Sätt 
\[
\mathbf w_2^*=\mathbf w_2-\mathbf u
\]
Vi kan fortsätta med samma metod och
induktivt  erhålles
en vektorer
$\bold w^*,\bold w^*_1,\ldots,\bold w^*_k$
så att vi med beteckningen
$ \bold w_0=\bold w^*$ har en 
direkt summa
\[
\mathbf V= \bigoplus_{j=0}^{j=k}\, \mathcal C(\bold w_j)
\]
där talen $ord(\bold w_j)$
avtar med växande $j$.
\medskip


\noindent
{\bf{Övning.}}
Uppdelningen av $\mathbf V$ i en direkt summa av cykliska delrum är
inte entydig.
Däremot
är dimensionerna hos de cykliska delrummen som
uppträder i Teorem xx
entydiga. Mer precist
gööler fölajde. Lät
\[ 
M= \dim(\mathbf V)
\]
och för varje $1\leq k\leq m$ låter vi
$\delta(k)$ beteckna antalet cykliska delrum i (*) som har dimensionen $k$.
Eftersom  (*) är en direkt summa gäller
likheten
\[
 \sum_{k=1}^{k=M}\, k\cdot \delta(k)=M
\]
Ett specialfall är att $\delta(1)= M$ vilket svarar mot att
ordningen hos varje nollskild vektor i$\mathbf V$
har ordning ett, dvs. $T$ är den triviala nolloperatorn.
Ett annat extremfall är då $delta(M)=1$ vilket
svarar mot att det finns en vektor $\mathbf v$
där
\[ 
\mathbf V= \mathcal C(\mathbf v)\quad\colon ord(\mathbf v)= M
\]
I det allmänna fallet
gäller antalet $\delta(1),\ldots,\delta(M)$ är entydigt fastlagda vid 
en uppdelning
(*) i Teorem xx.
\medskip


\noindent
För att bevisa entydighet betraktas
följande svit av delrum :
\[
\mathcal F_k= \mathcal N(T^k)\quad\, \colon\quad k=1,\ldots,M
\]
Uppenbart gäller
\[ 
\mathcal F(1)\subset\ldots \subset\mathcal F(M)=\mathfrak V
\]
där den icke avtagande sviten
av dessa delrum
existerar från start, dvs. innan $\mathfrak V$
delats upp i cykliska delrum.
Läsaren kan nu vidimera
att entydigheten hos heltalssviten  $\{\delta(k)\}$ följer av
likheterna
\[
\delta(1)= \dim\, \mathcal F(1)\quad\&\quad
2\cdot \delta(2)= \dim\mathcal F(2)-\dim\mathcal F(1)
\]
och så vidare. Med andra ord, för varje $3\leq k\leq M$
gäller
\[ 
k\cdot \delta(k)= 
\dim\mathcal F(k)-\dim\mathcal F(k-1)
\]





\newpage












RÄTTA Varianten !!!


\newpage

\medskip


\centerline{\bf{Förord}}


\bigskip

\noindent
Utvecklingen av datorer
har bidragit till 
att många problem
kan attackeras
via linearisering.  
Ett exempel är numeriska  lösningar till 
differentialekvationer
 som i allmännhet är   partiella - dvs. innehålla högre ordningens blandade derivator - samt 
 dessutom kan vara icke-linjära.
 Att "räkna med matriser"
 intar därför en central plats
 inom matematiken.
 Matriser  uppstår  för att beskriva linjära avbildningar mellan \emph{vektorrum} där vi 
 påminner om att ett vektorrum $\mathbf V$
 är en mängd vara element kallas vektorer
som bildar en  kommutativ  grupp. Mer precist kan
 vektorer adderas där den kommutativa och den associativa lagen gäller, dvs.
man har likheterna:
 \[
 \mathbf u+\mathbf v= \mathbf v+\mathbf v\quad\&\quad
 (\mathbf u+\mathbf v )+\mathbf w=
 \mathbf u+
 (\mathbf v+\mathbf w)
 \]
 för alla val  av tre vektorer.
 Dessutom existerar en kommutativ kropp $K$ som kallas $\mathbf V$:s skalärkropp 
där varje $\alpha\in K$
 avbildar en vektor
 \[
  \mathbf v\mapsto \alpha\cdot\mathbf v
 \]
och följande räknelag gäller för varje vektorpar och varje par $\alpha,\beta$ i $K$:
\[ 
(\alpha+\beta)(\mathbf u+\mathbf v)=
\alpha\cdot \mathbf u+ \beta\cdot \mathbf u+
\alpha\cdot \mathbf v+ \beta\cdot \mathbf v
\]
Vidare finns en nollvektor $\mathbf o$ i $V$
och här gäller att
\[
\mathbf v+(-1)\cdot\mathbf v= \mathbf o
\]
\medskip


\noindent
Vi kommer främst att studera vektorrum vars
skalärkropp är de reella talen
$\mathbf R$ eller de komplexa talen 
$\mathbf C$.
Fallet då $K$ är kroppen $Q$ av rationella tal
har naturligtvis också intresse, främst för tillämpningar
inom
talteori där aritmetik står i förgrunden.
Teorin för vektorrum då $K$ är en kommutativ kropp med karakteristik $\neq 0$, dvs. där det finns ett primtal $p\geq 2$ så att
\[
p\alpha=0\quad\forall\, \alpha\in K
\]
kommer inte att diskuteras eftersom
många resultat för vektorrum över
$\mathbf R$ inte 
längre är giltiga varför det fordras andra metoder för att
behandla linjära ekvationer i positiv karakteristik.
\medskip


\noindent
Man kan
redovisa teorin för lösningar av linjära ekvationssystem utan
att
hänvisa till 
geometriska bilder. Men
historiskt utvecklades matriskalkyl via stöd
av
analytisk geometri.
Så många resultat i de följande avsnitten
varvas med  geometriskt innehåll.
Grundläggande resultat inom linjär algebra fullbordades under 1800-talet. Bland
dem som bidragit kan nämnas
Cauchy, Gauss, Hamilton, Cayley, Sylvester,
Jacobi, Jordan, Kronecker samt  Frobenius, där den sistnämnde
 i arbeten under åren 1880 fram till 1910-talet
knöt samman
matriskalkyl med gruppteori genom
att till varje ändlig grupp införa
införa dess \emph{gruppalgebra}.
\medskip

\noindent
Till sist bör  Hermann Grassmann (1809-1877)
lyftas fram.
Hans banbrytande arbete \emph{Ausdehnungslehre} från 1844
lade grunden till
många resultat som numera intar en central roll inom såväl
differential - som algebraisk geometri och  även bidragit till 
utvecklingen av  algebraisk topologi och   homologisk algebra
där vektorrum ersättes med moduler
över associativa ringar $R$ som i allmännhet inte behöver vara kommutativa
och vars 
nollskilda element inte säkert är inverterbara.





\medskip


\noindent
Grassmann
införde  
vektorrummen
$\{\Lambda^k(\mathbf R^n)\}$
som för varje $1\leq k\leq n$
har dimension lika med $\binom {n}{k}$
där en bas ges av yttre  produkter
av
de euklidiska basvektorerna $\mathbf e_1,\ldots,\mathbf e_n$.
Mer precist är
\[ 
\Lambda^k(\mathbf R^n)= \oplus\, \mathbf R\cdot e_{i_1}\wedge\ldots\wedge \mathbf e_{i_k}\tag{*}
\]
där den direkta summan tas över alla $1\leq i_1<\ldots <i_k\leq n$.
Dessutom gäller den alternerande teckenregeln. Då $k=2$ gäller t.ex. 
\[
\mathbf e_i\wedge \mathbf e_\nu= -\mathbf e_\nu\wedge \mathbf e_i\quad\, i\neq \nu
\]
\medskip


\noindent
I avsnitt XXX beskrivs hur en $k$-tupel av linjärt oberoende vektorer
$\mathbf v_1,\ldots,\mathbf v_k$
ger en vektor
\[
\mathbf v_1\wedge\,\ldots \wedge\, \mathbf v_k
\]
i $\Lambda^k(\mathbf R^n)$ och om 
$\mathbf u_1,\ldots,\mathbf u_k$
är en annan $k$-tupel som genererar samma delrum i $\mathbf R^n$ som
$\mathbf v$-vektorerna
gäller likheten
\[
\mathbf u_1\wedge\,\ldots \wedge \, \mathbf u_k=\det B\cdot
\mathbf v_1\wedge\,\ldots \wedge \,\mathbf v_k
\] 
där
$\det B$  är determinanten hos den kvadratiska matrisen $B$
vars element ges av
\[ 
\mathbf u_p= \sum_{p=1}^{p=k}\, b_{pq}\cdot \mathbf v_q\quad\, 1\leq p\leq k
\]
\medskip

\noindent
Med stöd av dessa yttre produkter
kan man  sedan analysera  familjen av  $k$-dimensionella delrum i
$\mathbf R^n$ där varje sådant delrum motsvarar en
punkt i en projektiv algebraisk mångfald 
betecknad med $Grass(k,n)$. Dessa Grassmann-mångfalder
utnyttjas
för att exempelvis härleda
volymformler  hos $k$-dimensionella simplex
vilket sker i avsnitt XXX.
Här bör tilläggas att
linjär algebra främst  handlar
om att \emph{linearisera}
objekt av mer komplicerad natur som inbäddade
delmångfalder
i ${\bf{R}}^n$ eller studier av polynomekvationer 
som ingår i den algebraiska geometrin.
Att numeriskt lösa stora linjära system av ekvationer
har naturligtivs praktisk betydelse medan
teoretiska resultat syftar till
att
framställa samband med
"symboliska termer".

\medskip


\noindent
Ett exempel på
symbolisk analys
är det vackra arbetet av Maclaurin från 1724.
Han utgick från den 6-dimensionella skaran av
trianglar i $(x,y)$-planet där en enskild triangel bestäms av 
sin area och dess mittpunkt, dvs. medianernas
skärningspunkt, samt två hörnvinklar och till
sist
en rotation.
en 
I rummet av alla
tringlar kan man
nu introduera 5 bindande villkor genom
att
välja två korsande linjer $A,B$ samt
tre punkter
$p,q,r$ i $(x,y)$-planet.
Man erhåller då familjen
$\mathbf T(A,B,p,q,r)$
som består av trianglar $\Delta$ för vilka ett av dess hörin ligger på
linjen $A$ och ett annat på $B$. Vidare
skall
var och en av punkterna $p,q,r$ korsas av
en
av triangelns
sidor.
Maclaurin visade nu att orten
för
trianglar i denna klass svarar mot
en algebraisk 2:a gradskurva i fem variabler  
vilkas koefficienter bestäms av de varierande
storheterna
$A,B,p,q,r)$.
Detta resultat gav startskottet för en av
den linjära algebrans centrala
tillämpningar som
där man numera talar om modulirum
som studeras i högre  dimensioner där
man exempelvis söker klassificera
alla glatta projektiva algebraiska mångfalder över de komplexa talen


























\bigskip
\noindent
Grassmanns konstruktioner
utnyttjas  i differentialgeometri  där
yttre produkter av differentialformer på mångfalder
som infördes omkring 1890 av Elie Cartan spelar en central roll.
Han studerade 
även 
lösningar
av 
första ordningens integrabla 
differentialsystem där man
för varje 
 par av positiva heltal $n,N$ betraktar 
en $n$-tupel  av $(N,N)$-matriser
$\{ A^{(k)}\}$ vars  element 
är
formella potensserier
i $n$ variabler $x_1,\ldots,x_n$, dvs. de tillhör den lokala ringen
$\mathbf C[[ x_1,\ldots,x_n]]$.
Dessa $A$-matriser bildar ett integrabelt  system när man har likheterna 
\[ 
\partial_k(A^{j)})+ A^{(j)}\cdot A^{(k)}=
\partial_j(A^{(k)})+ A^{(k)}\cdot A^{(j)}\quad\colon\quad 1\leq j,k\leq n\tag{*}
\]
där $\partial_j(B)$ för en $(N,N)$-matris $B$
med element i 
$\mathbf C[[ x_1,\ldots,x_n]]$
erhålles genom att bilda den partiella derivatan med avseende på $x_j$
hos varje element i $B$-matrisen.
Med dessa beteckningar medför 
(*) att
det existerar en unik
matris $\Phi$ med element i den lokala potensserie-ringen sådan att
\[ 
\partial_j(\Phi)= A^{(j)}\quad\colon 1\leq j\leq n
\]
varvid konstant-termerna i $\Phi$-matrisens
element
är enhetsmatrisen $E_N$.

\medskip


\noindent
Inom 
ramen för
symplektiska former
påminner vi också om att kotangentbunten $T^*(M)$
över en  mångfald
$M$ har en kanonisk  symplektisk struktur.
Ett från övrig text separat avsnitt presenterar ett 
bevis av O.  Gabber
som i ett arbete från 1980
via elementär  - men synnerligen  snillrik ! -
matriskalkyl på ändligt-dimensionella vektorrum
bevisade att
den karakteristiska mängden som hör till
varje - i allmännhet överbestämt - system av
partiella differentialekvationer med analytiska koefficienter
är en involutiv. dvs. koisotrop -  delmängd av kotangentrummet.

\bigskip

\noindent

\centerline{\bf{En datoruppgift.}}
\medskip


\noindent
I Torsten Carlemans verk \emph{xxxx}
förekommer  bland mycket annat
ett rikt material följande från kapitlet i [ibid] som
ägnas åt ett djupt studium av kedjebråk och
deras - eventuella - konvergens.
Resultatet nedan ingår i Carlemans analys och
den intresserade läsaren kan på egen hand 
"testa datorn" genom att
för relativt stora $n$ i satsen och några - slumpvist - insatta numeriska värden på
de storheter som  förekommer i teoremet
ge närmevärden till
teoremets  utsaga.




\medskip

\noindent
\emph{Låt $n\geq 2$ och $A_1,\ldots,A_n$  en $n$-tupel av positiva tal samt
$0 < t_1<\ldots < t_m$ en strikt växande följd av reella tal.
Sätt}
\[
\mathbf A= \sum_{\nu=1}^{\nu=n}\, A_\nu\quad\&\quad
a=\mathbf A^{-1}\cdot \sum_{nu=1}^{\nu=n}\, t_\nu\cdot A_\nu
\]
\emph{samt definiera polynomet}
\[
P(x)= \sum_{\nu=1}^{\nu=n}\, \frac{A_\nu}{t_\nu-x}
\]

\medskip

\noindent
Med dessa beteckningar
gäller följande:

\medskip


\noindent
{\bf{Teorem.}}
\emph{Det existerar en unik $(n-1)$-tupel
av strikt växande tal $0<\tau_1<\ldots<\tau_{n-1}$
där}
\[
t_1<\tau_1<t_2<\tau_2<\ldots < t_{n-1}<\tau_{n-1}<t_n
\]
\emph{samt positiva tal $B_1,\ldots,B_{n-1}$ så att}
\[ 
P(x)= \frac{\mathbf A}{a-x-Q(x)}\quad\&\quad Q(x)= \sum_{\nu=1}^{\nu=n-1}\,
\frac{B_\nu}{\tau_\nu-x}
\]
\emph{Vidare gäller följande likheter för varje $1\leq p\leq n-1$}:
\[
\mathbf A= B_p\cdot \sum_{\nu=1}^{\nu=n}\, \frac{A_\nu}{(t_\nu-\tau_p)^2}
\]
\medskip


\noindent
{\bf{Anmärkning.}}
Satsen ovan
ger en inversion till
det välkända resultatet av Lagrange som
visar att om man har en rationell funktion
\[
F(x)= \frac{q(x)}{p(x)}
\]
där $p(x)$ är ett polynom av grad $n$
med $n$ stycken enkla reella rötter $t_1,\ldots,t_n$
och $q(x)$ ett polynom av grad $n-1$ så följer att
det existerar
en unik $n$-tupel  av reella tal $\{A_\nu\}$ - som alltså beror av
$p$:s rötter samt $q$-polynomets koefficienter så att
\[ 
F(x)=
\sum_{\nu=1}^{\nu=n}\, \frac{A_\nu}{x-t_\nu}
\]










\newpage


{\bf{Mekanik och linjär algebra.}}
Utmananade och samtidigt både 
realistiska och åskådliga exempel uppträder inom mekanik
där
linjär algebra kopplas till
differentialekvationer.
Författaren till
denna text hoppas vid ett senare tillfälle kunna
fylla ut det följande ganska sterila
innehållet med
exempel som
behandlar stela kroppars rörelse-ekvationer.
Själv har jag alltid fascinerats av
ekvationer som förekommer i   Sir William Thomsons  verk
\emph{The Treatise of  Natural Philosophy} från 1867. Där behandlas
bland annat  läran om fasta kroppars rotation
under påverkan av friktionskrafter vilket innebär  att man måste behärska så kallade holonoma system och
arbeta
med två - och ibland tre - olika
koordinatsystem för att härleda
icke konservativa rörelse-ekvationer medan en del av den kinetiska energin går förlorad på grund av friktionskrafter.
Här följer ett citat från den synnerligen läsvärda
läroboken \emph{Teknisk Mekanik} av 
Hjalmar Tallquist:
\medskip

\noindent
\emph{En illustration  på
den tvungna procession som
förekommer vid gyroskop
ges av en homogen
rotationsellipsoids rotation på ett horisontalt plan med friktion där
en frågeställning är under vilka villkor den reser sig upp sig i stående position.
Man kan göra experiment med ett hårdkokt ägg på en  bordsyta.
Problemet om snurrans uppresning löstes av William Thomson
som givit många
bidrag kring 
om fasta kroppars rotation och
deras tillämpningar.  I sin ungdom lär han tillbringat
en längre tid vid havskusten i Skottland
under förberedelse till
en examen då han roade sig med
att bringa alla möjliga rundslipade stenar
att dansa. En sådan kropps
resande på ända
under rotation åtföljes av häftiga  vaggande rörelser - så kallade nutationer -
som åter lugna sig då kroppen
rest sig upp !}.





\medskip




\noindent
\centerline{\bf{Kort om textens innehåll}}

\medskip																																																														
\noindent																																																																																																																					 
																																																																																																											
																																																																															
Vi arbetar i 																										arbetar i ${\bf{R}}^n$ vars punkter har koordinater som svarar mot 
en $n$-vektor
$\bold x=(x_1,\ldots,x_n)$.
Det  euklidiska avståndet mellan
två punkter givna av spetsarna hos två $n$-vektorer
$\bold x$ och $\bold y$
definieras av
\[ 
||\mathbf  x-\mathbf y||=\sqrt{(x_1+y_1)^2+\ldots+(x_n-y_n)^2}\tag{*}
\]
När $n=2$ svarar (*)
\emph{Pythagoras Teorem}, dvs. att hypotenusans
 kvadratiska längd är summan
av de kvadratiska längderna  kateterna i en
rätvinklig triangel.
Den \emph{inre produkten} mellan ett par av $n$-vektorer
definieras 
av
\[ 
\langle \mathbf x,\mathbf y\rangle=
x_1y_1+\ldots+x_ny_n\tag{*}
\]
\emph{Linjära avbildningar}
kan framställas med kvadratiska $(n,n)$-matriser vars $n^2$ många element är
reella tal.
Detta leder till
en rad intressanta - och ofta intrikata - resultat om
algebran $M_n(\mathbf R)$ vars element är  $(n,n)$-matriser $A$ som
representerar linjära avbildningar $\mathbf A$
från
$\mathbf R$ till sig själv. Här motsvarar
produkten $AB$ av två matriser 
den \emph{sammansatta} linjära avbildningen
$\mathbf A\circ \mathbf B$.
En $(n,n)$-matris $S$ kallas \emph{ortogonal} om
$\mathbf S$ bevarar euklidiska
avstånd, dvs. när
\[ 
||\bold S(\mathbf x)-\mathbf S(\mathbf y)||=
||\mathbf x-\mathbf y)||
\]
gäller för varje par av vektorer.
Ett fundamentalt resultat är att om $F$ är en
\emph{avståndsbevarande}
avbildning, dvs. om
\[
||F(\mathbf x)-\mathbf F(\mathbf y)||=
||\mathbf x)-\mathbf y)||
\]
gäller för alla vektorpar, så existerar
en
unik punkt
$\mathbf p\in \mathbf R^n$ och
en ortogonal matris $S$ så att
\[ 
F(\mathbf x)=\mathbf p+ \mathbf S(\mathbf x-\mathbf p)\tag{*}
\]
gäller för alla $\mathbf x$.
Beviset för (*)  ges i § XXX.
\medskip


\noindent
I avsnitt XX studeras
$k$-dimensionella  simplex som  
erhålles via en $k$-tupel av linjärt oberoende $n$-vektorer
$\mathbf u_1,\ldots,\mathbf u_k$
vilka  generar ett $k$-simplex betecknat med
$\Sigma(\mathbf u_1,\ldots,\mathbf u_k)$
och består av punkter
\[
s_1\mathbf u_1+\ldots+s_k\mathbf u_k\quad\colon \, s_1+\ldots+s_k=1\,\&\, s_\nu\geq 0
\]
I § XXX
bevisas formler  som
uttrycker den $k$-dimensionella
volymen hos
ett sådant simplex
via
$\bold u$-vektorernas koordinater.
Detta sker genom att
införa
en norm på vektorrummet
$\Lambda^k(\mathbf R^n)$
som ger likheten
\[
\text{Vol}_k(\Sigma(\mathbf u_1,\ldots,\mathbf u_k))=
||\mathbf u_1\wedge\ldots\wedge \mathbf u_k||\tag{*}
\] 
där högerledet betecknar  normen hos
den $\Lambda^k$-vektor som bildas av
$\mathbf u$-vektorerna. Likheten visas
med hjälp av 
\emph{Grams matriser}
där (*) är lika med
determinanten hos $\mathbf u$-vektorernas 
symmetriska $(k,k)$-matris
\[
Gr(\mathbf u_\bullet)=
\]




\medskip


\noindent
I § XXX visas 
den \emph{multilinjära versionen av
Pythagoras teorem} som säger att man har likheten
\[
\text{Vol}_k(\Sigma(\mathbf u_1,\ldots,\mathbf u_k))^2=
\sum\, \text{Vol}_k(\Pi_J(\Sigma(\mathbf u_1,\ldots,\mathbf u_k))^2
\] 
där summan i högerledet
tagits över
de $\binom {n}{k}$  många projektionerna som projicerar  det givna $k$-simplexet i
$\mathbf R^n$  till
simplex i de $k$-dimensionella
rummen som uppstår för varje $k$-tuppel
\[ 
J= (j_1,\ldots,j_k)\quad\colon 1\leq j_1<\ldots<j_k\leq n
\]

\newpage


\noindent
{\bf{Determinanter}}.
Till varje $(n,n)$-matris $A$ hör dess determinant
som betecknas med $\det(A)$.
Dess konstruktion ges i § xx och en
rad
resultat 
kommer att visas - samt användas ! -
eftersom determinanter
i många situationer  utnyttjas för att  
både härleda och klarlägga resultat.
Även om man primärt studerar
reella vektorrum visar det sig att
utvidgningen av
den linjära analysen till \emph{komplexa} vektorrum
ofta är fruktbart och rentav nödvändigt.
Ett skäl är  \emph{algebrans fundamentalsats}
som medför att
för varje $(n,n)$-matris $A$ har dess
\emph{karakteristiska polynom}
\[
\Delta_A(\lambda)= \det (\lambda\cdot E_n-A)
\]
komplexa rötter medan
reella rötter i allmänhet saknas.
Rötterna till detta polynom betecknas med $\sigma(A)$ och kallas för matrisens 
spektralmängd, eller kort för dess
\emph{spektrum}.
Geometriskt betyder detta att om
$A$ utvidgas till
en komplex-linjär avbildning på det $n$-dimensionella 
komplex rummet $\mathbf{C}^n$
så har dess associerade  linjära avbildning
alltid minst en \emph{egenvektor}.
§ xxx ägnas åt komplexa matriser
där
ett centralt tema handlarom   \emph{resolventmatrisen}
\[
R_A(\lambda= (\lambda\cdot (E_n-A)^{-1}
\]
där $\lambda$ är en komplex variabler som
varierar utanför matrisens spektrum $\sigma(A)$.
En rad viktiga resultat kan
nu härledas genom att
utnyttja
teorin för analytiska funktioner av en komplex varíabel. Så 
i avsnitt § xxx förutsättes läsaren vara förtrogen med den grundläggande teorin
om  komplex-analytiska funktioner.
\medskip


\noindent
{\bf{Extremalproblem och konvexitet.}}
Ett problem som
bidragit till utvecklingen av
matriskalkyl är att lösa följande 
uppgift. Låt $A$ vara en inverterbar $(n,n)$-matris med reella element.
Den avbildar  enhetssfären $S^{n-1}$ till
en ellipsoid  $\mathcal E$.
Betrakta nu en vektor $\mathbf b$ vars norm, dvs. avstånd till
origo är
större än $A$-matrisens norm definierad av
\[ 
||A||= \max_{\mathbf x\in S^{n-1}}\, ||A\mathbf x||
\]
Det visar sig att det nu existerar en unik vektor $\mathbf x_*\in S^{n-1}$ så att
\[
||A\mathbf x_*-\mathbf b||=
\min_{\mathbf x\in S^{n-1}}\, ||A\mathbf x-\mathbf b||\tag{*}
\]
Dernna existens och entydighet
härleddes av Joseph Lagrange som introducerade
\emph{Lagrangemultiplikatorer} för
att
lösa allmänna icke-linjära
extremalproblem där
de oberoende variablerna
inte får variera "hur som helst".
En alternativ lösning till (*) gavs senare av Gauss som 
bygger på  \emph{minsta kvadratmetodiken} och har
förtjänsten
att
den klarlägger hur $\mathbf x_*$ beror av 
$\mathbf b$.
Här bör understrykas att lösningen till 
(*) är intrikat eftersom varken vektorn $\mathbf x_*$
eller 
minimumvärdet i allmännhet  
kan beräknas via  lösningar till
linjära ekvationssystem.
Men en "god bit på väg" för
att
förenkla kalkyler som löser (*) genomfördes
av Julius Gram där man främst kan nämna hans två arbeten:

xxxxx

\bigskip

\noindent
Bland annat visade Gram att om $A$  är en
inverterbar matris där $\det A>0$
så existerar
två ortogonala matriser $T,S$ samt en diagonalmatris $D$ så att
\[ 
A= TDS
\]
Geometriskt svarar detta mot att
det existerar
en \emph{ortonormerad bas} av
enhetsvektorer
$\mathbf u_1,\ldots,\mathbf u_n$
sådana att bildvektorerna 
$\{A(\mathbf u_i)\}$ är ortogonala mot varandra.
Däremot kan deras
längder kan variera vilket svarar
mot att den diagonala matrisen $D$ i (*) inte är en skalär multipel av enhetsmatrisen $E_n$.

\medskip


\noindent
{\bf{Topologiska vektorrum.}}
Utvidgningen av
resultat om konvexa mängder i
$\mathbf  R^n$
till icke ändligt-dimensionella situationer 
är trivial och   § xxx  ägnas åt 
konstruktioner och resultat om
\emph{linjära rum försedda med en lokalt konvex topologi}.
Som framgår i  XXX  handlar   den
enda icke-triviala 
ingrediensen under passage från det ändligt-dimensionella
fallet till
allmänna  topologiska vektorrum
 i att
tillämpa 
\emph{urvalsaxiomet}.
I § xxx ingår även en beviset för en sats 
av Lars Hörmander  vars   kraftfulla och mycket generella 
innehåll 
täcker i stort sett alla
förekommande situationer där konvexa metoder utnyttjas 
för att
lösa linjära problem inom optimeringsläran.
\medskip


\noindent
{\bf{Teorin om $\sigma$-additiva mått.}}
Avsnitt § XXX ägnas åt 
W. Stieltjes 1890  banbrytande arbete från 1890
som lagt 
grunden till den
modern integrationsteorin
som initerats  av Arkimedes och Riemann.
Konstruktion och egenskaper av  \emph{samtliga $\sigma$-additiva mått}
i
$\mathbf R^n$
sker  genom att uttnyttja
dyadiska gitter. Med andra ord,
för varje heltal $N\geq 1$ betraktas
familjen $\mathcal D_N$
av 
slutna kuber 
vars $2^n$ många hörn
har koordinater
av formen
$2^{-N}\cdot m$ där $m$ genomlöper alla heltal.
Enhetskuben 
\[
\square=\{ \mathbf x \colon\, 0\leq x_\nu\leq 1\}
\]
är t.ex. uppdelad i $2^{Nn}$ många kuber från $\mathcal D_N$.
\medskip


\noindent
Lebesguemåttet ärt det specialfall
där man
volymer hos kuber anges via enligt den euklidiska
formeln, medan
den allmänna konstruktionen som gavs av  Stieltjes
ger upphov till
mått vars 
massa kan vara koncentrerad till
$k$-dimensionella delrum för varje $1\leq k\leq n-1$.

\medskip


\centerline{\bf{Blandade exempel}}.

\medskip

\noindent
Kapitel XX ägnas åt satser  som 
illustrerar den allmänna teorin med reservation för att
innehållet  i detta kapitel är av en mer avancerad natur.
Så materialet i detta kapitel  kan ses som  
en "överkurs"
jämfört med tidigare grundläggande resultat.













\newpage


\centerline{\bf{Innehåll}}


\bigskip


XXXX



XXXXX


\newpage
























































































\newpage














\centerline{\bf{Matriser och euklidisk geometri}}

\medskip


\noindent
René Descartes lanserade 
omkring 1640
sitt program för
att
härleda satser i euklidisk geometri genom att
utnyttja vad som numera kallas det cartesiska
$(x,y)$-planet där varje  punkt
tilldelas koordinater som består  av ett talpar $(a,b)$.
Upp till likformighet
är t.ex. varje triangel $\Delta(a,b)$
bestämd av tre hörnpunkter: 
\[
A= (0,0)\quad B=(1,0)\quad\&\quad C= (a,b)\quad\colon 0<a\leq 1\quad \, b>0\tag{*}
\]
Upp till likformighet
utgör bildar alltså
trianglarna  i planet  
en 2-parametrig skara
där
normaliseringen
(*) består i att 
sidan
$AB$ har längd 1 och placerats i längs $x$-axeln.


\medskip

\noindent
{\bf{Övning.}}
Pythagoras teorem visar  att vinkeln i hörnet $C$ blir rät, dvs. $\pi/2$ precis då
$a^2+b^2=1$ medan den är spetsig om
$a^2+b^2>1$.
Hörnvinkeln i $A$
ges av
\[ 
\phi(A) =\tan \,\frac{b}{a}
\]
och  i
$B$
blir hörnvinkeln 
\[ 
\phi(B)= \tan \frac{b}{1-a}
\]
Från dessa formler  kan läsaren vidimera att
trianglarna $\Delta(a,b)$
är \emph{icke-likformiga}
då talparet $(a,b)$ varierar i (*).
\medskip


\noindent
{\bf{Den inskrivna ellipsen.}}
Till varje  triangel $\Delta(a,b)$
associeras
$(2,2)$-matrisen
\[
A=
\begin{pmatrix}
1&a\\
0&b\\
\end{pmatrix}
\]
som avbildar de två basvektorerna $\bold e_x$ och $\bold e_y$
på
respektive  hörn $B=(1,0)$ samt $C= (a,b)$.
I den rätvinkliga triangeln $\Delta_*$
med hörn i origo och
de två euklidiska basvektorernas  spetsar
existerar dess \emph{inskrivna ellips $\mathcal E_*$}
som har sin medelpunkt i
medianernas skärningspunkt, dvs. i
$(1/3,1/3)$.
Nu avbildas  $\mathcal E_*$
 via den linjära avbildningen
som
definieras via $A$-matrisen ovan på en ellips 
\[ 
\mathcal E=A( \mathcal E_*)
\]
som är inskriven i
$\Delta(a,b)$, dvs. den  tangerar
var och en av $\Delta(a,b)$- triangelns sidor i deras mittpunkter !
\medskip


\noindent
Så utan att behöva genomföra geometriska konstruktioner
är existensen av den inskrivna ellipsen hos varje
triangel \emph{automatisk}  via den av Descartes initierade kalkylen.
Dessutom kan den inskrivna ellipsens centrum och
dess huvudaxlar 
räknas fram via  
matriskalkyl.
\medskip


\noindent
En invariant som hör till
varje triangel är kvoten mellan
längderna av  stor- respektive lill-axeln hos
dess inskrivna  ellips.
Låt oss betrakta ellipsen $\mathcal E(a)$
som ges av ekvationen
\[
x^2+\frac{y^2}{a^2}=1\quad \, 0<a<1
\]
Så här är $a$ kvoten mellan lill-och storaxel.
Uppgiften är nu att beskriva den 1-parametriga  skaran av trianglar
som har
$\mathcal E(a)$ som  inskriven  ellips.
För att finna denna skara av trianglar
betraktas den diagonala  matrisen
\[
D=
\begin{pmatrix}
1&0\\
0&a\\
\end{pmatrix}
\]
som avbildar enhetscirkeln $\mathcal C= \{x^2+y^2=1\}$ på
$\mathcal E(a)$.
Nu betraktas  familjen av liksidiga trianglar som
omskriver  $\mathcal C$
och
vilkas bilder under $D$
ger  skaran av trianglar som
omskriver $\mathcal E(a)$. Läsaren bör  med penna och papper illustrera
konstruktionen genom
att låta liksidiga trianglar rotera
medan de omskriver $\mathcal C$
och samtidigt notera
att de trianglar som uppstår som bilder av
dessa  roterande liksidiga trianglarna \emph{inte} är likformiga.



\medskip


\noindent
Mer precist betraktas först den specifika triangeln
$\Delta_0$ med hörn i
\[
xxxx
\]
Därefter  appliceras  den vridande avbildningen definerad av  matrisen
\[
S_\phi=
\]
Nu ger trianglarna 
\[
 \Delta(\phi)= A(S_\phi(\mathcal C_0))\quad\colon\,0\leq \phi\leq 2\pi
\]
skaran av trianglar som omskriver
$\mathcal E(a)$.

\bigskip


\centerline{\bf{En nyttig övning.}}
\medskip


\noindent
Betrakta en (2,2-)matris
\[
A=
\begin{pmatrix}
1&a\\
0&1\\
\end{pmatrix}
\]
där $a<0$.
Visa först att
$mathbf A$
avbildar enhetscirkeln $S^1$ på en ellips
$\mathcal E$
vars storaxel har längden
\[
zzzz
\]

För varje $R>xx$
betraktas en punkterna
\[ 
\mathbf b(R,\phi)=(R\cos\,\phi),R\cdot \sin\,\phi)) \quad\, 0\leq \phi\leq 2\pi
\]
Vi erhåller nu en funktion
\[
d(R,\phi)= \min_{{\bf x}\in \mathcal E}
||\mathbf b(R;(\phi)\mathbf x)||
\]
\emph{Uppgiften} är nu att finna
den \emph{exakta}  analytiska formeln för denna funktion 
då $R> xxx$ och vinkeln $\phi$ varierar.
Bestäm dessutom
funktionen
\[ 
\phi\mapsto \mathbf x_*(R,\phi)
\]
där $\mathbf x_*((R,\phi)$ 
är den punkt som ligger på
$\mathcal E$ och minimerar (*).
Naturligtvis  kan läsaren vid behov "plotta" på en dator och
se hur
funktionerna ovan beter sig då numeriska värden hos $a$ införes.














































\newpage












\centerline{\bf{Basbyten.}}
\medskip


\noindent
Den geometriska tolkningen av en $(n,n)$-matris $A$ med reella element är att
den
framställer en \emph{linjär avbildning} på det euklidiska rummet
${\bf{R}}^n$ där punkter
representeras av   koordinater
med avseende på
en föreskriven euklidisk bas $\bold e_1,\ldots,\bold e_n$.
En punkt 
\[
\bold x= x_1\bold e_1+\ldots+x_n\bold e_n
\]
avbildas  via $A$-matrisen till
\[
\bold y= y_1\bold e_1+\ldots+y_n\bold e_n
\]
där man för varje $1\leq p\leq n$ har
\[ 
y_p=\sum_{q=1}^{q=n}\, a_{pq}\cdot x_q
\]
\medskip

\noindent
Betrakta nu en $n$-tupel
$\bold v_1,\ldots\bold v_n$ av $n$ många \emph{linjärt oberoende} vektorer
i
${\bf{R}}^n$.
Det betyder att
varje vektor kan skrivas på ett entydigt sätt som
en ${\bf{R}}$-linjär kombination av
dessa $\bold v$-vektorer. Speciellt kan vi för varje $1\leq p\leq n$ skriva
\[
\bold e_p=
s_{p1}\bold v_1+\ldots+s_{pn}\bold v_n
\]
där $\{s_{pq}\}$
är en $(n,n)$-matris
som  betecknas med $S$.
Denna matris har en invers eftersom
vi omvänt kan uttrycka $\bold v$-vektorerna i den euklidiska basen, dvs.
\[
\bold v_p=
t_{p1}\bold e_1+\ldots+t_{pn}\bold e_n
\]
där $T=\{t_{pq}\}$ är en ny $(n,n)$-matris.
\medskip


\noindent
Den \emph{sammansatta linjära avbilningen} $T\circ S$ ger  
den identiska avbildningen i den ursprungliga euklidiska basen. Läsaren
bör vidimera
detta genom att visa att
\emph{matrisprodukten}
\[
TS= E_n
\]
dvs. $T$ är  inversen till $S$.
På samma sätt bör läsaren visa
att
$S\circ T=E_n$. 
Slutsatsen är att $S$ och $T$ kommuterar i matrisalgebran och man säger
att $S$ har en \emph{tvåsidig invers matris} som
betecknas med $S^{-1}$.
\medskip

\noindent
För varje $1\leq p\leq n$
erhålles:

\[
A(\bold v_p)= \sum_{i=1}^{i=n}\, t_{pi}\cdot A(\mathbf e_i)=
\sum_{i=1}^{i=n}
\sum_{k=1}^{k=n}\,t_{pi}a_{ik}e_k=
\sum_{i=1}^{i=n}
\sum_{k=1}^{k=n}\sum_{q=1}^{q=n}\,
t_{pi}a_{ik}s_{kq}\mathbf v_q\tag{i}
\]
Detta betyder att om
$A_\mathbf v$ är matrisen med element
\[
\alpha_{pq}=\sum_{i=1}^{i=n}
\sum_{k=1}^{k=n}\,t_{pi}a_{ik}s_{kq}
\]
så blir
\[ 
A(\mathbf v_p) =A_\mathbf v(\mathbf v_p)
\quad\, 1\leq p\leq n
\]
Det följer att $A=A_\mathbf v$
och enligt (i) ger nu konstruktion av matrisprodukter att
\[
A=A_\mathbf v=  TAS= S^{-1}AS\tag{*}
\]





\medskip


\noindent
{\bf{Slutsats}}.
Likheten (*) visar hur ett byte av den  euklidiska $\mathbf e$-basen
till en ny $\bold v$-bas
framställer den givna matrisen $A$ 
med  matrisen  $A_{\bold v}$
som ger samma linjära operator som $A$
medan 
där $A_{\bold v}$-matrisen
beskriver 
transformation av  punkter
uttryckta i  $\bold v$-basen.
\medskip


\noindent
I situationer där man primärt är intresserad av $A$-matrisens tillhörande linjära operator $\mathbf A$
utförs ett basbyte som i "godartade fall" ger ett
väsentligt enklare uttryck för den nya $A_\bold v$-matrisen.








\newpage








\centerline{\bf{Matriser och deras  normer.}}
\medskip


\noindent
Storleken hos en $(n,n)$-matris $A=\{a_{pq}\}$ 
kan mätas på flera olika sätt.
Ett exempel  är  \emph{Hilbert-Schmidt normen}:
\[ 
||A||_{HS}=\sqrt {\sum\sum\, |a_{pq}|^2}\tag{1}
\]
där vi under kvadratroten summerat över alla $1\leq p,q\leq n$.
I § zzz visas att (1)  är submultiplikativ,  d.v.s
för varje par av matriser gäller olikheten
\[
||AB||_{HS}\leq||A||_{HS}\cdot ||B||_{HS}
\]
Ett annat exempel är matrisens {operatornorm} som definieras
av
\[
||A||_{Op}=
\max _{|x|=1}\, \sqrt{ \sum_{p=1}^{p=n}\,
\sum_{q=1}^{q=n}\,(a_{pq}\cdot x_q)^2}\tag{2}
\]
Uttryckt med  den euklidiska inre produkten
kan (2) skrivas som
\[
||A||_{Op}= \max_{||x||=1}\, \sqrt{\langle Ax,Ax\rangle}\tag{2}
\]

\medskip


\noindent
{\bf{Övning.}}
Visa  att Cauchy-Schwarz olikhet ger
\[
||A||_{Op}\leq  ||A||_{HS}\tag{3}
\]
\medskip

\noindent
{\bf{Allmänna submultiplikativa normer.}}
Dessa ges av
en funktion
\[
A\mapsto \chi(A)
\]
där $\chi(A)$ är positiva reella tal för alla
$A$ utom den triviala nollmatrisen.
Vidare gäller för varje skalär $\alpha$ samt för
varje par av matriser $A,B$: 
\[
\chi(\alpha\cdot A)=|\alpha|\cdot \chi(A)\quad\, \alpha\in {\bf{R}}
\]
\[
\chi(A+B)\leq \chi(A)+\chi(B)
\]
\[
\chi(AB)\leq \chi(A)\cdot \chi(B)
\]
samt 
den normaliserade likheten
\[
\chi(E_n)=1
\]
dvs. enhetsmatrisens norm är ett.
\medskip


\noindent
{\bf{En sats av Hadamard.}}
I sin doktorsavhandling från 1894
bevisade Hadamard följande anmärkningsvärda teorem.
\medskip


\noindent
{\bf{Teorem.}}
\emph{För varje submultiplikativ norm $\chi$ och varje matris $A$ existerar gränsvärdet}
\[ 
\lim_{N\to \infty}\, [\chi(A^N)]^{\frac{1}{N}}\tag{*}
\]
\emph{där (*) är lika med
$A$-matrisens spektralradie}.
\medskip


\noindent
Spektralradien hos en $(n,n)$-matris
 $A$  är det största absoluta belopp som antas
av rötterna till
polynomet
\[ 
\det(\lambda\cdot E_n-A)
\]
\medskip



\noindent
\emph{Bevis.}
För en given matris $A$
sätter
vi
\[
\phi(N)= 
 [\chi(A^N]^{\frac{1}{N}}\quad\, N=1,2\ldots
\]
Vi påminner om att
för varje par av positiva reella tal gäller likheterna
\[
\log\,(ab)= \log\,a+\log\,b\,\quad \&\quad \, \log (a^{\frac{1}{N}})= \frac{\log a}{N}\quad\, N= 1,2,\ldots
\]
Tillämpas detta på $\phi$-funktionen följer från den submultiplikativa olikheten
att
\[
\frac{\log\,\phi(N+M)}{N+M}\leq
\frac{\log\,\phi(N)}{N}+
\frac{\log\,\phi(M)}{M}\tag{i}
\]
Speciellt kan vi låta $M=1$ medan $N$ är godtyckligt och har då 
\[
\frac{\log\,\chi(N+1)}{N+1}\leq
\frac{\log\,\chi(N)}{N}+
\frac{\log\,\chi(1)}{1}\tag{ii}
\]
Eftersom $\chi(1)$
ärt talföljden
\[
N\mapsto \log\,\frac{\log\,\chi(N+1)}{N+1}
\] 
avtagande vilket medför att även
 \[ 
N\mapsto \chi(A^N]^{\frac{1}{N}}
\] 
är avtagande vilket ger
det önskade gränsvärdet i Hadamards teorem.
Det återstår att visa varför detta gränsvärde är lika med$A$-matrisens spektralradie.
För detta utnyttjas $A$-matrisens 
resolvent
\[ 
R_A(\lambda)= (\lambda\cdot E_n-A)^{-1}
\]
som är en analytisk funktion utanför cirkelskivan $\{\lambda|\leq \rho(A)\}$
där den representeras av
Laurentserien
\[
\frac{E_n}{\lambda}+ 
\sum_ {n=1}^\infty \frac{A^\nu}{\lambda^{\nu+1} }
\]
som för varje $\chi$ konvergerar i
området
$\{|\lambda|>\chi(A)\}$.
Eftersom
resolventen nu har en pol på cirkeln med radien  $\rho(A)$ följer
Hadamard's Teorem.
\medskip

\noindent
{\bf{Anmärkning.}}
Vi hänvisar till
§ XXX
för fakta om resolventer som användes i beviset ovan.
Låt oss också påpeka att
i allmänhet gäller en strikt olikhet
\[
\rho(A)< ||A||_{Op}
\]
dvs. spektralradien är strikt mindre än matrisens operatornorm.
I § xx visas en sats av Schur som 
säger att likhet gäller ovan när det existerar en matris $B$ så att
\[ 
A= e^B
\]

\medskip



\centerline {\bf{Linjära normer.}}
\medskip


\noindent
Många intressanta normer
uppträder som inte nöfvängitvis är submultiplikativa, dvs. det
enda kravet är nu att man har en funktion som till varje $(n,n)$-matris $A$
tillordnar ett icke negativt reellt tal $\rho(A)$
där denna $\rho$-funktion uppfyller de två villkoren 
\[
\rho(A+B)\leq \rho(A)+\rho(B)\tag {i}
\] 
\[
\rho(\alpha\cdot A)= |\alpha|\cdot \rho(A)\tag{ii}
\]
för varje matrispar $A,B$ och varje reellt tal $\alpha$.
\medskip


\noindent
{\bf{Exempel.}}
För varje par av reella tal $p,q$ som båda är $>1$
defineras normen
\[ 
\rho_{p,q}(A)=
\big [\sum_{\nu=1}^{\nu=n}\, \big[\,\sum_{k=1}^{k=n} \, |a_{\nu,k}|^p\,\big]^q\,\big ]^{\frac{1}{pq}}
\]
En annan ofta förekommande norm
är:
\[ 
\rho_{\infty,1}(A)= \max_{1\leq \nu\leq n}\, \sum_{k=1}^{k=n}\, |a_{\nu,k}|
\]

\newpage


\centerline{\bf{Thorins konvexitetsteorem.}}

\medskip


\noindent
Till ett par $0<a,b<1$ införs en norm på $(n,n)$-matriser genom
\[
||A||_{a,b}= \max\, |\sum\sum \, a_{\nu,k}\cdot x_\nu\cdot y_k|\quad \colon\,\sum\, |x_\nu|^{1/a}= \sum\, |y_k|^{1/b}=1\tag{*}
\]
\medskip

\noindent
{\bf{Teorem.}}
\emph{För varje matris $A$ är}
\[
(a,b)\mapsto ||A||_{a,b}
\] 
\emph{en konvex funktion av
paret $(a,b)$ som  varierar i den öppna enhetskvadraten.}
\medskip


\noindent
Beviset återges i § XXX och bygger på en  olikhet av Hadamard som utnyttjar
resultat från  teorin om analytisk funktioner.







\newpage


\centerline{\bf{Konvexa mängder och deras normfunktioner}}

\medskip


\noindent
Vi skall införa
en familj av delmängder i
${\bf{R}}^n$ där $n\geq 2$.
\medskip


\noindent
{\bf{Definition.}}
\emph{En konvex kropp
är en slutet och begränsad delmängd
$K$ i ${\bf{R}}^n$
som innehåller
origo och
för varje par av punkter
$\bold p,\bold q$ i $K$ följer att sträckan}
\[
\ell(\bold p,\bold q)=\{s\cdot\bold p+(1-s)\bold q\}\subset K
\]
Vidare finns för varje $\bold p\neq 0$
ett positivt tal $a$ så att
\[
\ell(\bold o,a\cdot \bold p)\subset K\tag{*}
\]
\emph{där $\bold o$ betecknar origo.
Om $a^*$ är det största talet så att (*) gäller sätter
vi}
\[
\rho_K(\bold p)= \frac{1}{a}
\]







\medskip


\noindent
{\bf{Övning.}}
Notera först att konstruktionen av $\rho$-funktionen i (*)
medför att
\[ 
\rho_K(\bold s\cdot\bold p)= s\cdot \rho_K(\bold p)\quad\, s>0
\]
för varje punkt $\bold p$.
Det betyder att $\rho_K$-funktionen är positivt homogen där 
$\rho_k(\bold b)$ avtar mot noll när en
punkt närmar sig origo.
Använd  konvexiteten hos $K$ för att visa
olikheten
\[ 
\rho_K(\bold p+\bold q)\leq
\rho_K(\bold p)+\rho_K(\bold q)
\]
för varje punktpar i ${\bf{R}}^n$.
En ledning är att det räcker att föreställa sig
en konvex kropp i
planet som spänns upp av de två vektorerna som hör till
punkterna.
\medskip


\noindent
{\bf{Det symmetriska fallet.}}
Om $K$ är symmetrisk, dvs. om
\[ 
\bold p\in K\implies -\bold p\in K
\]
ser vi att (1) inkluderar  likheten
\[
\rho_K(s\cdot\bold p)= |s|\cdot \rho_K(\bold p)\quad s<0
\]
och vi  säger  bu att $\rho_K$ är en norm på $\bold R^n$ där
den symmetriska konvexa kroppen svarar
mot ett "enhetsklot", dvs.
\[
K= \{\bold p\quad\, \rho_K(\bold p)=1\}
\]
Till ett
par av vektorer
$\bold u,\bold v$
bildas deras differens
$\bold v-\bold u$
där spetsen hos $\bold u$ placeras i origo, varefter vi definierar
\[
||\bold v-\bold u||_K= \rho_K(\bold v-\bold u)
\]
\medskip

\noindent
{\bf{Normen  på matriser.}}
Om $A$ är en $(n,n)$-matris
och $K$ en  symmetrisk konvex kropp definieras
\[
||A||_K= \max_{||p||_K=1} \rho_K(A\bold p)
\]

\medskip


\noindent
För en given konvex kropp $K$
gäller alltså att $||A||_K\leq 1$ om och endast om
\[ 
A(K)\subset K
\] 
dvs. den linjära operatorn $A$ avbildar $K$ in i sig själv.
\medskip



\noindent
{\bf{Utvidgad teori till
icke-ändligt dimensionella vektorrum.}}
Övergången från matrisläran till icke dimensionella situationer
där man arbetar med 
\emph{lokalt konvexa rum} 
är nära nog  trivial eftersom   både 
konstruktioner och   satser
stödjer sig
på
ändligt-dimensionella  fall.
Resultat inom ramen för lokalt konvexa topologiska vektorrum har
dock
vida användningsområden, bl.a. inom optimeringsläran.
Ett allmännt om lokalt konvexa tum
som ofta kommer till användning
redovisas §§ där vi återger Lars Hörmanders
mycket slagkraftiga och generella resulat om
konvexa stödfunktionaler på helt allmänna
lokalt konvexa rum.

\newpage




\newpage



















































































































\newpage


































\centerline{\bf{MATRISER}}

\medskip


\noindent
En kvadratisk matris av rang $n\geq 2$ vars element är reella tal 
skrives på formen
\[
A=
\begin{pmatrix}
a_{11}&a_{12}&\ldots  &a_{1n}\\
a_{21}&a_{22}&\ldots &a_{2n}\\
\ldots&\ldots&\ldots&\ldots\\
\ldots&\ldots&\ldots&\ldots\\
a_{n1}&a_{n2}&\ldots &a_{nn}\\
\end{pmatrix}
\]
där de  dubbelindicerade reella talen $\{a_{pq}\}$
utgör matrisens element.
För varje $1\leq k\leq n$
svarar $n$-tupeln $(a_{k1},a_{k2}\ldots ,a_{kn})$
mot matrisens radvektor. På samma sätt erhålles 
kolonnvektorer där 
$(a_{11},a_{21},\ldots ,a_{n1})$
är matrisens  första kolonn.

\medskip

\noindent
En central uppgift handlar om att lösa
\emph{linjära system av ekvationer} där man för $n\geq 2$ söker
en lösning $\bold x= (x_1,\ldots,x_n)$
så att

\[
a_{11}x_1+a_{12}x_2+\ldots  +a_{1n}x_n=b_1
\]
\[
a_{21}x_1+a_{22}x_2+\ldots +a_{2n}x_n=b_2
\]
\[
\ldots\,\ldots\,\ldots
\]
\[
a_{n2}x_n+a_{2n}x_2+\ldots +a_{nn}x_n=b_n
\]


\medskip

\noindent
då $b_1,\ldots,b_n$ är
en $n$-tupel av reella tal.
Det "ideala fallet" är att
systemet har en entydig lösning
$\bold x$ för varje  $n$-vektor $\bold b$.
Ett exempel där detta inträffar
är då $A$-matrisens
diagonala element alla är 1 samt att
nollor står under diagonalen.
En  strikt uppåt triangulär matris ges  av formeln.

\[
U=
\begin{pmatrix}
1\,&u_{12}&\ldots  &\ldots&u_{1n}\\
0\,&1&\ldots &\ldots&u_{2n}\\
\ldots&\ldots&\ldots&\ldots\\
\ldots&\ldots&\ldots&1&u_{n-1,n}\\
0&0&\ldots&0&1\\
\end{pmatrix}
\]
För en sådan  matris
ser vi att man till varje $\bold b$-vektor erhåller en unik
lösning  $\bold x$ där 
$x_n=b_n$ varefter
\[
x_{n-1}= b_{n-1}-b_n\cdot u_{n-1,n}
\]
och så vidare.
Vi kan även betrakta strikt nedåt triangulära matriser
\[ 
L=
\begin{pmatrix}
1\,&0\,&\ldots  &0\\
\lambda_{21}\,&1&\ldots &0\\
\ldots&\ldots&\ldots&0\\
\ldots&\ldots&1&0\\
\lambda_{n1}&\lambda_{n2}&\ldots &1\\
\end{pmatrix}
\]
För en sådan matris  erhålles en lösning
genom
att börja med att $x_1=b_1$ varefter
\[ 
x_2=b_2-\lambda_{21}b_1
\]
och så vidare.
Ännu enklare löser man ett system då matrisen är diagonal, dvs. när $D$ är en matris sådan att
\[
\nu\neq k\implies d_{k\nu}=0\,\&\, d_{kk}\neq 0
\]
Notera att det här fordras  att varje diagonalelement hos $D$ är $\neq 0$ eftersom
en lösning måste vara av formen
\[
x_k= b_k/d_k\colon\, 1\leq k\leq n
\]
vilket  tvingar $d_k\neq 0$ så snart $b_k\neq 0$.
\medskip


\newpage

\centerline{\bf{Triangulär resolution}}
\medskip


\noindent
Diskussionen ovan visar nyttan av att kunna skriva en matris $A$ som en produkt
\[
A= LDU\tag{i}
\]
där $U$ är strikt uppåt och $L$ strikt nedåt triangulär
samt $D$
diagonal.
Det visar sig att
en sådan framställning av $A$ existerar och dessutom är entydig
under antagandet att matrisens  \emph{principala minorer}
alla är $\neq 0$.
Detta fundamentala resultat visades  - oberoende - av Jacobi och Sylvester omkring 1840.
\medskip

\noindent
Här återges ett  bevis hämtat från   
Alan Turings
artikel
\emph{Rounding-off errors in matrix processes}
som utkom i november 1947.
Turings artikel innehåller 
belysande kommentarer om metoder för att
lösa linjära system där det bör påpekas att hans artikel skrevs
\emph{innan} nutida kraftfulla datorer existerade.
Men 1947
hade utvecklingen kommit ganska långt och Turing   arbetade
från och med augusti 1945 vid National  Laboratory at Teddington 
med uppdrag från den dåvarande brittiska regeringen  att
skapa en \emph{Universal Machine}
dvs. en dator.
I sin artikel skriver Turing bl.a.
\medskip

\noindent
\emph{It seems probable that with the advent
of electronic computers
it will become
standard practice
to find the inverse of a matrix. This time has however
not yet arrived
and some consideration is therefore given
in this paper
to solutions with or without inversion of the $A$-matrix.}
\medskip


\noindent
Låt oss nu återge Turings bevis för att (i)
har en entydig lösning under
antagandet att $A$-matrisens principala minorer alla är $\neq 0$.
Till att börja med noteras att om
(i) gäller så har man ekvationerna.
\[
a_{pq}= \sum_{\nu}\, \lambda_{p\nu}\cdot d_\nu\cdot S_{\nu q}
\]
där summan för varje fixt par $p,q$ tas over
\[
1\leq \nu\leq\min(p,q)
\]
Givet matrisen $A$ skall vi nu visa hur
elementen hos de tre matriserna $L,D,U$
erhålles rekursivt.
Först noteras att
\[
a_{11}=\lambda_{11}\cdot d_1\cdot s_{11}= d_1
\implies a_{11}= d_1
\]
Antagandet om $A$:s principalminor  av rang
ett betyder att $a_{11}\neq 0$ och alltså följer
$d_1\neq 0$.
Eftersom $\nu\geq 2 \implies \lambda_{1\nu}=0$ kan läsaren nu kolla att
\[
a_{12\nu}=d_1\cdot u_{1\nu}\colon\, 2\leq \nu\leq n
\]
och därmed har första raden hos $U$-matrisen blivit bestämd. I nästa steg noteras att
\[
a_{21}=\lambda_{21}\cdot d_1
\,\& \,
a_{22}=\lambda_{21}\cdot d_1\cdot u_{12}+d_2
\]
Notera att dessa två likheter ger
\[
d_2+\frac{a_{21}\cdot a_{12}}{a_{11}}= a_{22}
\implies
d_2=\frac{a_{11}a_{22}- a_{12}a_{21}}{a_{11}}
\]
där antagandet att $A$-martrisens principala minor av rang 2 är $\neq 0$ ger $d_2\neq 0 $.
När $\nu\geq 3$ erhålles
\[ 
a_{2\nu}=\lambda_{21}d_1 u_{1\nu}+ d_2u_{2\nu}
\]
vilket bestämmer $U$-matrisens resterande element i andra raden.
I nästa steg noteras först att
\[ 
a_{31}=\lambda_{31}\cdot d_1\&\quad\,
a_{32}= \lambda_{31}\cdot u_{12}+\lambda_{32}\cdot d_2
\] 
där vi använde att $s_{11}=s_{22}=1$
Eftersom $s_{12}$ redan är känd och både $d_1$  och $d_2$ är $\neq 0$
ser vi att
elementen
$\lambda_{31}$ samt $\lambda_{32}$ blir bestämda.
Vidare erhålles
\[
a_{33}= \lambda_{31}d_1u_{13}+\lambda_{32}\cdot d_2u_{23}+d_3
\]
vilket bestämmer $d_3$. När $\nu\geq 4$ följer sedan
att
\[
a_{3\nu}=
 \lambda_{31}d_1u_{1\nu}+\lambda_{32}\cdot d_2u_{2\nu}+d_3u_{3\nu}
 \]
 och vi har därmed  bestämt den tredje raden hos $S$-matrisen.
\medskip

\noindent
 {\bf{Övning.}}
Läsaren bör här kolla ekvationerna  som har lett  fram till
uttrycket för $d_3$ och \emph{bevisa} att $d_3\neq 0$ följer från antagandet att
$A$-matrisens principala minor av rang 3 är $\neq 0$.
\medskip


\noindent
Med $n\geq 4$ fortsätter likartade rekursiva
formel, dvs. först bestäms 
$\lambda_{41},\lambda_{42},\lambda_{43}$ och därefter
$d_4$ varefter
elementen $s_{4\nu}\colon\, \nu\geq 5$ kan bestämmas, och så vidare.
\medskip


\noindent
Det är  instruktivt att
låta en dator utföra en triangulär faktorisering för 
"lagom stora" matriser
med numeriskt insatta element.
Notera att antagandet  om $A$-matrisens icke försvinnande principala minorer
behöver inte alltid gälla. Men om $A$:s determinant från början är $\neq 0$
kan
man visa att efter en omgruppering
av matrisens rader så gäller denna förutsättning som brukar kallas Jacobi-villkoret för en
inverterbar matris.
Sådana omgrupperingar  uppträder bland annat   när  löser
ett linjärt system med Gauss' elimination. Om exempelvis 
$a_{11}=0$ så  permuteras raderna där  man
låter en rad med  $a_{\nu1}\neq 0$ byta plats med första raden
och därefter
eliminerar
$x_1$ från alla övriga rader.


\newpage



\noindent
\centerline{\bf{En  resumé av Alan Turing.}}
\medskip



\noindent
Här följer utdrag från Turings artikel [Tur]
som skrevs 
medan
konstruktion av en dator ännu pågick. 

\medskip

\noindent
Betrakta  ett \emph{kvadratiskt } system
där ekvationerna i (*) skrivs på formen
\[
\bold A\bold x=\bold b
\]
och $\bold A$  betecknar  en $(n,n)$-matris.


\medskip


\noindent
Suppose we wish to solve the equations (*)
by the elimination method. The procedure is as follows.
Assuming   that $a_{11}\neq 0$
we first  add such multiples of the first equation to the others
so that the coefficient of $x_1$ is reduced to zero
in all rows excepting the first. 
We then add multiples of the second
equation to later ones until the coefficient
of $x_2$ is reduced to zero.
After $n-1$ steps of this nature
we shall be left with a set of equations
of the form
\[
\sum_{i\leq j}\, a_{ij}x_j=c_j\quad 1\leq i\leq n
\]
Under the condition that the diagonal elements $\{a_{\nu\nu}\}$ are all $\neq 0$
we  find 
from the equation $a_{nn}x_n=b_n$
the unknown $x_n$ , and by substituting
it in the the equation
\[ 
a_{n-1,n-1}x_{n-1}+ a_{n-1,n}x_n= b_{n-1}
\]
we then find $x_{n-1}$, and so on until
by this repeated back-substitution we have found all the coefficients
of the (originally) unknown vector $\bold x$.

\medskip

\noindent
This description of the elimination process is all that is requried to apply it.
We shall find it instructive, however, to
look at it further
from a number of points of view.
\medskip

\noindent




\noindent
{\bf{1}}. The process of replacing
rows of a matrix by linear combinations
of other rows may be regarded
as left mutliplication of the matrix by another matrix, so that
we first convert the equation $\bold A\bold x=\bold b$
into
\[ 
\bold J_1\bold A\bold x=\bold b_1
\]
and record the new matrix $\bold J_1\bold A$ and the new vector $\bold J_1\bold b$.
We then convert the system   into a system
\[
\bold J_2\bold J_1\bold A\bold x=\bold J_2\bold J_1\bold b_1
\]
and so on, until we finally
have
the system
\[
\bold J_{n-1}\cdots \bold J_2\bold J_1\bold A\bold x=\bold J_{n-1}\cdots\bold J_2\bold J_1\bold b_1
\]
In accordance with the triangular resolution
we may write
\[
\bold J_{n-1}\cdots \bold J_2\bold J_1=\bold L^{-1}
\quad\&\quad
\bold J_{n-1}\cdots \bold J_2\bold J_1\bold A=\bold D\bold U
\]
\medskip

\noindent
{\bf{2}}. The matrix $\bold L$
can be very easily obtained
from the $\bold J$-matrices. We have in fact that
\[
\bold L=\bold E_n+\sum_{r=1}^{n-1}\, (\bold E_n-\bold J_r)
\]
whose verifiation is left to the reader.
\medskip


\noindent
{\bf{3}}.
There is from the start no need
for us to take
either of the equations or the unkonwns in the order
in which they are given. In other words, if $\bold P$ and $\bold Q$ represent
permutations
we may solve instead
\[ 
\bold A'\bold x'=\bold b'
\] 
where
\[
\bold A'=\bold P\bold A\bold Q\quad \bold b'=\bold P\bold b\quad\&\quad \bold x'=\bold Q \bold x
\]
The permutations $\bold P$ and $\bold Q$
may be chosen bit by bit as we carry the process through.
One popular method is to let
$\bold Q$ be the identity, that is to take
the variables in the order given,  and to choose
$\bold P$ so that the coefficients in the matrices $\{\bold J_r\}$
do not exceed unit in  magnitude. This is always possible, and for almost all matrices
gives a unique $\bold P$.
\medskip

\noindent
Alternatively, this variation method may be described
by saying that $\bold P$ is chosen
so that the  element $d_1$ of the diagonal matrix $\bold D$
shall be as large as possible, and so on.
This procedure to
to take the largest
coefficient in the column
is  referred to as the \emph{Pivot method}
and when it is done the succesive diagonal  elemets $d_1,d_2,\ldots d_n$
are known as the first, second, ... ,last pivots. Here it is likely
that this chosen order give smaller errors
than other posssible. But let us remark that
there may also be an advantage
in choosing the largest coefficient of the given
matrix as a leading pivot.

\medskip

\noindent
{\bf{4}},
The work involving
in solving a set of $n$ equations by the elimination method
are as follows: 
\[
\frac {1}{3}\cdot n^3+\bold {O}(n^2)
\] 
multiplications
and recordings
of which
$\frac{1}{2}\cdot n^2$ recordings involve the vector $\bold b$..

\medskip

\noindent
{\bf{5}}.
If after we have solved one set oif equations $\bold A\bold x=\bold b$
and asked to solve a second
$\bold A \bold x=\bold b'$ while keeping the same matrix
$\bold A$, we only have to operate on
$\bold b$ with the $\bold J$-matrices to get
\[ 
\bold b'=\bold J_{n-1}\cdots\bold J_1\bold b
\]
where the $\bold J$-matrices
during the proceeses alrady performed
may be supposed to
have been kept for reference. Then we solve
\[ 
\bold D\bold U\bold x'= \bold b'
\]








\newpage

\centerline{\bf{Eliminationsmetoden}}



\medskip


\noindent
Ett linjärt ekvationssystem med $m$ ekvationer och $n$ obekanta
skrives på formen

\[
a_{11}x_1+a_{12}x_2+\ldots  +a_{1n}x_n=b_1
\]
\[
a_{21}x_1+a_{22}x_2+\ldots +a_{2n}x_n=b_2
\]
\[
\ldots\,\ldots\,\ldots
\]
\[
a_{m1}x_n+a_{m2}x_2+\ldots +a_{mn}x_n=b_m \tag{*}
\]
Då $m=n$ har vi ett kvadratiskt system och man
kan även att studera fallen $m<n$ och $m>n$.
Till  varje $1\leq\nu\leq m$ införs
vektorn
\[ 
S_\nu=(a_{\nu 1},a_{\nu 2}\, \ldots\,a_{\nu n},b_\nu)\tag{1}
\]
och vi sätter $\bold S=(S_1,\ldots,S_m)$. Systemet är nu definierat
av dessa  $m$ många $(n+1)$-vektorerna.
Mängden av 
alla $n$-vektorer 
$\bold x=(x_1,\ldots,x_n)$ samt $m$-vektorer $\bold{b}$
som tillsammans löser (*) betecknas med $L(\bold S)$ och
kallas systemets lösningsrum.

\medskip

\noindent
Låt $\bold T=(T_1,\ldots,T_m)$
vara ett annat system:
\[
\alpha_{11}x_1+\alpha_{12}x_2+\ldots  +\alpha_{1n}x_n=\beta_1
\]
\[
a_{21}x_1+a_{22}x_2+\ldots +a_{2n}x_n=\beta_2
\]
\[
\ldots\,\ldots\,\ldots
\]
\[
\alpha_{m2}x_n+\alpha_{mn}x_2+\ldots +\alpha_{mn}x_n=\beta_m
\]
\medskip


\noindent
{\bf{Definition.}}
\emph{Två system
$\bold S$ och $\bold T$ säges vara ekvivalenta om
de har samma lösningsrum, dvs. om}
\[
L(\bold S)=L(\bold T)
\]
\medskip


\noindent
{\bf{System på reducerad trappstegsform.}}
De uppträder när det finns
en strikt växande följd
\[
1\leq w_1<\ldots<w_r\leq m\quad\, 1\leq r\leq m
\]
där systemets ekvationer ges av
\[
x_{w_1}+ \sum_{j=w_r+1}^{j=m}\, a_{w_1 j}x_j= b_{w_1}
\]
\[
\ldots\ldots
\]
\[
x_{w_r}+ \sum_{j=w_r+1}^{j=m}\, a_{w_r j}x_j= b_{w_r}
\]
och om  $r<m$ tillkommer de triviala ekvationerna
\[
0= b_\nu\, \quad\,\forall
\,\nu\neq (w_1,\ldots,w_r)\tag{*}
\]
\medskip


\noindent
Hos ett sådant system består  lösningsmängden
av $(n+m)-$vektorer
$(\bold x,\bold b)$ där $\bold b$-vektorn till
att börja med satisfierar (*).
Låt  nu  $\bold w(\perp)$ beteckna alla $1\leq j\leq n$ som
inte tillhör r-tupeln $w_1,\ldots w_r$.
Vi ser nu att
man för varje val av
de $n-r$-många talen
\[
\{x_j\}\quad\colon\quad j\in \bold w(\perp)\tag{i}
\]
samt en godtyckligt vald $r$-vektor
\[
b_{w_1},\ldots,b_{w_r}\tag{ii}
\]
kan lösa systemet där
$x_{w_1},\ldots,x_{w_r}$
bestäms entydigt från talen hos (ii).
\medskip


\noindent 
{\bf{Några dimensionstal.}}
I fallet då $r<m$ och $w_r=m$
kan alltså de $n+m-r$ många talen 
\[ 
\{x_i\}\quad\,  i\in \mathbf w(\perp)\quad\,\&\quad \{b_j\} \quad j\in \mathbf w(\perp)
\] 
väljas godtyckligt, varefter  varje val av
$b_{w_1},\ldots,b_{w_r}$
ger en unik $r$-tupel $x_{w-1},\ldots,x_{w_r}\}$
som löser systemet.
Detta kan uttryckas så att
dimensionen för systemets homogena lösningar
är $n+m-r$ medan dimensionen för 
dess \emph{rang} blir lika med $r$ eftersom man
fritt kan välja $r$-tupeln
$\{b_{w_1},\ldots b_{w_r}\}$.

\medskip


\noindent
Elimination av ett godtyckligt system kan  utföras på ett entydigt vis där
$x$-variabler elimineras med växande $j$  och i slutsteget
erhålles
ett system på reducerad trappstegsform. Hela denna 
procedur  kan  sammanfattas med följande:

\medskip

\noindent
{\bf{Teorem}}.
\emph{Varje system $\bold S$ är ekvivalent med
ett unikt system på trappstegsform
som betecknas med $trapp(\bold S)$.}


\medskip










\noindent
{\bf{En kommentar.}}
Genom att \emph{permutera} index hos $x$-variablerna tillsammans med kolonner
hos $A$-matrisen kan
man utan egentlig inskränkning anta att man efter elimination erhåller
ett reducerat system på trappstegsform
där
\[
j=w_j\quad\, 1\leq j\leq e
\]
Med andra ord, här 
är
$w(\perp)= (r+1,\ldots,n)$. För  $\bold b$-vektorn  fordras  nu för en lösning att
\[
b_{r+1}=\ldots=b_m=0
\]
\medskip


\noindent
{\bf{Övning.}}
Visa att när systemet är skrivet
på reducerad trappstegsform följer
att dimensionen för
\emph{homogena lösningar}, dvs. när hela $\bold b$-vektorn är noll
blir  $n-r$,  medan \emph{rangen}, dvs alla $\bold b$-vektorer för vilka 
det existerar minst en lösning,
har dimension  $r$.





\bigskip


\centerline {\bf{Tre elementära radoperationer}}.
\medskip


\noindent
Till ett  system  $\bold S$ bildas ett nytt system
$\bold T$ via  en av följande konstruktioner där $1\leq k\leq n$ är valt i (1-2) nedan.

\medskip

(1) Sätt $T_k=\alpha\cdot S_k$ för något $\alpha\neq 0$ medan
$T_j=S_j$ för alla $j\neq k$
\medskip

(2) Sätt $T_k=S_k+a\cdot S_j$ för ett  reellt $a$ och index
$j\neq k$.
\medskip

(3) För ett par $j\neq k$ sättes
$T_j=S_k$ och $T_k=S_j$.
\medskip


\noindent
Det är uppenbart att (1-3)
behåller lösningsmängden, dvs. 
$L(\bold T)= L(\bold S)$.
\medskip


\noindent
{\bf{Övning.}}
Bevisa Teorem xx genom att utnyttja
de tre  elementära radoperationerna !
Som en ledning betraktar vi fallet då
$a_{j1}\neq 0$ för minst ett indextal $j$.
Bland dessa väljes det minsta talet $j_*$. Om $j_*>1$
ersätter vi $S_1$ med
den nya raden
\[ 
T_1=S_1+\frac{1}{a_{1j}}S_j
\]
vilket ger ett system vars  första rad  blir:
\[ 
x_1+\sum_{\nu=2}^{\nu=n}\, (a_{1\nu}-\frac{a_{2\nu}}{a_{1j}})=b_1-\frac{b_j}{a_{1j}}
\]
Om $x_1$ därefter förekommer i senare rader
dras en 
lämpliga multiplar av den första bort från de rader där $x_1$ förblir aktiv, dvs. då $a_{j1}\neq 0$.
På så vis erhålles 
ett system där
den enda plats som $x_1$ förekommer är i rad ett.
\bigskip


\newpage

\centerline{\bf{Icke kvadratiska matriser.}}

\medskip


\noindent
Vi skall här diskutera matriser där antalet kolonner inte är lika med antalet rader.
För ett par $n,m$
bildas en matris:
\[
A=
\begin{pmatrix}
a_{11}&a_{12}&\ldots  &a_{1n}\\
a_{21}&a_{22}&\ldots &a_{2n}\\
\ldots&\ldots&\ldots&\ldots\\
\ldots&\ldots&\ldots&\ldots\\
a_{m1}&a_{m2}&\ldots &a_{mn}\\
\end{pmatrix}
\]
Vi talar här om en $(n,m)$-matris, dvs. den har 
$n$ kolonner och $m$ rader.
Dess transponerat är $(m,n)$-matrisen 
$A^*$  med element
\[
a^*_{pq}= a_{qp}
\]
\medskip

\noindent
Vi kan nu konstruera två kvadratiska matriser, nämligen
$(n,n)$-matrisen $A^*A$ samt $(m,m)$-matrisen
$AA^*$.
Element i $A^*A$ ges av
\[
c_{pq}= \sum_{\nu=1}^{\nu=n} \, a^*_{p\nu}a_{\nu q}
= \sum_{\nu=1}^{\nu=n} \, a_{\nu p}a_{\nu q}\quad\, 1\leq p,q\leq n
\]
Elementen i $AA^*$
ges av
\[ 
d_{pq}= \sum_{\nu=1}^{\nu=m} \, a_{p\nu}a^*_{\nu q}
= \sum_{\nu=1}^{\nu=m} \, a_{p,\nu}a_{q,\nu}\quad\, 1\leq p,q\leq m
\]

\medskip


\noindent
Båda dessa matriser är \emph{symmetriska}
eftersom de två dubbelsummorna uppenbart ger likheterna
\[ 
c_{pq}=c_{qp}\quad\&\quad d_{pq}= d_{qp}
\]
\medskip


\noindent
För att beskriva  vilka lösningar som kan uppträda 
visar det sig vara nyttigt att
utnyttja
den \emph{euklidiska inre produkterna}
i ${\bf{R}}^n$ och  i ${\bf{R}}^m$.
Då $\bold u$ och $\bold v$
är två $n$-vektorer
är deras inre produkt 
\[ 
\langle \bold u,\bold v\rangle= u_1v_1+\ldots+u_nv_n
\]
På samma sätt definieras inre produkter av $m$-vektorer.
\medskip


\noindent
{\bf{Övning.}}
Låt $\bold x$ vara en $n$-vektor och $\bold y$ en $m$-vektor. Visa likheten
\[
\langle A\bold x,\bold y\rangle=
\langle \bold x,A^*\bold y\rangle\tag{*}
\]
\medskip

\noindent
I § XX beskrivs hur $A$-matrisen
kan $A$ uppfattas som en linjär avbildning från
${\bf{R}}^n$ till
${\bf{R}}^m$. Dess nollrum
definieras som 
\[
 \mathcal N(A)=\{\bold x\in {\bf{R}}^n\quad\colon\, A\bold x=0\}
\]
Värderummet ges av
\[
\mathcal R(A)=\{\bold y\in {\bf{R}}^m\quad\colon \exists \,\bold x\,\&\, A\bold x=\bold y\}
\]
Här är alltså $\mathcal R(A)$ ett delrum av
${\bf{R}}^m$ medan $\mathcal N(A)$ är ett delrum av
${\bf{R}}^n$. På samma sätt erhålles
\[
\mathcal N(A^*)\subset {\bf{R}}^m\quad\&\quad
 \mathcal R(A^*)\subset {\bf{R}}^n
\]

\medskip


\noindent
{\bf{Övning.}}
Visa att 
\[
\mathcal N(A^*)= \mathcal R(A)^\perp\tag{*}
\]
dvs. att en $m$-vektor $\bold y$ ger $A^*(\bold y)=0$ om och endast om
\[
\langle A(\bold x,\bold y)\rangle\quad\colon\quad \forall\, \bold x\in \bold{R}^n
\]

\noindent
På samma sätt erhålles likheten:
\[
\mathcal N(A)= \mathcal R(A^*)^\perp\tag{**}
\]

\medskip

\noindent
{\bf{Övning.}}
Visa att (*-**) ger
följande ekvation 
mellan dimensionerna hos 
de fyra delrummen som införts ovan.
\[
m= \dim(\mathcal N(A^*))+\dim(\mathcal R(A))\quad\&\quad 
n=\dim(\mathcal R(A^*))+\dim(\mathcal N(A))
\]
\medskip



\noindent
{\bf{Ett exempel.}}
Antag att $n<m$ och att
$\mathcal N(A)$ är nollrummet vilket ger
\[
\dim(\mathcal R(A)) =n
\] 
Samtidigt är 
\[ 
\mathcal N(A^*)\perp \mathcal R(A)
\]
vilket medför  att
den linjära operatorn 
$A^*$ 
ger en injektiv avbildning
från $\mathcal R(A)$ till
${\bf{R}}^n$.
Eftersom
dess värderum är $\perp$ till
$\mathcal N(A)$ som antogs vara noll, följer sedan att
den \emph{sammansatta} operatorn
\[ 
A^*\circ A
\]
ger en bijektiv avbildning på ${\bf{R}}^n$, dvs.
$(n,n)$-matrisen
$A^*A$ är inverterbar.











 














































































\newpage



\centerline{\bf{Minsta Kvadratmetoden.}}


\medskip


\noindent
För att par där $n<m$
betraktas systemet:
\[
a_{11}x_1+a_{12}x_2+\dots+a_{1m}x_n=b_1
\]
\[
a_{21}x_1+a_{22}x_2+\ldots+a_{2m}x_n=b_2
\]
\[
\ldots\,\ldots
\]
\[
a_{m1}x_1+a_{n2}x_2+\ldots+a_{mn}x_n=b_m\tag{*}
\]
\medskip

\noindent
Eftersom antalet
ekvationer är  strikt större än
de tillgängliga $x$-variablerna  är systemet  inte är lösbart för
alla $\bold b$-vektorer.
Man kan dock för varje $\bold b$
söka en $\bold x$-vektor så att
den kvadratiska formen
\[ 
Q(x_1,\ldots,x_m)=
\sum_{p=1}^{p=m}\,
(a_{p1}x_1+a_{p2}x_2+\ldots+a_{pn}x_n-b_p)^2
\]
antar sitt minsta värde.
Under antagandet att $A$-matrisens kolonner
spänner ett
$n$-dimensionellt delrum av ${\bf{R}}^m$
bevisade   Gauss att (*) antar ett minimum för en unik vektor $\bold x^*$ 
som löser 
det homogena linjära ekvationssystemet
\[
\frac{\partial Q}{\partial x_j}(\bold x)=0\quad\colon\, 1\leq j\leq n
\]
Via deriveringsregler betyder detta att $\bold x^*$ löser  (*) om och endast om 
\[
\sum_{p=1}^{p=m}\,
a_{pj}\cdot (a_{p1}x^*_1+a_{p2}x^*_2+\ldots+a_{pn}x^*_n-b_p)=0\quad\, 1\leq j\leq m\tag{*}
\]



\medskip


\noindent
{\bf{Övning.}}
Ovan har vi en $(m,n)$-matris $A$, dvs. med $n$ kolonner och $m$-rader.
Vi erhåller den transponerade $(n,m)$-matrisen $A^*$ med element
\[
a^*_{qp}= a_{pq}
\]
Nu är $A^*A$ en kvadratisk  $(n,n)$-matris
med element
\[
\alpha _{jq}=
\sum_{p=1}^{p=m}
\, a^*_{jp}\cdot a_{pq}=
\sum_{p=1}^{p=m}
\, a_{pj}\cdot a_{pq}
\quad\, 1\leq j,q\leq n
\]

\medskip

\noindent
Notera att matrisen $A^*\cdot A$ är symmetrisk.
Teorem från XX från § XX visar att när
$A$-matrisens kolonner spänner ett $n$-dimensionellt
delrum av ${\bf{R}}^m$
följer att den kvadratiska  $(n,n)$-matrisen
$A^*A$
är inverterbar. Vidare noteras att
om vi inför $n$-vektorn 
$\beta_\bullet$ där
\[ 
\beta_j=\sum_{p=1}^{p=m}\, a_{pj}\cdot b_p\quad 1\leq j\leq n
\]
svarar (*) mot att
$n$-vektorn $\bold x^*$ löser ekvationen
\[
A^*A(\bold x^*)=\bold\beta_\bullet
\]
Eftersom matrisen $A^*A$ är inverterbar gäller  såväl
existens som entydighet  vilket bevisar satsen av Gauss !
\medskip


\noindent
{\bf{Bestämning av minimumvärdet.}}
Det återstår  att bestämma minsta värdet som 
ges av 
\[ 
Q(\bold x^*)= Q(x_1^*,\ldots,x_n^*)\tag{1}
\]
Notera  att 2:a gradspolynomet
$Q$ beror av $\bold b$-vektorn.
Så vi betecknar (1) med $V(\bold b)$, dvs. vi har en funktion
\[ 
\bold b\mapsto V(\bold b)
\]
där högerledet är noll om och endast om
det linjära systemet (*) har en lösning.
Enligt den metodik som
gavs av Gauss bestäms
$V(\bold b)$
genom att
införa vektorn $\beta_\bullet$ i (xx).
Därefter bestäms $\bold x^*$
av det linjära systemet (xx)
varefter man har likheten
\[
V(\bold b)= Q(\bold x^*)
\]


\bigskip


\centerline{\bf{Ett annat variationsproblem.}}
\medskip


\noindent
Låt $A$ vara en  inverterbar $(n,n)$-matris där $n\geq 2$.
Enhetsklotet  i ${\bf{R}}^n$ betecknas med $B^n$ som alltså är
mängden av $n$-vektorer  $\bold x$ med norm
$||\bold x||\leq 1$.
Givet en $n$-vektor $\bold b$ sökes
\[
\min_{\bold x\in B^n}\,
|| A(\bold x)-\bold b||^2= \sum_{p=1}^{p=n}\, (a_{p1}x_1+\ldots+a_{pn}x_n-b_p)^2\tag{*}
\]
\medskip


\noindent
Detta problem
behandlades av Joseph Lagrange.
Till att börja med betraktas bildmängden
\[ 
A(B^n)=\{ A(\bold x)\quad\, ||x||\leq 1\}\tag{i}
\]
Om $\bold b$ tillhör (i)
är minimum naturligtvis noll eftersom vi kan ta
$\bold x^*=\bold b$.
Men då $\bold b$ är utanför
(i) uppstår ett variationsproblem där
Lagrange
bevisade följande
under antagandet att $A$-matrisen är inverterbar.

\medskip

\noindent
{\bf{Teorem.}}
\emph{Variationsproblemet (*) har en unik lösning
där minimum antas av
en vektor $\bold x^*$ vars norm är lika med ett och satisfierar ett linjärt ekvationssystem}
\[
 \sum_{p=1}^{p=n}\, a_{pj}(a_{p1}x^*_1+\ldots+a_{pn}x^*_n-b_p)= \lambda\cdot x^*_j\quad\,1\leq j\leq n
\]
\emph{för ett unikt positivt tal
$\lambda$}.
\medskip


\noindent
{\bf{Kommentar.}}
Numera kallas $\lambda$ efter sin upptäckare för
\emph{Lagrangemultiplikatorn}.
Att $\lambda$ bestäms via
det linjära systemet i satsen ovan
beror på att
man  dessutom har det bindande villkoret
$||\bold x^*||=1$.

\medskip


\noindent
För att bevisa Teorem XX
omformuleras problemet till följande. Sätt
\[
\mathcal E=A(B^n)
\]
som är en ellipsoid i ${\bf{R}}^n$. Dess \emph{strikta konvexitet}
ger  en unik
vektor $\bold y^*$ vars spets ligger på randen av
$\mathcal E$ så att
\[
\min_{\bold y\in \mathcal E}\, ||\bold y-\bold b||=
||\bold y^*-\bold b||\tag{*}
\]
Eftersom
$A$-matrisen antogs inverterbar
erhålles  en unik vektor $\bold x^*$ 
där
\[ 
A(\bold x^*)=\bold y^*
\]
som ger den unika lösningen
till det ursprungliga variationsproblemet.
En poäng är att man nu kan
bestämma både $\bold y^*$ samt
\emph{minimumvärdet} hos  (*).
Metoden är att
gå över till ett \emph{nytt ortonomerat system}
där huvudaxlarna hos ellipsoiden $\mathcal E$
ligger längs koordinaxlarna.
Mer precist ger materialet i XXX  existensen
av en \emph{ortogonal matris}
$U$ så att
så att
när
\[ 
\bold u= U(\bold y)
\]
övergår ekvationen som bestämmer randen till $\mathcal E$ 
utryckt i  $\bold u$-koordinaterna till:
\[
\frac{\bold u_1}{d_1^2}+\ldots+\frac{\bold u_n}{d_n^2}=1\tag{i}
\]
Eftersom $U$ bevarar den euklidiska normen
återstår nu att lösa extremalproblemet
\[
\min_{\bold u\in \mathcal E}\||\bold u-\bold U(\bold b)||
\]
Om vi sätter
\[ 
\beta_\bullet=U(\bold b)
\]
antas minimum av vektorn
$\bold u^*$ som uppfyller 
\[
u^*_i-\beta_i=\lambda\cdot \frac{u^*_i}{d_i^2}\quad\, 1\leq i\leq n\tag{i}
\]
där $\lambda>0$ är Lagrangemultiplikatorn.
Divideras båda leden ovan erhålles
\[
\frac{u_i-\beta_i}{d_i}= \lambda\cdot \frac{u_i}{d_i}\implies
\]
\[ 
\sum_{i=1}^{i=n}\, 
(\frac{u^*_i-\beta_i}{d_i})^2=\lambda^2\cdot \sum_{i=1}^{i=n}\, (\frac{(u^*_i)^2}{d^2_i}= \lambda^2
\]
där den sista likheten följer från (i).
Eftersom $\lambda>0$ erhålles
\[
\lambda= \sqrt{
\sum_{i=1}^{i=n}\, 
(\frac{u_i-\beta_i}{d_i})^2}\tag{ii}
\]
Med detta $\lambda$-värde insatt i ekvationen (i)
erhålles det sökta minimumvärdet  hos (*) till det ursprungliga variationsproblemet och ges av formeln:
\[
\sqrt{\sum_{i=1}^{i=n}\,(u^*_i-\beta_i)^2}=
\sqrt{
\sum_{i=1}^{i=n}\, 
(\frac{u^*_i-\beta_i}{d_i})^2}\tag{iii}
\]










 

















 
 






\newpage


\centerline{\bf{Determinanter}}

\medskip


\noindent
Konstruktionen av determinanten 
bygger   på permutationers
associerade
 \emph{signumtal}.
 Mer precist betraktas heltalen $1,\ldots,n$ där $n\geq 2$.
En permutation
ges av en svit
\[ 
\bold j= (j_1,\ldots,j_n)\tag {i}
\] 
där de indicerade $j$-talen alla är olika och varje $j_\nu$  ett heltal mellan 1 och $n$.
Signaturen  hos $\bold j$ är nu +1 eller -1 beroende på om det fordas ett
jämnt eller ett udda antal platsbyten för att  återställa 
$\bold j$ till
den
neutrala permutationen $\bold n= (1\ldots,n)$.
Då  $n=3$ betraktas t.ex. permutationen
\[ 
(312)
\]
dvs. 1 flyttas till position 2, och 2 till 3 samt 3 till 1.
Här blir signaturen +1 eftersom det
endast fordas att
$3$ flyttas förbi 1 och 2. Däremot är signaturen hos permutation
\[ 
(3,2,1)
\]
-1.
För allmännt $n\geq 2$ betecknas signaturen hos en
permutation (i) med
$sign(\bold j)$ som alltså är +1 eller -1.

\medskip


\noindent
{\bf{Övning.}}
Visa att om två tal byter plats i en permutation medan
de övriga
kvarstår på sina  platser så ändras alltid permutationens tecken, dvs. från jämn till
udda eller vice versa.
Om exempelvis $n=6$ och vi har en permutation
\[
\bold j=(j_1,3,j_3,j_4,6,j_6)
\]
där alltså $j_2=3$ och $j_5=6$
så har permutationen
\[
\bold j^*=(j_1,6,j_3,j_4,5,j_6)
\]
omvänt tecken !
\medskip


\noindent
Notera också att antalet permutationer av $n$ heltal är lika med 
\[ 
n\,!= 2\cdot 3\cdots (n-1)n
\]
samt visa att precis  hälften av dessa permtutationer
har signatur  +1.
\medskip


\noindent
Mängden av permutationer av  $1,\ldots n$ utgör element i
en grupp betecknad med $S_n$ som kallas för den symmetriska gruppen av rang $n$.
Speciellt hör till
varje $\bold j$ dess invers  betecknad med
$\bold j^{-1}$.
\medskip

\noindent
{\bf{Övning}}
Visa att $\bold j$ och $\bold j^{-1}$ har
samma signatur.






\medskip

\noindent
Till varje permutation kan vi bilda en matrix där varje kolonn
har nollor överallt utom på en plats vars 
element är 1. Spelregeln är att
\[ 
a_{j_k,k}=1\colon\quad 1\leq k\leq n
\]
medan övriga  element är noll.
Till permutationen $(3,1,2)$ hör t.ex. matrisen
\[
A=\begin{pmatrix}0&0&1\\
1&0&0\\
0&1&0\\
\end{pmatrix}
\]
Sedan vi lärt oss att räkna ut determinanter
kan läsaren konfirmera att $\det(A)=1$ vilket svarar mot
signaturen hos $(3,1,2))$ är jämn.
Betraktas istället (3,2,1) vars signatur är -1
blir dess matris
\[ 
B=
\begin{pmatrix}0&0&1\\
0&1&0\\
1&0&0\\
\end{pmatrix}
\]
där vi ser att $\det(B)=-1$.


\newpage


\centerline{\bf{Konstruktion  av determinanten}}

\medskip

\noindent
Låt $A= \{a_{pq}\}$ vara en $(n,n)$-matris.
Då definieras
\[ 
det \, A= \sum\, sign(\bold j)\cdot a_{1,j_1}\cdot a_{2,j_2}\cdots a_{n,j_n}\tag{*}
\]
\medskip

\noindent
Notera att summan innehåller $n\,!$ många termer.

\medskip


\noindent
{\bf{Övning}}
Läsaren uppmanas att med stöd av signaturreglerna hos permutationer
visa följande utsagor från Torsten
Carlemans eminenta lärobok
i differential och integralkalkyl:
\medskip


\noindent
1.  \emph{Om två rader eller två kolonner byter plats med varandra så ändras determinantens tecken.
Speciellt gäller att om två kolonner eller två rader överensstämmer så är D=0}.
\medskip

\noindent
2. \emph{Om man till en rad eller kolonn tillägger
motsvarande element i en annan
rad, respektive kolonn
multiplicerade med samma konstant $\alpha$, så ändras ej determinantens värde.}
\medskip


\noindent
{\bf{Övning.}}
I (*) har determinanten räknats fram genom utveckling
efter matrisens kolonner.
Man har även formeln 
\[
det\, A=
\sum\, sign(\mathbf{v})\cdot a_{\nu_1,1}\cdot a_{\nu_2,2}\cdots a_{\nu_n,n}\tag{**}
\]
där summan nu tagits  över alla permutationer
av
$\bold{v}$.
För att visa likheten (*)= (**) utnyttjas att signaturen hos 
\emph{inversen} till varje permutation $\bold j$
är lika med $sign(\bold j)$.
Notera också
att om $A^*$ är den transponerade
matrisen med element
\[ 
a^*_{pq}= a_{qp}
\] 
så ger (**) likheten
\[ 
det(A)= det(A^*)
\]


\medskip

\noindent
{\bf{Produktsatsen.}}
Låt $A$ och $B$ vara två $(n,n)$-matriser
Deras produkt ges av matrisen $C$ med element
\[
c_{pq}= \sum_{\nu=1}^{\nu=n}\, a_{p\nu}\cdot b_{\nu q}
\]
Nu gäller  likheten
\[
det(AB)=det(A)\cdot det(B)\tag{*}
\]


\medskip


\noindent
{\bf{Övning.}} Bevisa (*).
Ledningen är att
\[
det(C) =\Sigma \Sigma\, \,sign(\nu_1,\ldots,\nu_n)\cdot a_{1,j_1}\cdots a_{n,j_n}\cdot b_{j_1,\nu_1}\cdots b_{j_n,\nu_n}\tag{i}
\]
varvid  summan tas över alla permutationer av $\mathbf \nu$ samt över \emph{alla} $n$-tupler
$j_1,\ldots,j_n$.
För varje fix $\bold j$-tupel sker summation över
$\bold \nu$ varvid $B$-matrisens determinant
räknas ut. Determinanters försvinnande från Övning 1 ovan 
medför att varje sådan summa är noll utom då
$n$-tupeln $j_1\ldots,j_n$ är en permutation
av
$1,\ldots n$ och för
varje 
sådan permutation gäller likheten
\[
\sum_{\bold\nu}
sign(\nu_1,\ldots,\nu_n)\cdot b_{j_1,\nu_1}\cdot b_{j_n,\nu_n}
= det(B)\cdot sign(\bold j)
\]
Alltså blir vänsterledet hos (i) lika med
\[
det(B)\cdot \sum\, sign(\bold j)\cdot a_{1j_1}\cdot a_{nj_n}
= det(B)\cdot det(A)
\]
\medskip


\noindent
{\bf{En gränsvärdesformel.}}
Betrakta två $(n,n)$-matriser $\Phi$ och $A$.
Till varje $\epsilon>0$ bildas matrisen
\[
\Psi=\Phi+\epsilon\cdot A\cdot \Phi
\]
\medskip


\noindent
{\bf{Övning.}}
Introducera $A$-matrisens spår
\[ 
Tr(A)= a_{11}+\ldots+a_{nn}
\]
och visa att
för små $\epsilon$ gäller
\[
\det(\Psi)= \det(\Phi)+\epsilon\cdot Tr(A)\cdot \det(\Phi)+ \bold O(\epsilon^2)\implies
\]
\[
\lim_{\epsilon\to 0}\,
\frac{\det(\Phi+\epsilon\cdot A)-\det(\Phi)}{\epsilon}= Tr(A)\cdot \det(\Phi)
\]
\medskip

\noindent
{\bf{Ledning.}}
Determinanten för $\phi+\epsilon\cdot A)$
ges av
\[
\sum\, sgn(j_1,\ldots,j_n)\cdot
\,\sum\, \phi_{1,j_1}+\epsilon\cdot \sum_{\nu=1}^{\nu=n}\, a_{1\nu}\phi_{\nu,j_1})\cdots
\,\sum\, \phi_{n,j_n}+\epsilon\cdot \sum_{\nu=1}^{\nu=n}\, a_{n\nu}\phi_{\nu,j_n})
\]
\medskip


\noindent
Den del av hela summan där ingen $\epsilon$-faktor förekommer
är lika med $\det(\Phi)$.
När vi bortser från
produktens termer där minst två $\epsilon$ förekommer
behåller man endast en
$A$-term. betrakta t.ex.
de linjära $\epsilon$-termer som
erhålles då vi bevarar en av termerna 
$\{\epsilon\cdot a_{1,\nu}\}$ där $1\leq \nu\leq n$.
Om $2\leq \nu\leq n$
erhålles då
\[
\epsilon\cdot a_{1,\nu}\cdot \sum\,sign(j_1\ldots,j_n)\cdot
\sum\,\phi_{\nu,j_1}\phi_{2,j_2}\cdots \phi_{n,j_n}\tag{i}
\]
Vi sder att (i) blir lika med noll om
$2\leq \nu\leq n$ eftersom
vi då räknat ut en determinant där
$\Phi$-matrisens $\nu$:te
kolonn fått ersätta den första.
Så det kvarstår
endast
en term då $\nu=1$ som ges av
\[ 
\epsilon\cdot a_{11}\cdot \det(\Phi)
\]
Liknande formel uppträder om vi för
$2\leq k\leq n$ bevarar
en av termerna $a_{k,\nu}$.
Så efter addition över $1\leq k\leq n$ erhålles
den efterfrågade gränsvärdesformeln.
\medskip


\noindent
{\bf{En nyttig gränsvärdesformel.}}
Låt A vara en $(n,n)$-matris
och för varje $\epsilon>0$
betraktas matrisen
\[ 
E_n+\epsilon\cdot A
\]
Vi bildar dess determinant och
då gäller följande gränsvärdesformel:
\[ 
\lim_{\epsilon\to 0}\,
\frac{\det(E_n+{\epsilon A}}{\epsilon}=\sum\sum\, (-1)^{p+q}\cdot a_{pq}\tag{*}
\] 
där dubbelsumman tas över alla $1\leq p,q\leq n$, dvs. vi har helt enkelt  tagit summan av
$A$-matrisens samtliga element.
\medskip

\noindent
{\bf{Övning.}} Visa (*) genom att använda
determinantens konstruktion och
att man vid  gränsövergången ignonerar
termer då två eller flera $\epsilon$-faktorer uppträder !
Här kan nämnas att
(*) kan utnyttjas för att ge ett  enkelt bevis för det välkända
\emph{fixpunktsteoremet} som säger
att om
$f$ är en kontinuerlig avbildning
från det slutna enhetsklotet $B^n$ till sig själv
så har $f$ minst en fixpunkt !
Se § XX för detaljer.

\medskip





\centerline{\bf{Några ytterligare exempel}}


\medskip

\noindent
Litteraturen om determinanter är mycket omfattande
där ett centralt tema är att finna allmänna formler som rymmer såväl ett kombinatorisk
som ett geometriskt innehåll.
Här följer några resultat.

\medskip

\noindent
{\bf{En sats av Sylvester.}}
Låt  $A=$ vara en $n,n)$-matris
och sätt
\[
b_{r-1,s-1}=a_{11}a_{rs}-a_{r1}a_{1s}\quad\,2\leq r,s\leq n
\]
Detta ger en $(n-1,n-1)$-matris $B$ där vi sätter
$b_{11}$ på plats (1,1) och så vidare.
\medskip

\noindent
{\bf{Teorem.}} \emph{Under antagandet att $a_{11}\neq 0$ gäller likheten}
\[
a_{11}^{n-2}\cdot det(A)= det(B)
\]

\noindent
{\bf{Sylvester's determinantformel}}.

\medskip

\noindent
Låt $N\geq 3$ och $S$ är en symmetrisk $N,N)$-matris:


\[
S=\begin{pmatrix}
s_{11}&s_{12}&\ldots &s_{1N}\\
s_{21}&s_{22}&\ldots &s_{2N}\\
\ldots&\ldots&\ldots&\ldots\\
\ldots&\ldots&\ldots&\ldots\\
s_{N1}&s_{N2}&\ldots &s_{NN}\\
\end{pmatrix}
\]
där vi alltså har $s_{pq}= s_{qp}$.
Nu konstrueras följande fyra  matriser där de tre första är av ordning $N-1$
och den sista av ording $N-2$.
\medskip

\[
S_1= 
\begin{pmatrix}
s_{22}&s_{23}&\ldots &s_{2N}\\
s_{32}&s_{33}&\ldots &s_{3N}\\
\ldots&\ldots&\ldots&\ldots\\
\ldots&\ldots&\ldots&\ldots\\
s_{N2}&s_{N3}&\ldots &s_{NN}\\
\end{pmatrix}
\quad\colon\quad 
S_2= 
\begin{pmatrix}
s\uuu{12}&s\uuu{13}&\ldots &s\uuu{1N}\\
s\uuu{22}&s\uuu{23}&\ldots &s\uuu{2N}\\
\ldots&\ldots&\ldots&\ldots\\
\ldots&\ldots&\ldots&\ldots\\
s\uuu{N-1,2}&s\uuu{N-1,3}&\ldots &s\uuu{N-1,N}\\
\end{pmatrix}
\]
\medskip
\[
S_3= 
\begin{pmatrix}
s\uuu{11}&s\uuu{12}&\ldots &s\uuu{1,N-1}\\
s\uuu{21}&s\uuu{22}&\ldots &s\uuu{2,N-1}\\
\ldots&\ldots&\ldots&\ldots\\
\ldots&\ldots&\ldots&\ldots\\
s\uuu{N-1,1}&s\uuu{N-1,2}&\ldots &s\uuu{N-1,N-1}\\
\end{pmatrix}
\]
\medskip

\noindent
Vi har också $(N-2,N-2)$ matrisen
som erhålles när
de extrema raderna och kolonnerna på plats 1 och N tagits bort:

\[ 
S_*=\begin{pmatrix}
s\uuu{22}&s\uuu{23}&\ldots &s\uuu{2,N-1}\\
s\uuu{32}&s\uuu{33}&\ldots &s\uuu{3,N-1}\\
\ldots&\ldots&\ldots&\ldots\\
\ldots&\ldots&\ldots&\ldots\\
s\uuu{2,N-1}&s\uuu{3,N-1}&\ldots &s\uuu{N-1,N-1}\\
\end{pmatrix}
\]
\medskip

\noindent
Med dessa betekningar
gäller följande likhet som bevisades av Sylvester 1840:

\[
\det(S)\cdot \det(S_*)=
\det S_1)\cdot \det S_3-\bigl(\det S_2\bigr)^2\tag{*}
\]











\bigskip

\newpage



\noindent
\centerline{\bf{Hankel matriser.}}

\medskip




\noindent
Let $\{c_0,c_1,\ldots\}$
vara en följ av komplexa tal.
Till varje par $(p,n)$ av icke-negativa  heltal
bildas $(p+1),p+1)$-matrisen:


\[
\mathcal C_ n^{(p)}=
\begin{pmatrix}
c_{n}&c_{n+1}&\ldots&c_{n+p}\\
c_{n+1}&c_{n+2}&\ldots&c_{n+p}\\
\ldots&\ldots&\ldots&\ldots\\
c_{n+p}&c_{n+p+1}&\ldots&c_{n+2p}\\
\end{pmatrix}
\]

\medskip


\noindent
Matrisens determinant betecknas med
$\mathcal D_n^{(p)}$.
 De kallas för de rekursiva Hankeldeterminanterna där det visar sig
 att de
 återspeglar egenskaper hos talföljden
 $\bold c= \{c_n\}$.
 Till att börja med definieras \emph{rangen}
 av $\bold c$:
 
\medskip

\noindent
{\bf{Definiion.}}
\emph{För varje $n\geq 0$ bildas
den oändliga vektorn}


\[ 
\xi_n=(c_n,c_{n+1},\ldots)
\]
\emph{Sviten $\{c_ n\}$ säges ha ändlig rang
om det existerar
ett heltal $r^*$ så att  vektorerna $\xi_1,\ldots,\xi_{r^*}$
är linjärt oberoende medan övriga vektorer är
linjära kombinationer av dessa.}
.
\medskip

\noindent
{\bf{Övning}}
Varje oändlig svit  $\{c_n\}$ definierar en \emph{formell potensserie}

\[
f(x)=\sum_{\nu=0}^\infty\, c_\nu x^\nu \tag{*}
\]
Till varje $n\geq 1$ sätter vi
\[
\phi_n(x)= x^{-n}\cdot(
f(x)-\sum_{\nu=0}^{n} c_\nu x^\nu)=
\sum_{\nu=0}^\infty c_{n+\nu} x^\nu
\]
Visa att
$\{c_n\}$ har ändlig rang
om och endast
om
följden
$\{\phi_n(x)\}$ generar ett ändligt-dimensionellt delrum
i det linjära rummet vars vektorer utgöres av
en godtyckligt vald potensserie.
\medskip

\noindent 
Använd sedan  detta för att
visa
att när
$\{c_n\}$ har ändlig rang så existerar  ett positivt heltal $p$
och en
$(p+1)$-tupel $(a_0,\ldots,a_p)$ av komplexa tal som inte allla är noll
där
potensserien

\[ 
a_0\cdot  \phi_0(x)+\ldots+a_p\cdot \phi_p(x)=0
\]
Multipliceras denna identitet med
$x^p$ följer att
\[
(a_p+a_{p-1} x+\ldots+a_0 x^p)\cdot f(x)=q(x)
\]
där $q(x)$ är ett polynom.
\medskip

\noindent
{\bf{Slutsats.}}
med stöd av Önvingen följer att
sviten $\{c_n\}$ har ändlig rang om och endast
om
den formella potensserien
(*) är en rationell funktion, dvs.
\[ 
f(x)= \frac{q(x)}{p(x)}
\]
\medskip


\noindent
{\bf{B.1 Övning.}}
Visa omvändningen, dvs.
antag att den formella potensserien
\[
\sum\, c_\nu\cdot x^\nu= \frac{q(x)}{p(x)}
\] 
för ett par av polynom och visa  att då har $\{c_n\}$ ändlig rang.
\medskip

\noindent
Till sist gäller följande resultat vars bevis lämnas som övning.


\medskip


\noindent
{\bf{B.2 Proposition.}}
\emph{Följden  $\{c_n\}$ har ändlig rang
om och endast om
det existerar ett heltal $p$ så att}

 \[
\mathcal D_0^{(p)}\neq 0\quad\&\quad
D_0^{(q)}=0\quad \colon\quad q>p\tag{4}
\]
\emph{Vidare gäller här likheten}
\[ 
p=r^*
\]


\centerline{\bf{Hankel's formel för Laurent serier.}}

\medskip


\noindent
Betrakta en rationell funktion
\[
R(z)= \frac{q(z)}{z^p-d_1z^{p-1}+\ldots
+d_{p-1}z+ d_p]}
\]
där graden hos polynomet
$q$ är  $\leq p-1$.
För tillräckligt stora värden hos absoluta belopp av $|z|$
existerar en konvergent Laurentserien:


\[ 
R(z)= \frac{c_0}{z}+ 
\frac{c_1}{z^2}+\frac{c_2}{z^3}+\ldots
\]
Inför nu $p\times p$-matrisen
\[
A=\begin{pmatrix}
0&0&&\ldots&0&c_p\\
1&0&0&\ldots&0&c_{p-1}\\
0&1&0&\ldots&\ldots&c_{p-2}\\
\ldots&
\ldots&
\ldots&
\ldots&\ldots&\ldots
\\
0&0&0&\ldots&1&c_1\\
\end{pmatrix}
\]
\medskip

\noindent
{\bf{Övning.}}
Visa likheten
\[ 
\mathcal D_n^{(p)}=\mathcal D_0^{(p)}\cdot \det(A)^n\quad\,\forall\, n\geq 1
\]



\medskip

\noindent
{\bf{B.6 Hadamard-Kroneckers identitet.}}
För varje par
av positiva tal $p$ ocgh $n$ gäller likheten


\[
\mathcal D_n^{(p+1)}\cdot
\mathcal D_{n+2}^{(p-2)}=
\mathcal D_ n^{(p+1)}
\mathcal D_{n+2}^{(p-1)}-
\bigl[\mathcal D_{n+1}^{(p)}\,\bigr]^2\tag{*}
\]



\newpage



\centerline{\bf{7. Hadamard's spektralradius teorem.}}

\bigskip

\noindent
Hadamard's avhandling  \emph{Essais sur l'études des fonctions donnés par leur
dévelopment d Taylor} innehåller ett mycket rikt  material.
Här skall vi
presentera resultat från
dess andra kapitel.
Betrakta en potensserie
\[
 f(z)=\sum\, c_nz^n
\]
vars konvergensradie är ett positivt tal $\rho$
vilket betyder att $f(z)$ är en komplexanalytisk 
funktion i
den öppna cirkelskivan av radie $\rho$ men har sedan minst en singulär punkt
på cirkeln
$\{|z|=\rho\}$. Vi påminner om det klassiska resultat 
som säger att konvergensradien ges av formeln
\[
\rho= \limsup_{n\to+\infty}\, |c_n|^{\frac{1}{n}}\tag{*}
\]
\medskip


\noindent
Hadamard visade i sin avhandling  att egenskaper hos
$\{c_n\}$-svitens
rekursiva Hankel-matriser
och deras tillhörande determinanter
klargör när $f$:s singulariter på
$|z|=\rho$ består av poler och  resultatet i 
nedanstående Teorem xx
bestämmer 
antalet poler - räknade med multiplicitet - i
en cirkel med radie strikt större än $\rho$.
Till att börja med definieras följande tal för varje $p\geq 1$.

\[
\delta(p)=\, 
\limsup_{n\to \infty}\, 
[\mathcal D_n^{(p)}]^{\frac{1}{n}}\tag{i}
\]

\noindent
Eftersom $\{\mathcal D_n^{(0)}\}=\{c_n\}$ gäller enligt det gjorda antagandet:
\[
\delta(0)= \frac{1}{\rho}=\limsup_{n\to \infty}\, |c_n|^{\frac{1}{n}}\tag{ii}
\]
Så för varje $\epsilon>0$  existerar en  konstant $C_\epsilon$ 
så att
\[ 
|c_n|\leq C\cdot (\rho -\epsilon)^{-n}\quad\, n=1,2,\ldots\implies
\]


\[
|\mathcal D_n^{(p)}|\leq (p+1) !\cdot C_\epsilon^{p+1}(\rho-\epsilon)^{-(p+1)n}\implies
\]
\[
\delta(p)= \limsup_{n\to \infty}\, 
\bigl[\mathcal D_n^{(p)}\bigr]^{\frac{1}{n}}\leq \rho^{-(p+1)}\tag{iii}
\]

\medskip


\noindent
Antag nu att det existerar
något $p\geq 1$ där man har en  strikt olikhet

\[
\delta(p)<\rho^{\vvv(p+1)}\tag{1}
\]
och låt  $p$  vara det minsta talet där (2) gäller. Så för detta val av $p$ finns ett
tal $\rho_*>\rho$ så att
\[
\delta(p)=\rho_*^{-1}\cdot
\rho ^{-p}\tag{2}
\]
Med dessa beteckningar kan vi annonsera Hadamards vackra resultat.




\medskip

\noindent
{\bf{Teorem.}} \emph{Med $p$ minimalt vald enligt ovan
följer att
$f(z)$ fortsätter till en
meromorf  funktion i cirkelskivan med radie
$\rho_*$ där  antalet
poler räknade med multiplicitet  högst  är $p$.}


\bigskip


\noindent
Beviset fordrar  flera steg.
Till att börja med har man följande resultat vars bevis lämnas som en - relativt svår - övning åt läsaren.



\medskip

\noindent
{\bf{Lemma. }}\emph{Det minimala valet av  $p$ 
medför att man har en  - unrestricted - gränsvärdesformel:}

\[
\lim_{n\to \infty}\, 
\bigl[\mathcal D\uuu n^{(p-1)}\bigr]^{\frac{1}{n}}=
\rho ^{-p}\tag{*}
\]

\bigskip


\newpage


\noindent
\centerline {\bf{Den meromorfa fortsättningen till 
 $\{|z|<\rho_*\}$.}}
\medskip


\noindent
Lemma A medför att det existerar ett positivt
heltal $n_*$ så att

\[
n\geq n_*\implies \{\mathcal D_n^{(p-1)}\neq 0
\]
Detta ger för varje $n\geq n_*$
en unik $p$-vektor
$(A_n^{(1)},\ldots, A_n^{(p)})$ som löser ekvationerna:
which solves the inhomogeneous system
\[
\sum_{k=0}^{k=p-1}
\, c_{n+k+j}\cdot A_n^{(p-k)}
=-c_{n+p+j}\quad\colon\quad 0\leq j\leq p-1
\]
vilket på matrisform kan skrivas:

\[
\begin{pmatrix}
c_{n}&c_{n+1}&\ldots&c_{n+p-1}\\
c\uuu{n+1}&c\uuu{n+2}&\ldots&c\uuu{n+p}\\
\ldots&\ldots&\ldots&\ldots\\
c\uuu{n+p-1}&c\uuu{n+p}&\ldots&c\uuu{n+2p-2}\\
\end{pmatrix}\,
\begin{pmatrix}A_n^{(p)}\\\ldots\\\ldots\\\ldots\\
A_n^{(1)}\end{pmatrix}=-
\begin{pmatrix}c_{n+p} \\\ldots\\\ldots\\\ldots\\
c_{n+2p-1}\end{pmatrix}\tag{*}
\]

\medskip


\noindent
{\bf{Övning}}. Sätt
\[
H_n=
c_{n+2p}+ A_n^{(1)}\cdot  c_{n+2p\vvv 1}+
\ldots+ A_n^{[(p)} \cdot c_{n+p}
\]
och visa att en uträkning av
$\mathcal D\uuu n^{(p)}$ 
visa med utvecklning av den sista
kolumnen i Hankel-matrisen $\mathcal C_n{(p)}$
att man har likheten:

\[ 
H_ n=
\frac{\mathcal D_n^{(p)}}{\mathcal D_n^{(p-1)}}\tag{i}
\]
\medskip

\noindent
Nu visar  gränsvärdesformeln (2) samt Lemma XX
att det för varje $\epsilon>0$ existerar
en konstant $C_\epsilon$ så att följande olikhet gäller för tillräckligt stora $n$:

\[ 
|H_n|\leq C_\epsilon \cdot 
\bigl(\frac{\rho+\epsilon}{\rho_ *-\epsilon}\bigr)^n\tag{ii}
\]

\noindent
Next, 
put
\[ 
\delta\uuu n^{k}=A\uuu {n+1}^{(k)}\vvv A\uuu n^{(k)}
\quad\colon\quad 1\leq k\leq p\tag{iii}
\]

\medskip

\noindent
Solving (*) above for $n$ and $n+1$ a computation shows that
the $\delta$\vvv numbers satisfy the system
\[
\sum\uuu{k=0}^{k=p\vvv 1}
\, 
c\uuu {n+j+k+1}\cdot \delta \uuu n^{(p\vvv k)}=0
\quad\colon\quad 0\leq j\leq p\vvv 2
\]
\[
\sum\uuu{k=0}^{k=p\vvv 1}
\, 
c\uuu {n+p+k}\cdot \delta \uuu n^{(p\vvv k)}=
\vvv (c\uuu{n+2p}+ A\uuu n^{(1)}\cdot  c\uuu{n+2p\vvv 1}+
\ldots+ A\uuu n^{[(p)} \cdot c\uuu{n+p})
\tag{iv}
\]
\medskip



\noindent
The $\delta$\vvv numbers in the linear system ( iv)
are found  via Cramer's rule. 
The minors of degree $p\vvv 1$ in the Hankel matrices 
$\mathcal C\uuu {n+1}^{(p\vvv 1)}$ have elements from
the given
$c$\vvv sequence and  (7.0)  implies
that every such minor has an absolute value majorized by
\[
C\cdot (\rho\vvv \epsilon)^{\vvv (p\vvv 1)n}
\] 
where $C$ is a constant 
which is independent of $n$.
We conclude that the $\delta$\vvv numbers satisfy
\[
|\delta \uuu n^{(k)}|\leq |\mathcal D\uuu n^{(p\vvv 1)}|^{\vvv 1}\cdot 
C\cdot (\rho\vvv \epsilon)^{\vvv (p\vvv 1)n}\cdot |H\uuu n|\tag{v}
\]


\noindent
The unrestricted limit in Lemma 7.2
give  upper bounds for
 $|\mathcal D\uuu n^{(p\vvv 1)}|^{\vvv 1}$ so that  (iii) and (v) give:
 
\medskip
 
 \noindent
{\bf{7.5 Lemma}}
 \emph{To each $\epsilon>0$ there is a constant
 $C\uuu\epsilon$ such that}
 \[
|\delta \uuu n^{(k)}|\leq 
C\uuu\epsilon\cdot
 \bigl(\frac{\rho+\epsilon}{\rho\uuu *\vvv \epsilon}\bigr)^n
 \quad\colon\quad 1\leq k\leq p
\]
\medskip


\noindent
{\bf{7.6 The polynomial $Q(z)$}}.
Lemma 7.5  and (iii) entail that
the sequence $\{A\uuu n^{(k)}\,\colon\, n=1,2,\ldots\}$
converges for every $k$ and  we set
\[
A\uuu *^{(k)}=\lim_{n\to\infty}\, A\uuu n^{(k)}\,
\]
 Notice   that Lemma 7.5 after summations of geometric series gives
a constant $C_1$ such that
\[
|A\uuu *^{(k)}\vvv A\uuu n^{(k)}|\leq C_1\cdot 
\bigl(\frac{\rho+\epsilon}{\rho\uuu *\vvv \epsilon}\bigr)^n\tag{7.6.i}
\]
hold for every $1\leq k\leq p$ and every $n$.

\noindent
Now we consider the sequence
\[
b\uuu n=
 c\uuu{n+p}+ A\uuu *^{(1)}\cdot c\uuu {n+p\vvv 1}+
\ldots A\uuu *^{(p)}\cdot c\uuu n\tag{7.6.ii}
 \]
Equation (*) applied to   $j=0$  gives
\[
b\uuu n= 
(A\uuu *^{(1)}\vvv A\uuu n ^{(1)})
\cdot c\uuu {n+p\vvv 1}+
\ldots +(A\uuu *^{(p)}\vvv A\uuu n^{(p)})\cdot c\uuu n\tag{7.6.iii}
\]

\medskip

\noindent
Next, we have already seen that $|c\uuu n|\leq C\cdot(\rho\vvv \epsilon)^{\vvv n}$
hold for some constant $C$ which
together with (7.6.i)  gives:
\medskip

\noindent
{\bf{7.7 Lemma.}}
\emph{For every $\epsilon>0$ there exists a constant $C$ such that}
\[
|b\uuu n|\leq C\cdot \bigl(\frac{1+\epsilon}{\rho\uuu *}\bigr)^n
\]
\medskip

\noindent
Finally, consider the polynomial
\[
Q(z)=  1+ A\uuu *^{(1)}\cdot z+
\ldots A\uuu *^{(p)}\cdot z^p
\]

\noindent
Set $g(z)= Q(z)f(z)$ which has a power series
$\sum\, d_\nu z^\nu$
where 
\[
b_n=
c_n\cdot   A_*^{(p)}+\ldots
c_{n+p-1}A_*^{(1)} +c_{n+p}=d_{n+p}
\]
\medskip


\noindent
Above $p$ is fixed so Lemma 7.7 and the trivial spectral radius formula 
show that
$g(z)$ is analytic in the disc $|z|<\rho_*$. This
proves that $f$ extends and the poles are contained in
the zeros of the polynomial $Q$ which occur  in the annulus
$\rho\leq |z|<\rho_*$.







 
 























\noindent






















\newpage
\centerline{\bf{Cramers inversionsformel}}

\medskip


\noindent
Låt $A$ vara en $(n,n)$-matris där $detA)\neq 0$.
Vi skall  konstruera dess inversa matris  genom att använda   minorer - dvs. underdeterminanter - till
$A$.
Låt oss först  ta bort matrisens första kolonn och betrakta matrisen

\[
B_1=
\begin{pmatrix}
a_{12}&a_{13}&\ldots  &a_{1n}\\
a_{22}&a_{23}&\ldots &a_{2n}\\
\ldots&\ldots&\ldots&\ldots\\
\ldots&\ldots&\ldots&\ldots\\
a_{n2}&a_{n3}&\ldots &a_{nn}\\
\end{pmatrix}
\]
Genom att stryka
en av rad erhålles en kvadratisk $(n-1)$-matris.
För varje $1\leq i\leq n$ sätter vi
\[
M(1,i)=
det\begin{pmatrix}
a_{12}&a_{13}&\ldots  &a_{1n}\\
\ldots&\ldots&\ldots&\ldots\\
a_{i-1,2}&a_{i-1,3}&\ldots &a_{i-2,n}\\
a_{i+1,2}&a_{i+1,3}&\ldots &a_{i+1,n}\\
\ldots&\ldots&\ldots&\ldots\\
a_{n2}&a_{n3}&\ldots &a_{nn}\\
\end{pmatrix}
\]
Låt oss  nu räkna fram $det(A)$ via
utveckling efter dess första kolonn som ger:
\[
det(A)= \sum_{i=1}^{i=n}\, a_{i1}\cdot (\sum_{(i)}\, sign(j_1,\ldots,j_{n-1})\cdot  a_{j_1,2}\cdots a_{j_{n-1},n})
\]
där den inre summan för varje fixt $i$ tagits över permutationer 
som utesluter $i$ och därför blir $M(1,i)$.
Teckenregel för permutationer ger 
\[ 
sgn(i,j_2,\ldots,j_n)=(-1)^{1+i}\cdot sgn(i,j_2,\ldots,j_n)\implies
\]
\[ 
det(A)=\sum_{i=1}^{n}\, a_{1i}\cdot (-1)^{i+1}\cdot M(1,i)
\]

\medskip

\noindent
Om vi  ersätter $\{a_{1i}\}$ med $\{a_{ki}\}$ för något $2\leq k\leq n$
men behåller $B$-matrisen erhålles
en nollsumma eftersom 
determinanten hos en matris med två identiska kolonner är noll. Alltså är
\[ 
\sum_{i=1}^{n}\, a_{ki}\cdot (-1)^{i+1}\cdot M(1,i)=0\quad\colon\, 2\leq k\leq n
\]
\medskip


\noindent
Vi kan utföra likartade konstruktioner där  vi för varje $2\leq k\leq n$
betraktar  matrisen $B_k$ som uppstår genom att ta bort kolonn på plats $k$.
Precis som i fallet $k=1$ bildas nya minorer
$M(k,i)$ och
teckenregler för permuationer ger:
\[ 
det (A)= \sum_{i=1}^{n}\, a_{ki}\cdot (-1)^{i+k}\cdot M(k,i)
\]
samt försvinnande summor
\[ 
0=\sum_{i=1}^{n}\, a_{\nu i}\cdot (-1)^{i+k}\cdot M(k,i)\quad\, \nu\neq k
\]

\medskip


\noindent
{\bf{Slutsats}}.
Definiera  $(n,n)$-matrisen
$\bold {Cr}$
där elementen ges av 
\[ 
c_{ik}= (-1)^{i+k} M(k,i)
\]
där ekvationerna ovan medför att 
matrisprodukten $A\cdot \bold{Cr}$ är lika med enhetsmatrisen $E_n$.





\newpage


\centerline {\bf{Jordans
inversion av matriser.}}
\medskip


\noindent
Låt $A$ vara en inverterbar $(n,n)$-matris.
Vi skall  konstruera
$A^{-1}$
genom att bilda en följd av
matriser
 $\{\bold J_1,\ldots\bold J_{n-1}\}$
 så att matrisprodukten
 \[
 \bold J_{n-1}\cdots\bold J_1\cdot A=D\tag{*}
 \]
 där $D$ är en diagonalmatris. Från (*) följer sedan att
 \[
 A^{-1}= D^{-1}\cdot 
 \bold J_{n-1}^{-1}\cdots\bold J_1`{-1}\tag{**}
\]
$\bold J$-matriserna konstrueras  
rekursivt där diagonalelementen
hos  varje matris alla är ettor, och för
varje fixt $r$
avviker 
$\bold J_r$
endast från enhetsmatrisen
$E_n$ i sin  $r$:te kolonn. 
Med andra ord, elementen $\{j_{pq}(r)$ hos  $\bold J_r$
uppfyller:
\[
q\neq r\implies j_{pq}(r)=\Delta(p,q)
\] 
där $\Delta$ är Kronecker's deltasymbol som 
antar värdet 1 om och endast om $p=q$ 
\medskip


\noindent
I första steget är $\bold J_1$
matrisen vars element
ges av 
\[
\bold J_1(1,1)=1\quad\&\quad
\bold J_1(i,1)=
-\frac{a_{i1}}{a_{11}}\quad\, i\geq 2\tag{i}
\]
\[ 
q\geq 2\implies \bold J_1(i,q)= \Delta(i,q)\tag{ii}
\]
Sätt
\[ 
A_1=\bold J_1A
\]
och definiera matrisen $\bold J_2$ genom 
\[
\bold J_2(2,2)=1\quad\&\quad
\bold J_2(i,2)=
-\frac{A_1(i1)}{A_1(i,1)}\quad\, i\neq 2
\]
medan övriga kolonner har
endast en etta längs diagonalen.
Därefter sätter vi $\bold A_2=\bold J_2\bold J_1\cdot A$
och upprepar konstruktionen.
Till sist erhålles matrisen
\[
A_{n-1}= \bold J_{n-1}\cdot\bold J_1\cdot A
\]

\medskip

\noindent
{\bf{Övning.}}
Visa att $A_{n-1}$ är en diagonalmatris. En ledning är att
för varje $1\leq r\leq n-1$ innebär konstruktionen att för  varje $1\leq r\leq n-1$ och
alla $i\neq r$ gäller likheten nedan för alla $j$:
\[
A_r(ij)=
A_{r-1}(i,j) + \bold J_r(i,r)\cdot A_{r-1}(rj)\quad\, i\neq r
\]
\medskip


\noindent
{\bf{Exempel.}}
Låt $n=2$ och
\[
A= \begin{pmatrix}1&c\\
b&d\\
\end{pmatrix}
\]
där det antas att 
determinanten $d-bc\neq 0$.
Vi får 
\[
\bold J_1=\begin{pmatrix}1&0\\  
-b&1\\
\end{pmatrix}\,\quad \&\quad A_1=\begin{pmatrix}1&c\\
0&-bc+d\\
\end{pmatrix}
\]
I nästa steg ger konstruktionen av $J_2$ att
\[
\bold J_2=\begin{pmatrix}1&\frac{-c}{d-bc}\\  
0&1\\
\end{pmatrix}\implies
A_2=\bold J_2\cdot A_1=\begin{pmatrix}1&0\\  
0&d-bc\\
\end{pmatrix}
\]
där sista termen är en diagonalmatris.
\medskip

\noindent
{\bf{Exampel  2.}}
Betrakta 
matrisen 
\[
A=\begin{pmatrix}1&a\\  
0&1\\
\end{pmatrix}
\]
\medskip
Läsaren bör nu verifiera att
\[ 
\bold J_1=E_2\implies A_1=A\implies
\]
\[ 
\bold J_2= \begin{pmatrix}1&-a\\  
0&1\\
\end{pmatrix}\implies
A\bold J_2=\begin{pmatrix}1&a\\  
0&1\\
\end{pmatrix}\cdot
\begin{pmatrix}1&-a\\  
0&1\\
\end{pmatrix}=E_2
\]





































































\newpage





\centerline{\bf{Grams determinantformel}}.
\medskip


\noindent
Varje $(n,n)$-matris svarar mot att
bilda $n$ många kolonnvektorer. Med andra ord, låt
$\bold v_1,\ldots,\bold v_n$ vara vektorer i
$\bf{R^n}$ där vi för varje $1\leq q\leq n$ skriver
\[
\bold v_q=v_{1q}\bold e_1+\ldots+v_{nq}\cdot \bold e_n
\]
varvid $\bold e_1,\ldots,\bold e_n$ är den euklidiska basen i
${\bf{R}}^n$.
Om $\bold V$ betecknar den \emph{ordnade} $n$-tupeln
$\bold v_1,\ldots,\bold v_n$ bildas matrisen
\[
A(\bold V)= \begin{pmatrix}
v_{11}&v_{12}&\ldots  &v_{1n}\\
v_{21}&v_{22}&\ldots &v_{2n}\\
\ldots&\ldots&\ldots&\ldots\\
\ldots&\ldots&\ldots&\ldots\\
v_{n1}&v_{n2}&\ldots &v_{nn}\\
\end{pmatrix}
\]
\medskip


\noindent
{\bf{Övning}}.
Låt $A(\bold V^*)$ vara den transponerade matrisen.
Visa att produkten
$A(\bold V)\cdot A(\bold V^*)$ är  den symmetriska matrisen
\[
\begin{pmatrix}
\langle \bold v_1,\bold v_1\rangle
&
\langle \bold v_1,\bold v_2\rangle
&\ldots  &\langle\bold v_1,\bold v_n\rangle\\
\langle \bold v_2,\bold v_1\rangle
&
\langle \bold v_2,\bold v_2\rangle
&\ldots  &\langle\bold v_2,\bold v_n\rangle\\

\ldots&\ldots&\ldots&\ldots\\
\ldots&\ldots&\ldots&\ldots \\
\langle \bold v_n,\bold v_1\rangle
&
\langle \bold v_n,\bold v_2\rangle
&\ldots  &\langle\bold v_n,\bold v_n\rangle\\

\end{pmatrix}
\]
där matrisens element bildats genom att
ta den inre produkten mellan
$\bold v$-vektorerna.
Matrisen  betecknas med $Gr(\bold V)$ och kallas för Gram-matrisen som hör till
den ordnade
mängden $\bold V$ av $n$-vektorer.
Produktregeln för matriser
ger likheten
\[
\det(A(\bold V))^2=\text{det}\, Gr(\bold V)
\]
\medskip

\noindent
{\bf{Ortogonala matriser.}}
En $n$-tupel 
av vektorer $\bold w_1,\ldots,\bold w_n$ ger en \emph{ortonormerad bas}
i ${\bf{R}}^n$
då
\[
\langle \bold w_i,\bold w_i\rangle=1\quad 1\leq i\leq n\quad \,\&\quad
i\neq k \implies\, \langle \bold w_i,\bold w_k\rangle=0
\] 
Detta svarar mot att $Gr(\bold W)$ är enhetsmatrisen så att likheten i Övning 1
ger
\[
det(A(\bold W))^2=1\tag{i}
\]
dvs. matrisens determinant är +1 eller -1.



\medskip



\noindent
{\bf{Teorem.}}
Om $S=\bold A(\bold W))$
där $\bold W=(\bold w_1,\cdots,\bold w_n)$
är en ortonormerad bas så gäller likheten
\[ 
S^{-1}= S^*\tag{*}
\]
\medskip

\noindent
\emph{Bevis}.
Notera att element 
$t_{pq}$ i matrisen 
$S^*S$ ges av
\[
t_{pq}= \sum_{i=1}^{i=n}\, 
s^*_{pi}\cdot s_{iq}=
\sum_{i=1}^{i=n}\, 
s_{ip}\cdot s_{iq}= \langle \bold w_p,\bold w_q\rangle
\]
Eftersom $\bold W$
enligt antagande utgör ett ortonormerad familj
följer (*).


\newpage

\noindent
{\bf{Övning.}}
Visa att varje ortogonal matris
\emph{bevarar} den euklidiska  inre produkten.
Med andra ord, om $S= A(\bold W)$ för etty  ortonormerat system $\bold W$ 
så gäller likheten
\[
\langle S\mathbf u,S\bold v\rangle= \langle \bold u,\bold v\rangle\tag{**}
\]
för alla vektorpar $\bold u,\bold v$.
\medskip


\noindent
{\bf{Gruppen $SO(n)$.}}
Då $S= A(\bold W)$ är ortogonal
har vi redan noterat att Grams determinantformel medför att
dess kvadrerade determinant antar värdet 1.
Alltså blir $det(S)$ lika med +1 eller -1.
Beteckna med $SO(n)$ alla ortogonala matriser vars  determinant  är +1.
Läsaren bör vidimera att om $S$ och $T$ är två ortogonala matriser så är deras produkt $ST$ också ortogonoal.
Av detta följer att $SO(n)$ är en \emph{grupp}
vars neutrala element är enhetsmatrisen.
Vi hänvisar till § XX för
ett
studium i fallet $n=3$.









\newpage

\noindent
\centerline{\bf{Fallet $n=3$.}}
\medskip


\noindent
Gruppen $SO(3)$ 
har speciellt intresse och användes bland annat av Leonard Euler i hans
studium av stela kroppars kinematik.
En geometrisk bild för att framställa element i $SO(3)$.
erhålles genom att betrakta enhetssfären
$\bold S^2$ i ${\bf{R}}^3$.
En ortogonal matris $S$ avbildar de tre euklidiska basvektorerna till
parvist ortogonala enhetsvektorer.
Betrakta först bildvektorn av
$\bold e_x= (1,0,0)$:
\[ 
S(\bold e_x)= \bold u\tag{1}
\]
Med en bild av $S^2$ som  kan åskådliggöras genom att
sitta framför en jordglob
kan läsaren konfirmera
att
varje enhetsvektorer $\bold v$ som är $\perp$ till $\bold u$
måste ligga på
en storcirkel. Då exempelvis $\bold u=(0,0,1)$ har sin spets i nordpolen ligger $\bold v$ på ekvatorcirkeln.
Vidare gäller att sedan  
\[
S(\bold e_y)=\bold v\tag{2}
\]
fastlagts
så bestäms 
vektorn $S(\bold e_z)$
\emph{entydigt bestämd} av
(1-2) när $S$-matrisen tillhör $SO(3$.
Mer precist gäller:

\medskip

\noindent
{\bf{Teorem.}}
\emph{För en ortogonal matris $S\in SO(3)$
där enhetsvektorer $\bold u,\bold v$ blivit bestämda enligt ovan, följer att}
\[
S(\bold e_z)= \bold u\times\bold v
\]
\emph{där högerledet är  kryssprodukten
av paret $\bold u,\bold v$.}
\medskip

\noindent
Vi påminner här om att  kryssproduktens koordinater
ges av följande formel:
\[
\bold u\times\bold v= (u_y\cdot v_z-u_z\cdot v_y)\cdot \bold e_x+
(u_xv_z-u_zv_x)\cdot \bold e_y+ (u_xv_y-u_yv_x)\cdot \bold e_z\tag{*}
\]
\medskip


\noindent
Vi påminner om att  (*) baseras på de tre fundamentala likheterna
\[ 
\bold e_x\times\bold e_y=\bold e_z\quad \bold e_x\bold e_z=-\bold e_y\tag{**}
\quad \bold e_y\times\bold e_z=\bold e_x
\]
\medskip


\noindent
{\bf{En determinantformel}}.
Bilda en (3,3)-matris med
kolonnvektorerna
$\bold u,\bold v,\bold w$.
Då gäller likheten:
\[ 
\det A(\bold u,\bold v,\bold w)=\langle \bold u\times \bold v,\bold w\rangle
\]
Läsaren uppmanas att visa (*) med en direkt kalkyl.
\medskip


\noindent
{\bf{Åskådlig mekanik.}}
Utvecklingen och behovet av 
matriskalkyler
initierades av Euler i hans banbrytande studier av stela kroppars rörelse i
${\bf{R}}^3$.
Mer precist betraktas en stel kropp $K$
som kan flyttas i ${\bf{R}}^3$ 
där stelheten betyder att inbördes avstånd mellan punktpar
$\bold p,\bold q$ inte ändras.
Euler införde  ett \emph{kroppsfixerat} ortonormerat system
som vi betecknar med $\bold V_K$.
Det betyder att man väljer
en
central punkt $\bold o\in K$
samt ytterligare 
tre punkter
$\bold k_1,\bold k_2,\bold k_3$
där vektorena
\[
\bold e_\nu=\bold k_\nu-\bold o\quad 1\leq \nu\leq 3
\]
utgör ett positivt ortonormerat systyem, dvs.
för deras inbördes vektorprodukter gäller i analogi med (*) att

\[ 
\bold e_1\times\bold e_2=\bold e_3\quad \bold e_1\bold e_3=-\bold e_2\tag{**}
\quad \bold e_2\times\bold e_3=\bold e_1
\]
\medskip

\noindent
Med dessa beteckningar
följer  att
när  $K$ flyttas från en specifik position
där 
de fyra vektorerna $\bold o,\bold p_1,\bold p_2,\bold p_3$ 
från början har
föreskrivna positioner i ${\bf{R}}^3$,
så bestäms kroppens nya position dels av
att $\bold o$ flyttas till
en punkt $\bold o^*$ och för
$\bold p$-punkterna gäller 
\[
\bold p_*=\bold o^*+ S(\bold p-\bold o)\tag{*}
\]
där $S$ är en ortogonal  $(3,3)$-matris
som svarar mot hur den stela kroppens förflyttning
inte bara sker med en translation utan dessutom har den \emph{roterats}.
Under antagandet
att en sådan förflytting sker
"kontinuerligt" så bevaras kroppens urspungliga orientering varför
matrisen $S$ alltid tillhör $SO(3)$ och kallas för rotationsmatrisen.
\medskip

\noindent
För att
erhålla en effektiv kalkyl
betraktade Euler således  \emph{två distinkta} ortonormerade rum, nämligen
"åskådarens" $(x,y,z)$-rum samt
det kroppfixerade rummet
$\bold V_K$.
En \emph{subtil  poäng} är att
rotationsmatrisen $S$
i (*) inte beror på valet av $\bold o$. Mer precist bör läsaren vidimera att när
(*) gäller så följer att  för \emph{varje punkt} $\bold q$
i den stela kroppen gäller  likheten:
\[ 
\bold p^*=\bold q^*+ S(\bold p-\bold q)\quad\forall\, \bold p\in \bold V_K\tag{*}
\]
Rotationsmatrisen som uppstår vid en stel förflyttning är alltså \emph{oberoende} av
den punkt i $K$ som väljes för att beskriva en "ren translation".
\medskip

\noindent
{\bf{Den Eulerska vinkelhastigheten.}}
Låt $t$  vara en tidsvariabel och
låt oss nu studera en tidsberoende
stel rörelse. Till att börja med
kan man följa
rörelsen hos en vald punkt
$\bold o\in K$ som  ger
en "vanlig" vektorvärd funktion"
\[ 
t\mapsto \bold q(t)\tag{1}
\]
Dessutom  bestäms rotationsmatrisen
vid varje tidpunkt, dvs. man har en tidsberoende
$SO(3)$-värd funktion
\[ 
t\mapsto S_t\tag{2}
\]
Den tidsberoende rörelsen hos en
punkt $\bold p\in K$ ges nu  av
\[ 
\bold p^*(t)= \bold q^*(t)+S_t(\bold p-\bold q)\tag{3}
\]
Vi kan bilda  tidsderivatan som i vänsterledet ger
\emph{hastighetsvektorn} 
för 
$\bold p^*$ och (3) ger likheten 
\[
\frac{d}{dt}(\bold p^*(t)=\frac{d}{dt}\bold q^*(t)+
\frac{d}{dt}S_t(\bold p-\bold q)\tag{4}
\]
\medskip


\noindent
{\bf{Den Eulerska vinkelhastigheten.}}
I (4) har vi tagit tidsderivatan av $t\mapsto S_t$ som är en linjär operator från
$\bold V_K$ till
åskådarens rum.
Vi kan nu bilda dess invers och
får alltså en linjär operator på $\bold V_K$ definierad genom
\[
W_t=S_t^*\circ \frac{d}{dt}S_t
\]
Den ortonormerade basen $\bold e_1,\bold e_2,\bold e_3$ bestämmer en inre produkt
och eftersom $S_t$ är  ortogonal gäller
\[
\langle S_t\bold u,S_t\bold v\rangle= \langle \bold u,\bold \rangle\tag{i}
\]
för alla vektorpar $\bold u,\bold v$ i $\bold V_K$.
Vänsterledets tidsderivata är alltså
noll
vilket ger likheten
\[
0=\langle \frac{d}{dt}S_t\bold u,S_t\bold v\rangle+
\langle S_t\bold u, \frac{d}{dt}S_t\bold v\rangle
\tag{ii}
\]
Nu noteras att man har likheterna 
\[
\langle S_t^*\circ \frac{d}{dt}S_t\bold u,\bold v\rangle
\quad\&\quad \langle S_t\bold u,S_t^*\circ \frac{d}{dt}S_t\bold v,\rangle
\]
varför (ii) och konstruktionen av $W_t$ ger
\[
\langle W_t\bold u,\bold v\rangle+
\langle\bold u,W_t\bold v\rangle=0
\]
Alltså är $W_t$ en \emph{antisymmetrisk} operstor på
$\bold V_K$ och ges därför via en kryssprodukt. Med andra ord finner vi en vektor
$\omega(t)\in \bold V_K$ så att (4) kan skrivas på formen
\[
\frac{d}{dt}(\bold p^*(t))=\frac{d}{dt}\bold q^*(t)+
S_t(\omega(t)\times(\bold p-\bold q))\tag{*}
\]
\medskip

\noindent
Formeln (*) användes av Euler för att härleda rörelse-ekvationer.
Ett exempel  är då $K$ roterar kring en fast punkt som då kan väljas 
som $\bold q$ varvid dess tidsderivatan hos $\bold q^*$ automatiskt är noll.
Så här gäller för varje punkt $\bold p\in K$:
\[
\frac{d}{dt}(\bold p^*(t))=
S_t(\omega(t)\times(\bold p-\bold q))\tag{**}
\]
Med hjälp av (*') kan
kroppens tidsberoende rotation
bestämmas via ett system av differentialekvationer
där man i ett första skede
kan koncentrera sig på att analysera
den vektorvärda funktionen $t\mapsto \omega(t)$..
Till att börja med kan nämligen (**) användas för att
uttrycka kroppens \emph{kinetiska energi}
via den Eulerska vinkelhastighen.
\medskip


\noindent
{\bf{Övning.}}
Utan egentlig inskränkning kan det antas att $K$ består av ett ändligt antal masspartiklar
$\bold p_1,\ldots,\bold p_N$
där varje enskild partikel har en positiv massa $m_\nu$. 
Vidare kan vi från start välje det ortonormerade  systemet i $\bold V_K$ så att den fasta punkten
$\bold q$ är origo.
Per definition är kroppens kinetiska energi given av
\[
\bold T= \frac{1}{2}\cdot \sum\, m_\nu\cdot ||\frac{d}{dt}(\bold p^*_\nu)||^2\tag{i}
\]
Nu utnyttjas att $S_t$ är ortogonal  villket innebär att vi lika gärna kan 
använda normen i det kroppsfixerade rummet. Detta ger likheten
\[
\bold T= \frac{1}{2}\cdot \sum\, m_\nu\cdot ||\omega(t)\times\,\bold p_\nu||^2\tag{ii}
\]
\medskip


\noindent
{\bf{En ytterligare förenkling.}}
Kalkylerna ovan gäller för varje ortonormerad bas i $\bold V_K$.
Euler
konstruerade nu en ON-bas där
efterföljande kalkyler kan förenklas genom att införa följande linjära operator på $\bold V_K$:
\medskip

\noindent
{\bf{Tröghetsoperatorn $\mathfrak{T}$}}. Den definieras
genom
\[ 
\mathfrak{T}(\bold u)= \sum\, m_\nu\cdot (\bold p_\nu\times \bold u)\times \bold  p_\nu
\]
\medskip

\noindent
{\bf{Övning.}}
Visa att $\mathfrak{T}$ är symmetrisk, dvs. att följande likhet gäller för varje par av vektorer i $\bold V_K$:

\[
\langle \mathfrak{T}(\bold u),\bold v\rangle=
\langle \bold u, \mathfrak{T}(\bold v)\rangle\tag{i}
\]
\medskip














































 














\newpage



\centerline {\bf{Grams ortogonalisering}}


\medskip


\noindent
I
det $n$-dimensionella rummet
${\bf{R}}^n$ representeras
varje punkt $p$  av en vektor 
$(x_1,\ldots x_n)$ vars spets
anger dess position, dvs. 

\[ 
p=x_1\bold e_1+\ldots+x_n\bold e_n
\]
I den euklidiska metriken är avståndet från $p$ rtill origo - eller alternativt
längden hos vektorn  $\mathbf p$  - lika med
\[
\mathbf p= \sqrt{x_1^2+\ldots+x_n^2}
\]


\noindent
{\bf{Inre produkter.}}
Låt $\bold q= (y_1,\ldots,y_n)$ var en annan vektor.
Till paret
$\bold p,\bold q$
bildas det reella talet
\[
\langle \bold p,\bold q\rangle=
x_1y_1+\ldots+x_ny_n
\]
som kallas för den \emph{inre produktern} mellan de två vektorerna.
Speciellt blir 
\[
\langle \bold p,\bold p\rangle= |\bold p|^2= x_1^2+\ldots+x_n^2
\]
\medskip


\noindent
{\bf{Övning}}.
Visa följande olikhet  för varje par av vektorer:
\[
|\langle \bold p,\bold q\rangle\,|\leq |\bold p|\cdot |\bold q|\implies
-1\leq \frac{\langle \bold p,\bold q\rangle}{\bold p,\bold q}\leq 1\tag{*}
\]
Denna olikhet ger en unik vinkel  $0\leq \phi\leq \pi$ så att
\[
\cos \phi=\frac{\langle \bold p.\bold q\rangle}{\bold p,\bold q}
\]
Speciellt är $\phi= \pi/2$ om och endast om
\[
\langle \bold p,\bold q\rangle=0
\]
När detta gäller säges de två vektorena vara ortogonala och man skriver 
\[
\bold p\perp \bold q
\]
\medskip

\noindent
{\bf{Ortogonalisering.}}
Låt $\bold p,\bold q$ vara ett par av vektorer.
Vi bildar   vektorn
\[ 
\bold q^*=\bold q-\frac{\langle \bold p,\bold q\rangle}{|\bold p|^2}\cdot \bold p
\]
där läsaren bör verifiera att
\[
\bold q^*\perp \bold p
\]
Vi kan alltså skriva  $\bold q$ som en summa
\[ 
\bold q=\bold q^*+\alpha\cdot \bold p
\]
där $\alpha$ är en reellt tal och
$\bold q^*$ är ortogonal till $\bold p$.
\medskip


\noindent
Antag nu att $\bold q^*\neq 0$ och
betrakta en tredje vekor
$\bold r$ samt konstruera
vektorn
\[ 
\bold r^*=\bold r-\frac{\langle \bold p,\bold r\rangle}{|\bold p|^2}\cdot \bold p-
\frac{\langle \bold q^*,\bold r\rangle}{|\bold q^*|^2}\cdot \bold q^*
\]
Läsaren kan verifiera att $\bold r^*$ är ortogonal mot både
$\bold p$ samt $\bold q^*$.
Notera också att $\bold r^*$ är nollvektorn om och endast om
$\bold r$ tillhör delrummet som spänns av de två vektorerna $\bold p$ och $\bold q$.
\medskip


\noindent
{\bf{Övning.}}
Låt $k\geq 2$ och $\bold v_1,\ldots,\bold v_k$
vara  \emph{linjärt oberoende} vektorer.
Visa att efter  upprepad konstruktion som ovan erhålles
en $k$-tupel av \emph{parvist ortogonala} vektorer
$\bold w_1,\ldots,\bold w_k$
där
\[ 
\bold w_p=a_{p1}\bold v_1+\ldots+a_{pp}\cdot \bold v_p\quad\, 1\leq p\leq k
\]
vilket ger  element i  en nedåt triangulär matris

\[ 
A=
\begin{pmatrix}
a_{11}&0&\ldots &0\\
a_{21}&a_{22}&\ldots &0\\
\ldots&\ldots&\ldots&\ldots\\
\ldots&\ldots&\ldots&\ldots\\
a_{k1}&a_{k2}&\ldots &a_{kk}\\
\end{pmatrix}
\]
Notera att då $p=1$ inleds konstruktionen genom att låta
$\bold w_1=\bold v_1$ så elementet
$a_{11}=1$, medan längderna  
hos övriga $\bold w$-vektorer i allmännhet är  $\neq 1$.
Vi inför  därför  vektorerna
\[
\bold u_p= \frac{1}{||\bold w_p||}\quad 1\leq p\leq k
\]
som ger en ny nedåt triangulär matris $B$ där
\[ 
\bold u_p=b_{p1}\bold v_1+\ldots+b_{pp}\cdot \bold v_p\quad\, 1\leq p\leq k
\]
Utskrivet på matrisform gäller alltså likheten
\[
\begin{pmatrix}
\quad \bold u_1\\
\quad \bold u_2\\
&\\
&\\
\quad \bold u_k\\
\end{pmatrix}=
\begin{pmatrix}
b_{11}&0&\ldots &0\\
b_{21}&b_{22}&\ldots &0\\
\ldots&\ldots&\ldots&\ldots\\
\ldots&\ldots&\ldots&\ldots\\
b_{k1}&a_{k2}&\ldots &b_{kk}\\
\end{pmatrix}
\begin{pmatrix}
\quad \bold v_1\\
\quad \bold v_2\\
&\\
&\\
\quad \bold v_k\\
\end{pmatrix}
\]

\medskip


\noindent
Vi kan uttrycka resultaten ovan på följande vis.
De linjärt oberoende $\bold v$-vektorerna generar 
ett $k$-dimensionellt delrum av
${\bf{R^n}}$ betecknat med $L(\bold v_1,\ldots,\bold v_k)$ 
som   består av vektorer
\[
a_1\bold v_1+\ldots+a_k\bold v_k\quad\,\colon\quad  a_1,\ldots,a_k\in {\bf{R}}
\]
Grams ortogonalisering
ger 
nu parvist ortogonala $\bold u$-vektorer som
spänner samma delrum och dessutom gäller likheter 
\[ 
L(\bold u_1,\ldots\bold u_j)=
L(\bold v_1,\ldots\bold v_j)\quad\, 1\leq j\leq k
\]
samtidigt som $\bold u$-vektorerna
ger ett \emph{ortonormerat} system.
\medskip


\noindent
{\bf{Övning.}}
Eftersom $\bold u$-vektorerna bildar  ett ortonormerat system
är deras Gram-matrtis
$E_k$.
Använd detta för att visa likheten
\[
det \, Gr(\bold V)=(b_{11}\cdots b_{kk})^{-2}\tag {*}
\]
där $\{b_{ii}\}$
är diagonalelementen i $B$-matrisen.
\medskip


\centerline{\bf{Fallet $k=n$.}}
\medskip


\noindent
När  $\bold v_1,\ldots\bold v_n$
är linjärt oberoende kan de identifieras med kolonnvektorer i en inverterbar matris
som  betecknas med $\bold V$.
Som ovan konstrueras en ortonormerad bas
$\bold u_1,\ldots,\bold u_n$
och med $k=n$ i (*) erhålles
den nedåt triangulära $(n,n)$-matrisen  $B$ och
konstruktionen av matrisprodukter  ger  likheten
\[
\bold U= B\cdot \bold V\tag{i}
\]
där  matrisen $B$ är  inverterbar och   dess determinant 
ges av produkten  $b_{11}\cdots b_{nn}\neq 0$.
Multipliceras (i) till vänster med $B^{-1}$
som är  uppåt triangulär
erhålles:
\medskip


\noindent
{\bf{Teorem.}} \emph{För  varje inverterbar matris $A$ existerar
en ortogonal matris $\bold U$ och en uppåt triangulär matris $S$ så att}
\[
A= S\cdot \bold U
\]


\newpage



\centerline{\bf{Polarisering.}}
\medskip


\noindent
Låt $A$ vara en inverterbar matris. Då gäller följande:

\medskip


\noindent
{\bf{Teorem.}}
\emph{Det existerar ett par av ortogonala matriser $U,V$ samt en diagonalmatris $D$ så att}
\[
A= VDU
\]
\medskip

\noindent
Beviset baseras på att studera 
bilden av enhetssfären
$S^{n-1}$ under den linjära avbildningen som definieras av $A$-matrisen.
Låt $(x_1,\ldots,x_n)$ vara de euklidiska koordinaterna
i ${\bf{R}}^n$ och  $\bold y=(y_1,\ldots,y_n)$
de euklidiska koordinater i bildrummet.  Vi har ekvationerna:
\[
y_p=\sum_{q=1}^{q=n}\, a_{pq}x_q\quad\, 1\leq p\leq n
\]
Låt $\{b_{pq}\}$ beteckna elementen hos den inversa matrisen
$A^{-1}$. Då erhålles
\[
x_p=\sum_{q=1}^{q=n}\, b_{pq}y_q\quad\, 1\leq p\leq n
\]
Vi har även ekvationen
\[ 
\sum_{p=1}^{p=n}\, (\sum_{q=1}^{q=n}\, b_{pq}y_q)^2=\sum\, x_p^2=1
\]
Vänsterledet ger en kvadratisk ekvation för $y$-variablerna, dvs.
 vi har ett homogent andragradspolynom $Q$ där 
bilden $A(S^{n-1})$ bestäms av ekvationen:
\[
Q(y_1,\ldots y_n)=1\quad\&\quad
 Q(y_1,\dots,y_n)= \sum\sum\, b_{pq}y_p\cdot y_q 
\]
I § XX visas att det existerar
en ortogonal matris $U$ som via variabelbytet
$U(\bold u)=\bold y$ ger
\[ 
Q(\bold u)= \sum_{i=1}^{i=n}\,
\frac{u_i^2}{d_i^2}\tag{*}
\]
där $d_1,\ldots d_n$ är positiva tal.
Här är  (*)  
ekvationen för en ellips och nu
införes  den diagonala matrisavbildningen
\[ 
\bold u_i\mapsto d_i\cdot \bold v_i
\]
som  representeras av en diagonalmatris $D$.
\medskip


\noindent {\bf{Slutsats.}}
Bilden $A(S^{n-1})$ uttryckt i $\bold v$-koordinater
svarar mot enhetssfären
definierad genom
\[ 
v_1^2+\ldots+v_n^2=1
\]
Alltså avbildas   enhetssfären $S^{n-1}$ på sig själv av den sammansatta linjära operatorn 
som  framställs av matrisprodukten
$DUA$.
Enligt resultat i § XX
betyder detta att $SDU$
är ortogonal och 
därmed är även dess inversa matris ortogonal. Låt oss beteckna den med $V$ som nu ger
\[ 
VDUA= E_n\implies
A=U^{-1}D^{-1}V^{-1}= U^*D^{-1}V^*
\tag{i}
\]
Här är $D^{-1}$  diagonal medan $U^*$ och $V^*$
båda är ortogonala vilket ger
den önskade produktframställningen av $A$-matrisen  i Teorem XX.



\newpage


\centerline{\bf{Symmetriska matriser}}

\medskip



\noindent
En $(n,n)$-matris $A$ är symmetrisk om
\[
a_{pq}= a_{qp}\quad\, 1\leq pq\leq n\tag{*}
\]
Vi skall nu återge ett bevis av Weierstrass
som visar att varje symmetrisk matris kan framställas
på diagonalform i en ortonormerad
bas.

\medskip

\noindent
{\bf{Teorem.}}
\emph{När (*) gäller existerar
en ortonormerad bas $\bold u_1,\ldots,\bold u_n$ 
samt en svit av
reella tal
$\mu_1\geq \mu_2\ldots\geq \mu_n$
så att}
\[ 
A(\bold u_i)= \mu_i\cdot \bold u_i\quad 1\leq i\leq n
\]
\medskip


\noindent
\emph{Bevis.}
Sätt
\[ 
M_1= \max_{||\bold x||=1}\, \langle A\bold x,\bold x\rangle\tag {i}
\]
På den kompakta enhetssfären
$S^{n-1}$ finner vi nu en vektor $\bold x^*$ där likhet gäller hos (i).
Vi skall nu visa att $\bold x^*$ är en \emph{egenvektor}, dvs. det finns
ett tal $\mu_1$ så att
\[ 
A\bold x^*=\mu_1\cdot \bold x^*\tag{ii}
\]
För att visa detta utnyttjas den euklidiska inre produkten. Eftersom den är
\emph{icke urartad} följer
existensen av ett egenvärde hos (ii) om vi lyckats visa följande implikation:
\[
\bold y\perp\bold x^*\implies
\bold y\perp \,A\bold x^*\tag{iii}
\]
För att visa (iii) betraktas ett reellt tal  $s$ och en vektor $\bold y\perp \bold x^*$
som dessutom är en enhetsvektor, dvs. $||\bold y||=1$.
Symmetrin hos $A$ ger likheten
\[
\langle A\bold x^*,\bold y\rangle
=\langle A\bold y,\bold x^*\rangle
\]
vilket medför att följande likhet gäller för varje reellt tal $s$:

\[
\langle A(\bold x^*+s\bold y ),\bold x^*+s\bold y\rangle=
M_1+2s\cdot \langle A\bold x^*,\bold y\rangle+s^2\cdot \langle A\bold y,\bold y\rangle\tag{iv}
\]
Eftersom enhetsvektorn  $\bold y$ är $\perp$ till
$\bold x^*$ gäller också:
\[
||\bold x^*+s\bold y^*||^2= 1+s^2\implies
\]
\[
\frac{1}{\sqrt{1+s^2}}\cdot
\langle A(\bold x^*+s\bold y ),\bold x^*+s\bold y\rangle\leq M_1\tag{v}
\]
Denna olikhet och (iv) ger 
\[
(\sqrt{1+s^2}-1)\cdot M_1\leq
2s\cdot \langle A\bold x^*,\bold y\rangle
+s^2\cdot \langle A\bold y,\bold y\rangle\tag{vi}
\]
Här gäller denna  olikhet \emph{för alla reella tal} $s$.
Men då måste
$\langle A\bold x^*,\bold y\rangle$. Ty om detta tal t.ex. är
$a>0$ ser läsaren att
(v) inte kan gälla om $s=-\epsilon$ och $\epsilon$ är ett tillräckligt  litet positivt tal.
\medskip


\noindent
Vi har nu funnit en egenvektor
som ger en extremal till
(i). Den betecknas  med $\mathbf u_1$ och
i nästa steg studerade Weierstrass det
restriktiva extremalprolblemet
där
\[ 
M_2=\max_{\mathbf x}\,\langle A\bold x,\mathbf x\rangle
\quad ||x||=1\quad\& \perp \mathbf  u_1
\]
På samma vis som ovan erhålles  en egenvektor $\bold u_2$
där
\[
\langle A\bold u_2,\bold u_2\rangle =M_2\quad\&\quad \bold u_2\perp \bold u_1
\]
Då  $n\geq 3$ kan processen upprepas
och efter $n$ steg erhålles
den önskade $n$-tuppeln  av parvist ortogonala egenvektorer
vilkas egenvärden bildar en avtagande följd av reella tal.
\medskip

\noindent
{\bf{En matrisformel.}}
I teroemet ovan har vi funnit egenvektorer som bildar en ortonormerad bas och alltså
ger kolonner i en ortgonal matris. Så vi har följande konsekvens:
\medskip

\noindent
{\bf{Teorem.}}
\emph{Om $A$ är en inverterbar och symmetrisk matris existerar en ortogonal matris $U$ samt en diagonal matris $S$ så att}
\[
A= U^*DU\tag{*}
\]
\medskip

\noindent
Läsaren bör notera att
omvändningen gäller, dvs. om en matris $A$ kan framställas enligt (*) så är
den symmetrisk.

\medskip



\centerline {\bf{Ett exempel.}}
\medskip


\noindent
Låt $n=2$ och betrakta matrisen
\[
A=
\begin{pmatrix}
1&b\\
b&1\\
\end{pmatrix}
\]
där $b>0$.
För en variabel $\lambda$ erhålles 
\[ 
\ det(\lambda\cdot E_2-A)=
(\lambda-1)^2-b^2\implies \lambda_1=1+b\quad\colon\, \lambda_2=1-b
\]
Till  roten $1+b$
sökes en egenvektor $(x,y)$ där
\[
x+by=(1+b)x\quad\&\quad\, bx+y=(1+b)y\implies x=y
\]
Den normerade egenvektorn
ges alltså av
\[ 
\bold u_1=\frac{1}{\sqrt{2}}\cdot (1,1)
\]
och vi erhåller 
\[ 
M_1= \langle A\bold u_1,\bold u_1\rangle=(1+b)
\]
Med roten $1-b$
bestäms en ny egenvektor $(x,y)$ av
\[
x+by=(1-b)x\quad\&\quad bx+y=(1-b)y\implies x=-y
\]
vars normerade egenvektor
blir
\[ 
\bold u_2=\frac{1}{\sqrt{2}}\cdot (1,-1)
\]
Den är $\perp \bold u_1$ och vi har
\[
M_2=1-b
\]
\medskip

Notera att kalkylerna ovan ger den
förväntade likheten
\[ 
1-b^2=\det A= M_1\cdot M_2
\]

\newpage

\centerline{\bf{Diagonalisering av symmetriska matriser.}}
\medskip



\noindent
Låt oss först studera familjen av \emph{kvadratiska former}
som består  av 2:a grads polynom
\[ 
Q(x_1,\ldots,x_n)= \sum_{p\leq q\leq n}
\, c_{pq}\cdot x^p\cdot x^q
\]
\medskip


\noindent
Här förekommer \emph{blandade termer} för par
$p\neq q$ där $c_{pq}\neq 0$.
Efter en lämplig variabelsubstitution kan
alla blandade  termer elimineras.
Antag t.ex. att $c_{11}\neq 0$ och notera att
vi kan skriva
\[ 
Q(\mathbf (x)=c_{11}\cdot (x_1+\frac{c_{12}}{2c_{11}}+\ldots +\frac{c_{1n}}{2c_{11}})^2
+Q_1(x_2,\ldots,x_n)
\]
där $Q_1$ endast beror av $x_2,\ldots,x_n$
Så substitutionen
 
\[ 
y_1=x_1+\frac{c_{12}}{2c_{11}}+\ldots +\frac{c_{1n}}{2c_{11}}\implies
\]
\[
Q(y_1,x_2,\ldots,x_n)=c_{11}y_1^2+Q_1(x_2,\ldots,x_n)
\]
\medskip


\noindent
Induktion över $n$ ger existens 
av en variabelsubstitution så att $Q$ i nya $\mathbf y$-variabler ges av 
\[ 
Q(\mathbf y)= d_1y_1^2+\ldots+d_ny_n^2\tag{*}
\]
\medskip

\noindent
Läsaren bör notera att (*) kan erhållas 
på många sätt, dvs.
mängden av diagonaliserande variabelsubstitioner
kan vara   omfattande.
För att
undersöka detta närmare införs
$Q$-polynomets associerade symmetriska matris
$A$ vars diagonala element ges av 
\[
a_{\nu\nu} =c_{\nu\nu}
\quad\, \& \quad
1\leq p<q\leq n \implies a_{pq}= a_{qp}= \frac{1}{2}\cdot c_{pq}
\]
\medskip


\noindent
{\bf{Övning.}}
Visa följande
likhet för alla $n$-vektorer $\mathbf x$:
\[ 
Q(\mathbf x)= \langle A\mathbf x,\mathbf x\rangle
\]
samt att klassen av diagonaliserande
variabelsubstitutioner
svarar mot klassen av symmetriska och inverterbara matriser $S$ sådana att
\[ 
S^*AS= D\tag{*}
\]
där högerledet är en diagonal matris som i allmännhet beror av $S$.
\medskip


\centerline{\bf{Tröghetsteoremet.}}
\medskip


\noindent
Låt $A$ vara en symmetrisk och inverterbar matris
där 
Teorem XX ger en unik svit
$mu_1\geq \ldots\geq \mu_n$ och en ortogonal matris $U$
så att
\[
U^*AU= D(\mu_1,\ldots\mu_n)
\]
I XX har vi funnit en klass $\mathcal D(A)$
av symmetriska operator för vilka
\[ 
S^*AS= D(s_1,\ldots,s_n)
\]
där talen $s_1,\ldots,s_n$
i högerledets diagonala matris i allmänhet beror av $S$-matrisen.
Men vi har följande partiella entydighet
som endast tar hänsyn till
hur många positiva-respektive negativa tal som
uppräder hos $\mu$- respektive $s$-följden.

\medskip

\noindent
{\bf{Teorem.}}
\emph{Antalet positiva $\mu$ respektive positiva $s$-tal är lika.}



\medskip

\noindent
\emph{Bevis.}
Weierstrass' bevis av Teorem XX ger  en ortonormerad
bas $\bold u_1,\ldots,\bold u_n$ i
${\bf{R}}^n$
där
\[ 
A(\bold u_i)= \mu_i\cdot \bold u_i
\]
och $\mu_1\geq \ldots \mu_n$.
Låt oss utan egentlig inskränkning anta att
$\mu_1,\ldots,mu_r$ alla är $>0$ medan
$\mu_{r+1}\,\ldots,\mu_n$ alla är $<0$.
Samtidigt gäller
\[
AS(\bold e_k)= d_k\cdot S(\bold e_k)\quad\, 1\leq k\leq n
\]
Bilda nu det linjära delrummet $\bold V_S(-)$
i  ${\bf{R}}^n$ som genereras av
vektorena
\[
S(\bold e_k)\quad\colon s_k<0
\]
vilket betyder att
\[ 
 \bold v\in \bold V_S(-)\implies
 \langle A\bold v,A\bold v\rangle<0\tag{i}
\]
Om
vi nu har en strikt olikhet
\[ 
\bold V_S(+)= < r
\]
så följer att
\[
V_+(U)\cap\, V_(S)\neq 0
\]
Men detta ger en motsägelse eftersom vi dels har (i) samtidigt som
 \[
 \bold v\in \bold V_+(U))\implies
 \langle A\bold v,A\bold v\rangle>0
 \]
 











































































































\newpage



\centerline{\bf{Singulära värden}}

\medskip

\noindent
Till varje  inverterbar $(n,n)$ matris $A$.
hör
en unik följd
av positiva tal
\[ 
\mu_1\geq\mu_2\geq \cdots 
\geq \mu_n>0 
\]
som erhålles genom
en svit av 
extremalproblem. Låt nämligen $S^{n-1}$ vara  enhetssfären i ${\bf{R}}^n$
och sätt
\[
M_1= \max_{\mathbf x\in S^{n-1}}\, \langle A\mathbf x,A\mathbf x\rangle\tag{1}
\]
Eftersom $S^{n-1}$ är kompakt finns
en enhetsvektor  $\mathbf u_1^*$ där 
maxium antas, dvs. 
\[
M_1=  \langle A\bold u_1^*,A\bold u_1^*\rangle\tag{2}
\]
I nästa steg
betraktas extremalproblemet där vi endast
använder
enhetsvektorer som är ortogonala  mot $\bold u_1^*$.
\[ 
M_2=
\max_{\bold x\perp u_1^*}\, \langle \bold x,A\bold x\rangle\tag{2}
\]
Vi får en ny enhetsvektor  $\mathbf u ^*_2\perp\mathbf u_1^*$ där likhet gäller i (2).
Dessa  alltmer   restriktiva
extremalproblem  upprepas i $n$ steg
och ger
en ortogonal familj av enhetsvektorer
$\mathbf u_1^*,\ldots\,\mathbf u_n^*$
samt en följd positiva tal $M_1\geq \ldots _\geq M_n>0$
där
\[
M_k= \langle A\mathbf u^*_k,A\mathbf u ^*_k\rangle\quad 1\leq k\leq n\tag{3}
\]
\medskip

\noindent
{\bf{Definition.}}
\emph{Matrisens singulära värden består av talen}
\[
\mu_k=\sqrt{M_k}\quad 1\leq k\leq n
\]
\medskip


\noindent
Vi skall nu
bestämma $\mu$-talen 
på ett - skenbart - annorlunda vis.
Betrakta  den transponerade matrisen  $A^*$ 
som ger följande likhet för varje vektor $\bold x$:
\[
\langle A\bold x,A\bold x\rangle = \langle A^*A\bold x,\bold x\rangle
\]
Sätt
\[
B=A^*A
\]
och notera att denna matris
är symmetrisk. Till  varje vektor 
$\bold x=(x_1,\ldots,x_n)$
sätter vi
\[ 
 \langle A\bold x,A\bold x\rangle =Q(x_1,\ldots,x_n)=\langle B\bold x,\bold x\rangle
= \sum\sum\, b_{pq}x_qx_p
\]
där den kvadratiska formen $Q$ är positiv eftersom  $A$-matrisen är inverterbar.
Mer precist gäller:
\[
\bold x\neq 0\implies
\langle A\bold x,A\bold x\rangle >0
\] 
\medskip

\noindent
Låt oss nu kasta om $A$ och $A^*$  och sätt
\[
M^*_1= \max_{\bold x\in S^{n-1}}\, \langle A^*\bold x,A^*\bold \rangle
\]
Vi får då en  enhetsvektor $\xi_1^*$ där
\[
M^*_1= \langle A^*\bold \xi_1^*,A^*\bold \xi_1^*\rangle
\]
som ger  $A^*$-matrisens största singulära värde
\[
\mu_1^*=\sqrt{M^*_1}
\]
och på samma vis som för matrisen $A$ erhålles $A^*$-matrisens 
singulära värden
\[ 
\mu_1^*\geq \mu_2^*\geq\ldots\geq \mu_n^*>0
\]
Vi har  också den symmetriska matrisen
\[ 
C=AA^*
\]
och noterar att man  har likheten
\[
\langle A^*\bold x,A^*\bold x\rangle =\langle C\bold x,\bold x\rangle
= \sum\sum\, c_{pq}x_qx_p
\]
\medskip

\noindent
{\bf{Varning !}}
I allmännhet kommuterar inte matriserna $A$ och $A^*$, dvs.
det kan inträffa att
\[ 
B\neq C
\]
Men även i fallet när dessa  två symmetriska matriser är olika
så gäller följande:
\medskip


\noindent
{\bf{Teorem}}.
\emph{Matriserna $A$ och $A^*$ har samma singulära värden.}
\medskip


\noindent
\emph{Bevis}
Betrakta   den symmetriska matrisen
$A^*A$.
Teorem xx i § xx ger 
en ortonormerad  bas 
$\bold u_1,\ldots,\bold u_n$
så att
\[ 
A^*A(\bold u_i)=\mu_i^2\cdot \bold u_i\tag{i}
\]
Applicera  $A$ till vänster som tack vare 
den associativa lagen för matrisprodukter ger
likheten
\[
(A\cdot A^*)(A(\bold u_1))= \mu_i^2\cdot A(\bold u_i)
\]
Alltså är $A\bold u_i$   egenvektor till
den symmetriska matrisen
$AA^*$.
Eftersom detta gäller för varje $i\leq i\leq n$ följer 
likheterna $\mu_i=\mu_i^*$
från   Teorem XX.
\medskip


\noindent
{\bf{En extra poäng.}}
Låt oss anta att
de singulära värdena är olika, dvs.
\[
\mu_1>\mu_2>\ldots>\mu_n>0\tag{*}
\]
Enligt Teorem xxx är
egenvektorerna till $AA^*$ 
med olika egenvärden
ortogonala. Så 
från beviset ovan kan vi dra följande slutsats:
\medskip


\noindent
{\bf{Teorem}}
\emph{När (*) gäller så överför den 
linjära avbildningen definierad av $A$
den ortonormerade
basen $\{\bold u_i\}$
till
en familj av ortogonala vektorer}
\[ 
\bold v_i= A\bold u_i
\]
\emph{där längderna}
\[
||\bold v_i||= \mu_i
\]




\newpage


\centerline {\bf{Grassmannrummen $\Lambda^k({\bf{R}}^n)$.}}
\medskip



\noindent
Vi skall betrakta
\emph{multilinjära} avbildningar från ${\bf{R}}^n$ till
de reella talen.
\medskip


\noindent
{\bf{Definition.}}
\emph{Låt $1\leq k\leq n$. En  multilinjär 
$k$-form $B_k$
avbildar varje ordnad $k$-tupel av
vektorer $\bold u_1,\ldots,\bold u_k)$
till
ett reellt tal}
\[ 
B_k(\bold u_1,\ldots,\bold u_k)
\]
som dessutom är
linjär i varje enskild vektor.
Vi har t.ex. att
\[
B_k(\bold u_1+\bold v_1,\ldots,\bold u_k)=
B_k(\bold u_1,\ldots,\bold u_k)+
B_k(\bold v_1,\ldots,\bold u_k)
\]
\emph{för varje par $\bold u_1,\bold v_1$ medan
de resterande vektorerna
är fixa.
På samma sätt är $B_k$ additiv när vektorer på plats $2\leq i\leq k$ adderas
med övriga hålles fixerade.
Vidare gäller att om $\alpha$ är ett reellt tal så blir}
\[
B_k(\alpha\cdot \bold u_1,\ldots,\bold u_k)=
\alpha\cdot B_k( u_1,\ldots,\bold u_k)
\]
\emph{och samma gäller om
vi multiplicerar
en vektor $\bold u_i$ med en skalär
$\alpha$.}
 
\bigskip


\noindent
Räknelagarna medför  att
en $k$-linjär form
$B_k$  bestäms av sina värden
på de
$n^k$ många
$k$-tuplerna
\[
B_k(\bold e_{i_1},\ldots,\bold e_{i_k})
\]
där varje $i_\nu$ är något av talen $1,\ldots,k$.
Så om $\mathcal B_k$ betecknar rummet av alla $k$-linjära avbildningar
ger detta ett linjär rum av dimension $n^k$.

\medskip

\noindent
{\bf{Definition.}}
En $k$-linjär form
$B_k$ är \emph{alternerande} om

\[
B_k(\bold u_{i_1},\ldots,\bold u_{i_k})=sign(i_1,\ldots,i_k)\cdot 
B_k(\bold u_1,\ldots,\bold u_k)
\] 
gäller för alla $k$-tupler av vektorer.
Dessa bildar ett delrum av $\mathcal B_k$ som betecknas med
$\mathcal {A}lt_k$.

\medskip


\noindent
{\bf{Övning.}}
Visa att dimensionen hos vektorrummet
$\mathcal{A}lt_k$ är lika med $\binom{n}{k}$
samt att det 1-dimensionella rummet
$\mathcal{A}lt_n$
erhålles via determinantens  konstruktion.
Mer precist gäller:

\medskip

\noindent
{\bf{Teorem XX}}
\emph{Till varje alternerande $n$-form  $B_n$ hör ett unikt reellt tal så att}
\[ 
B_n(\bold u_1,\ldots,\bold u_n)= \alpha\cdot det(A(\bold u_\bullet))
\]
där högerledet räknar ut determinanten hos $(n,n)$-matrisen
$A(\bold u_\bullet)$ vars kolonner ges av den ordnade
$n$-tupeln  av $\bold u$-vektorer.
\bigskip


\noindent
Med stöd av Teorem XX skall vi
nu
konstruera alternerande former
med hjälp av en  ordnad familj av vektorer.
Betrakta $k$ linjärt oberoende vektorer
$\bold v_1,\ldots,\bold v_k$.
Dessa
ger en $(n-k)$-form där varje 
ordnad $(n-k)$-tupel
$\bold u_1,\ldots,\bold u_{n-k}$
avbildas till
det reella talet
\[
det A(\bold v,\bold u)\tag{*}
\]
där $A(\bold v,\bold u)$ betecknar 
den kvadratiska matrisen
vars $k$ första kolonner
är $\bold v_1,\ldots,\bold v_k$
och $\bold u_1,\ldots \bold u_{n-k}$ resterande kolonnvektorer.


\medskip

\noindent
{\bf{Det linjära rummet $\Lambda^k({\bf{R}}^n)$.}}
Låt $\bold e_1,\ldots,\bold e_n$ vara de euklidiska basvektorerna.
Vi bildar  ett vektorrum 
där ordnade $k$-tupler av $\bold e$-vektorena utgör  en bas. Mer precist är
\[
\Lambda^k({\bf{R}}^n)=
\bigoplus\, {\bf{R}}\cdot \bold e_{i_1}\wedge\ldots\wedge \ldots \bold e_{i_k}
\]
där summan tas över alla $k$-tupler
$1\leq i_1<\ldots<i_k\leq n$.
Om $\sigma(\bullet)$ är en permutation
av en ordnad $k$-tupel i summan ovan  gäller likheten
\[
\bold e_{\sigma(i_1)}\, \wedge\ldots\wedge \, \bold e_{\sigma(i_k)}=
sign(\sigma)\cdot \bold e_{i_1}\wedge\ldots\wedge  \bold e_{i_k}\tag{*}
\]
När $k=2$ gäller
t.ex. att
\[ 
\bold e_\nu\wedge \bold e_i=
- \bold e_i\wedge \bold e_\nu\quad\, \nu\neq i
\]
Däremot gäller  för $k=3$ likheten
\[
\bold e_3\wedge \bold e_1\wedge \bold e_2=
\bold e_1\wedge \bold e_2\wedge \bold e_3
\]

\newpage

\centerline {\bf{Vektorprodukten
$ \bold v_1\wedge\ldots\wedge  \bold v_k$.}}


\medskip


\noindent
Till varje  $k$-tupel av vektorer $\bold v_1,\ldots,\bold v_k$
bildas en vektor i
$\Lambda^k$ genom
att skriva ut varje enskild $\bold v$-vektor i
den euklidiska basen och därefter använda likheterna (*).
Låt oss som ett exempel ta $k=2$ och betrakta ett vektorpar
\[
\bold v=v_1\bold e_1+\ldots+v_n\bold e_n
\bold u=u_1\bold e_1+\ldots+u_n\bold e_n
\]
Då blir
\[ 
\bold v\wedge\bold u
\sum_{1\leq i<k\leq n}\,
(v_i\cdot u_k-v_k\cdot u_i)\cdot \bold e_i\wedge \bold e_k
\]

\medskip

\noindent
{\bf{En Determinantformel. }}
Till en $k$-tupel $\bold v_1,\ldots,\bold v_k$
bildas en matris med $k$ många kolonner och $n$ rader:

\[
A(\bold v_1,\ldots,\bold v_k)= \begin{pmatrix}
v_{11}&v_{12}&\ldots  &v_{1k}\\
v_{21}&v_{22}&\ldots &v_{2k}\\
\ldots&\ldots&\ldots&\ldots\\
\ldots&\ldots&\ldots&\ldots\\
v_{n1}&v_{n2}&\ldots &v_{nk}\\
\end{pmatrix}
\]
\medskip

\noindent
Genom att ta ut $k$  rader i matrisen erhålles en kvadratisk $(k,k)$-matris
vars determinant  kan beräknas.
Så till varje tupel $1\leq i_1<\ldots<i_k$
erhålles talet

\[
M_{\bold v_\bullet}(i_1,\ldots,i_k)= \det
\begin{pmatrix}
v_{i_1,1}&v_{i_1,2}&\ldots  &v_{i_1,k}\\
v_{i_2,1}&v_{i_2,2}&\ldots &v_{i_2,k}\\
\ldots&\ldots&\ldots&\ldots\\
\ldots&\ldots&\ldots&\ldots\\
v_{i_k,1}&v_{i_k,2}&\ldots &v_{i_k,k}\\
\end{pmatrix}
\]

\medskip

\noindent
{\bf{Övning.}}
Visa  likheten

\[
\bold v_1\wedge\ldots\,\bold v_k=
\sum\, 
M_{\bold v_\bullet}(i_1,\ldots,i_k)\cdot \bold e_{i_1}\wedge\ldots\wedge \,\bold e_{i_k}\tag{*}
\]

\bigskip


\centerline {\bf{Normen på $\Lambda^k$.}}


\medskip

\noindent
Till varje  vektor 
\[
\sum\, c_{i_1\ldots,i_k}\cdot \bold e_{i_1}\wedge\ldots\wedge  \bold e_{i_k}
\]
definieras dess norm 
\[
\sqrt{\, \sum\, |c_{i_1,\ldots, i_k}|^2}
\]
Från (*)  följer att den kvadratiska normen  hos 
en $k$-tupel av vektorer  blir
\[
||\bold v_1\wedge\ldots\,\bold v_k||^2=
\sum\, 
(M_{\bold v_\bullet}(i_1,\ldots,i_k)^2\tag{**}
\]
\medskip


\noindent
{\bf{Teorem.}}
\emph{För varje $k$-tuppel $\bold V= (\bold v_1,\ldots,\bold v_k)$
gäller likheten}:

\[
det \, Gr(\bold V)= ||\bold v_1\wedge\ldots\,\bold v_k||
\]




\medskip


\noindent
{\bf{Övning.}}
Visa detta teorem med stöd av tidigare resultat.
Notera också att
teoremet ovan tillsammans med (*) ger likheten
\[
det \, Gr(\bold V)^2=
\sum\, 
(M_{\bold v_\bullet}(i_1,\ldots,i_k)^2\tag{***}
\]























































\newpage

\centerline{\bf{Volymformler}}

\medskip


\noindent
Vektorer i
${\bf{R}}^n$ betecknas  med $\bold v$ eller $\bold u$
där  vi pånminner om att vektorernas spetsar  svarar mot en punkter
${\bf{R}}^n$.
Låt  $\bold v,\bold u$ vara ett vektorpar som inte är
parallela, dvs.
cosinsvinkeln $\phi$  mellan dem
antar 
ett värde $0<\phi<\pi$.
De två vektorerna  spänner ett 2-dimensionellt simplex
\[ 
\Sigma(\bold v,\bold u)=\{s\bold v+t\bold u\colon\, 0\leq s,t\leq 1\}\tag{*}
\]
I analogi med känd formel från den euklidiska geometrin då man beräknar arean hos en parallelogram
definieras 
\[ 
\text{Area}(\Sigma(\mathbf v,\mathbf u))
=\sqrt{||\mathbf  v||^2\cdot ||\mathbf u||^2)(1-\cos^2\phi)}
\]
Med  beteckningen
\[
\text{Ar}(\bold v,\bold u)
=\text{Area}(\Sigma(\bold v,\bold u)
\] 
följer efter kvadrering  att
\[
\text{Ar}(\bold v,\bold u)^2=||\bold v||^2 \cdot ||\bold u||^ 2-\langle \bold v,\bold u\rangle^2 
\]
där högerledet
är  lika med
determinanten hos Grams (2,2)-matris:
\[
\begin{pmatrix}
\langle \bold v,\bold v\rangle,&\langle \bold v,\bold u\rangle\\
\langle \bold v,\bold u\rangle&\langle\bold u,\bold u\rangle\\
\end{pmatrix}
\]

\medskip


\noindent
\centerline{\bf{Pythagoras  Teorem}}
\medskip


\noindent
Vi kan  projicera simplexet i (*)
på
2-dimensionella delrum av
$\bold R^n$ som
genereras av
ett par av  euklidiska basvektorer.
Om exempelvis $n=3$ finns 3 sådana projektioner
\[
\pi_{12}(x_1,x_2,x_3)= (x_1,x_2)
\]
\[
\pi_{13}(x_1,x_2,x_3)= (x_1,x_3)
\]
\[
\pi_{23}(x_1,x_2,x_3)= (x_2,x_3)
\]
Med $\Sigma=\Sigma(\bold u,\bold v)$
erhålles tre plana områden:
\[ 
S_{12}= \pi_{12}(\Sigma)
\quad
S_{13}= \pi_{13}(\Sigma)
\quad
S_{23}= \pi_{23}(\Sigma)
\]
\medskip


\noindent
{\bf{Teorem.}}
\emph{Man har likheten}
\[
Ar(\Sigma)^2=Ar(S_{12})^2+
Ar(S_{13})^2+Ar(S_{23})^2\tag{*}
\]
\medskip


\noindent
\emph{Bevis.}
Skriv ut vektorerna
\[ 
\bold v=v_1\bold e_1+v_2\bold e_2+v_3\bold e_3
\]
\[ 
\bold u=u_1\bold e_1+u_2\bold e_2+u_3\bold e_3
\]
Areaformeln
i det 2-dimensionella fallet ger
\[
Ar(S_{12}^2= (v_1^2+v_2^2)(u_1^2+u_2)^2-(v_1u_1+v_2u_2)^2
\]
\[
Ar(S_{13}^2= (v_1^2+v_3^2)(u_1^2+u_3)^2-(v_1u_1+v_3u_3)^2
\]
\[
Ar(S_{23})^2= (v_2^2+v_3^2)(u_2^2+u_3^2-(v_2u_2+v_3u_3)^2
\]
Samtidigt gäller att
\[
Ar(\Sigma)= (v_1^2+v_2^+v_3)^2)(u_1^2+u_2^2+u_3)^2-
(v_1u_1+v_2u_2+v_3u_3)^2
\]
Läsaren kan nu vidimera (*) med en direkt algebraisk kalkyl.

\medskip

\noindent
{\bf{Fallet $n\geq 4$}}.
Även här gäller   Pythagoras teorem. Men nu blir
direkta kalkyler mer involverade.
När $n=4$ förekommer t.ex. 6 projektioner
\[ 
\pi_{i\nu}(\bold x)= (x_i,x_\nu)
\quad 1\leq i<\nu\leq 4
\]
där man nu har likheten
\[
Area(\Sigma(\bold u,\bold v))^2=
S_{12}^2+S_{13}^2+S_{14}^2+S_{23}^2+S_{24}^2+S_{34}^2
\]
För att härleda Pythagoras sats då $n=4$, och mer allmännt för  $n\geq 5$
utnyttjas
resultat från § XX.
Låt oss  beskriva hur Pythagoras sats kan uttryckas
med hjälp av determinanter.
Med $n\geq 4$
betraktas ett 
vektorpar $\bold u,\bold v$ i ${\bf{\bold R^n}}$.
Den 2-dimensionella aren som spänns upp
av $\Sigma(\bold u,\bold v)$
bestäms av  matrisen 
\[
A(\bold u,\bold v)=
\begin{pmatrix}
u_1&v_1\\
u_2&v_2&\\
\ldots&\ldots\\
\ldots&\ldots\\
u_n&v_n\\
\end{pmatrix}
\]


\medskip


\noindent
Vi  bildar $\binom{n}{2}$ många (2,2)-matriser
genom att behålla två rader i matrisen varefter vi
räknar ut deras determinanter. När $1\leq i<k\leq n$
behållles rad $i$ och rad $k$ och sätt
\[ 
M(i,k)=\det \begin{pmatrix}
u_i&v_i\\
u_k&v_k&\\
\end{pmatrix}
\]
Med dessa beteckningar gäller
\medskip


\noindent
{\bf{Teorem.}}
\emph{Man har likheten}
\[
Area(\Sigma(\bold u,\bold v))^2=\sum_{1\leq i<k\leq n}\, M(i,k)^2
\]
\medskip


\noindent
Teoremet  följer från
Övning XX i förra avsnittet
och likheten
\[
Area(\Sigma(\bold u,\bold v))^2=det(Gr(\bold u,\bold v)^2
\]

\medskip

\noindent
{\bf{En specifik volymformel.}}
Låt $\bold v_1,\ldots,\bold v_k$ vara linjärt oberoende vektorer
i $\bold R^n$ där $2\leq k\leq n-1$.
Vi bildar deras $k$-simplex och dess
$k$-dimensionella volym blir enligt beteckningar och resultat från XXX

\[ 
\sqrt{Gr(\bold V)}= ||\bold v_1\wedge\ldots\wedge \bold v_k||
\]
Betrakta nu projektionen
\[ 
\pi(x_1,\dots,x_n)=(x_1,\dots,x_k)
\]
och antag att
dess restriktion till
$\Sigma(\bold V)$
är
injektiv. Notera att ett ekvivalent villkor är att
\[
M_{\bold v_\bullet}(1,\ldots,k)=
\det\begin{pmatrix}
v_{1,1}&v_{1,2}&\ldots  &v_{1,k}\\
v_{2,1}&v_{2,2}&\ldots &v_{2,k}\\
\ldots&\ldots&\ldots&\ldots\\
\ldots&\ldots&\ldots&\ldots\\
v_{k,1}&v_{k,2}&\ldots &v_{k,k}\\
\end{pmatrix}\neq 0
\]
\medskip


\noindent
I det $k$-dimensionella rummet med koordinater
$x_1,\ldots,x_k)$
erhålles  nu $\pi(\Sigma(\bold V))$
och enligt XXX gäller
\[
vol_k(\pi(\Sigma(\bold V))= \sqrt{M_{\bold v_\bullet}(1,\ldots,k)}\tag{*}
\]
\medskip

\noindent
Vi skall nu ge en formel
som anger
förhållandet mellan denna
volym och $vol_k(\Sigma(\bold V))$.
För varje  $1\leq j\leq k $ sätter vi
\[
||\bold v_j||_*=\sqrt{\sum_{\nu=k+1}^{k=n}\, v_{\nu,j}^2}
\]
\[ 
||\bold V||_*=\sqrt{\,  \prod_{j=1}^{n-k}\, (1+||\bold v_j||^2)}
\]

\medskip

\noindent
{\bf{Teorem.}} \emph{Med beteckningar som ovan
gäller
likheten}

\[
vol_k(\Sigma(\bold V))= \sqrt{M_{\bold v_\bullet}(1,\ldots,k)}\tag{*}
\cdot ||\bold V_*||
\]






















\newpage



Spara men ej i texten !!
\medskip



\centerline{\bf{Bilinjära former}}
\medskip


\noindent
En bilinjär form på
${\bf{R}}^n$
ges av  en reellvärd funktion $B$ som till
vajre par av vektorer $\bold v,\bold u$
tilldelar ett reellt tal $B(\bold u,\bold u)$ 
som uppfyller
additonslagen:
\[
B(\bold v_1+\bold v_2,\bold u)=
B(\bold v_1,\bold u)+
B(\bold v_2,\bold u)=
\,\&\,
B(\bold v,\bold u_1+\bold u_2)=
B(\bold v,\bold u_1)+B(\bold v.\bold u_2)
\]
Vidare gäller för varje reelt tal $\alpha$ att
\[
\alpha\cdot B(\bold v,\bold u)=
B(\alpha\bold v,\bold u)+
B(\bold v,\alpha\bold u)
\]
\medskip


\noindent
Notera 
att bilineariteten medför att $B$  bestäms av sina värden
\[
\beta_{ik}= B(\bold e_i,\bold e_k)\quad\colon 1\leq i,k\leq n
\]
Dessa $n^2$ många $\beta$-tal ger element
i en 
$(n,n)$ matris som netecknas med 
$\mathcal B$.
Notera att vi på detta viset har en 1-1
korrespondens mellan
mängden av bilinjära form och
$(n,n)$-matriser.
\medskip


\noindent
{\bf{Alternerade resp. symmetriska föromer.}}
Låt $B$ vara en bilinjär form. Vi erhåller en ny bilinjär form
\[
Alt(B)(\bold v,\bold u)= \frac{1}{2}[
B(\bold v,\bold u)-
B(\bold u,\bold v)]
\]
samt en symmetrisk bilinjär form
\[
Sym(B)(\bold v,\bold u)= \frac{1}{2}[
B(\bold v,\bold u)+
B(\bold u,\bold v)]
\]
Notera  likheten
\[ 
B= Alt(B)+Sym(B)
\]
Matrisen som hör till
$Alt(B)$ är antiymmetrisk, dvs.
för dess element $\{c_{ik}\}$ gäller
att
\[ 
c_{ik}=-c_{ki}
\]
och speciellt är matrisens diagonala element alla noll.
Låt osss införa följande:
\medskip


\noindent
{\bf{Definition}}.
\emph(En $(n,n)$-matris $S$ kallas symplektisk om
den är anti-symmetrisk,dvs, när
\[
s_{ik}= -s_{ki}\quad\, 1\leq i<k\leq n
\]

\medskip


\noindent
Låt oss nu betrakta ett par av vektorer $\bold v,\bold u$
som antas vara linjärt oberoende.
Till detta vektorpar bildas
den anti-symmetriska matrisen med element
\[
s_{ik}= v_i\cdot u_k-v_k\quad\colon 1\leq i<k\leq n
\]
varefter vi låter
\[ 
s_{ki}= -s_{ik}\quad\colon 1\leq i<k\leq n
\]
Denna matris betecknas med
\[
S(\bold v\wedge \bold u)
\]
där vi noterar att dess konstruktion beror på i vilken ordningsföljden
hos de två 
vektorerna, dvs. det gäller att
\[
S(\bold u\wedge \bold v)=-
S(\bold v\wedge \bold u)
\]
\medskip

\centerline{\bf{Vektorrummet $\Lambda^2(\bf{R}^n)$}}.
\medskip


\noindent
Mängden av alla alternerade bilinära former
bildar ett vektorrum över ${\bf{R}}$ vars dimension är
$\frac{n(n-1)}{2}=\binom{n}{2}$ där en bas 
ges av de speciella formerna $B_{ik}$ där man har 
\[ 
B_{ik}(\bold e_i,e_k)=1\,\quad\,
B_{ik}(\bold e_j,e_\nu)=0\,\quad\colon
\,(j,\nu)\neq (i,k)
\]
Den tillhörande vektorn i
$\Lambda^2({\bf{R}})$ betecknas med $\bold e_i\wedge \bold e_k$. Vi har alltså
\[
\Lambda^2({\bf{R}})=
\bigoplus_{1\leq i<k\leq n}
\, \bf{R}\cdot \bold e_i\wedge\bold e_k
\]
Vi ser nu att
Utgående från dessa basvektorer
ger konstruktionen
av $S(\bold v\wedge\bold u)$ likheten
\[
 \bold v\wedge\bold u=
\sum_{1\leq i<k\leq n}\,
(v_iu_k-v_ku_i)\bold e_i\wedge \bold e_k
\]
\medskip


\noindent
{\bf{Definition.}}\emph
{Vektorn $\bold v\wedge \bold u\in \Lambda^2({\bf{R}})$
kallas  deras
yttre produkt}.
\medskip

\noindent
För en allmänn vektor
\[ 
\xi= \sum_{1\leq i<k\leq n}\, \xi_{ik}\cdot \bold e_i\wedge \bold e_k
\]
definieras dess euklidiska längd:
\[
||\xi||= \sqrt{\sum\, \xi_{ik}^2}
\]
och den  inre produkten mellan ett vektorpar $\xi,\eta\in \Lambda^2{\bf{R}}$ definieras genom
\[
\langle \xi,\eta\rangle=
 \sum_{1\leq i<k\leq n}\, \xi_{ik}\eta_{ik} 
\]
\medskip


\noindent
{\bf{Övning.}}
Konfirmera att  med stöd av beteckningar ovan att
när $\bold v,\bold u$ är ett par av vektorer i ${\bf{R}}^n$
så gäller
liheten
\[
||\bold v\wedge \bold u||^2=
\sum_{1\leq i<k\leq n}\,
Area(\pi_{ik}(\Sigma(\bold v,\bold u))^2
\]
{\bf{Slutsats}}.
Pythagoras teorem för vektorpar i
${\bf{R}}^n $
svarar mot likheten
\[
||\bold v\wedge \bold u||^2=
G(\bold v,\bold u)\tag{*}
\]


\newpage



\centerline{\bf{Komplexa matriser}}

\medskip

\noindent
Till varje $n\geq 2$ kan vi införa $(n,n)$-matris $A$  vars element $\{a_{pq}\}$
är komplexa tal.
Produkter av sådana matriser 
samt deras determinanter
definieras på samma sätt som för matriser med reella tal som element.
Till en komplex $(n,n)$-matris $A$ hör dess \emph{karakteristiska polynom}
\[
\det(\lambda\cdot E_n-A)= \lambda^n+q_{n-1}\lambda^{n-1}+\ldots+q_0
\]
där $\lambda$ är en komplex variabel och
högerledet är ett polynom av grad $n$ vars koefficienter nu är komplexa tal.
\emph{Algebrans fundamentalsats}
medför att (*) har rötter som i allmännhet utgöres av komplex tal.
Detta leder till existens av \emph{egenvektorer} när $A$ identifieras med
en ${\bf{C}}$-linjär avbildning från the
$n$-dimensionmalla komplexa vektorrummet
${\bf{C}}^n$ in i sig själv.

\medskip


\noindent
{\bf{Exempel.}}
Merd $n=2$ betraktas matrisen
\[ 
A= cos etc
\]
Dess tillhörande linjära oiperastor på
${\bf{R}}^2$ saknar egenvektorer eftersom
medan den vrider plana reella vektorer med vinkeln $\phi$.
Men om vi låter $A$ verka som en ${\bf{C}}$-linjär operator på
\[
{\bf{C}}^2= {\bf{C}}\cdot \bold e_1+{\bf{C}}\cdot \bold e_2
\]
så erhålles två egenvektorer.
För att bestämma dessa
noteras första att
\[ 
\Delta_A(\lambda)= (\lambda-\cos\,\phi)^2-\sin\,\phi^2=
\lambda^2-2\cdot \lambda\cdot \cos\,\phi+1\implies (\lambda-\cos\,\phi)^2=1-\cos^\phi= -\sin^2\phi
\]
Vi erhåller nu två rötter
\[
\lambda_1=\cos\,\phi+i\cdot \sin\phi= e^{i\phi}\quad\&\quad
\lambda_2=\cos\,\phi-i\cdot \sin\phi= e^{-i\phi}
\]
Till roten $\lambda-1$ hör nu en egenvektor där man alltså skall söka en komplex vektor
\[
\bold z= z_1\bold e_1+z_2\bold e_2
\]
så att
\[
\cos\phi\cdot z_1-\sin\phi\cdot z_2=e^{i\phi}\quad\,\&\quad
-\sin\phi\cdot z_1+\cos\phi\cdot z_2=e^{i\phi}\implies
\]
Elimineras $z_2$ kan läsaren vidimera att
\[
\cos^2\phi\cdot z_1-\sin^2\phi\cdot z_1=2e^{i\phi}
\implies \cos\,2\phi\cdot z_1= e^{i\phi}
\]

\medskip


\noindent
{\bf{Den hermiteska inre produkten.}}
Den euklidiska inre produkten på ${\bf{R}}^n$
ersättes på det komplex rummet av
\[ 
\langle z,w\rangle=
z_1\bar w_1+\ldots+z_n\bar w_n\tag{*}
\]
där vi alltså tagit de komplexa konjugaterna av
den komplexa $n$-vektorn $\bold w$.
Notera att (*) svarar mot den euklidiska inre produkten
i specialfallet då
$\bold z$ och $\bold w$ båda är reella vektorer.
Längden hos en komplex vektor definieras genom 
\[
||\bold z|| =\sqrt{\langle z,z\rangle}= \sqrt{\sum\, |z_\nu|^2}
\]
\medskip


\noindent
{\bf{Hermiteska matriser.}}
Till varje $(n,n)$-matris
$A$ bildas matrisen $A^*$
vars element ges av:
\[ 
a^*_{pq}= \bar a_{qp}
\]
Man tar alltså den komplexa konjugaten hos $A$-elementoch flyttar sedan
deras postion på samma vis som
vid konstruktion av transponerade matriser i det reella fallet.
\medskip


\noindent
{bf{Övning.}} Visa likheten nedan för alla komplexa $n$-verktorer $\bold u,\bold v$.
\[ 
\langle A\bold u,\bold v\rangle= \langle \bold u,A^*\bold v\rangle
\]
samt att
\[ 
A^*B^*= (BA)^*
\] 
gäller för alla par av komplexa matriser.
Notera också att om
$C$ är en komplex mattris så kan vi skriva ut dess element
\[
c_{pq}= a_{pq}+ib_{pq}
\]
där $a_{pq}$ och $b_{pq}$ är reella. Vi har med andra ord
$C= A+iB$ där $A,B$  är reella matriser.
Konstruktionen av $C^*$ ger
att
\[ 
C^*=A^*-iB^*
\]
där $B^*$är den reella transponerade matrisen till $B$.
Notera att eftersom $i^2=-1$ så följer att
 \[ 
C^*C= (A^*-iB^*)(A+iB)=
A^*A+B^*B+ i(A^*B)- B^*A\tag{**}
\]

\medskip


\noindent
{\bf{Unitära matriser.}}
En inverterbar
komplex mattis $U$kallas unitär om
\[ 
\langle U\bold u,U\bold v\rangle
\]
\medskip

\noindent
Notera att (xx) och (i) ovan medför att
\[
\langle U^*U\bold u,\bold v\rangle=\langle \bold u,\bold v\rangle
\]
Detta gäller för alla vektorpar $\bold u,\bold v$
vilket medför att
\[ 
U^*U= E_n
\]
Inversen till en unitär matris ges alltså
av $U^*$.
Läsaren bäör notera att klassen av unitära matriser
med \emph{reella} element svarar mot 
klassen av ortogonala matriser.
\medskip


\noindent
{\bf{Övning}}.
Låt $U$ vara unitär och skriv
\[ 
U=S+iT
\]
där $S,T$ är reella.
Vi får då
\[
E_n= U^*U=(S^*-iT^*)(S+iT)= S^*S+T^*T+i(S^*T-T^*S)
\]
vilket medför att
\[
S^*T=-T^*S\quad\&\quad S^*S+T^*T= E_n
\]


















\newpage


\centerline{\bf{Cayley-Hamilton teorin}}

\medskip


\noindent
I dettya avsnitt  studeras matriser vars element är komplex tal.
Låt $n\geq 2$. Till 
varje $(n,n)$-matrix $A$ hör dess \emph{karakteriska polynom}
\[ 
\Delta_A(\lambda)=det(\lambda E_n-A)\tag{i}
\]
där  $E_n$ är identitetsmatrisen
och $\lambda$ en komplex variabel.
\medskip


\noindent
{\bf{Övning.}}
Visa att (i) är ett polynom av grad $n$ som skrivs på formen

\[
\Delta_A(\lambda)= \lambda^n+d_1\cdot \lambda^{n-1}+\ldots+d_{n-1}\lambda+ d_n
\]
där 
\[
c_n= (-1)^n\cdot det(A)\quad\&\quad
c_1= -(a_{11}+\ldots+a_{nn})
\]

\medskip

\noindent
{\bf{Matrisens spektrum och dess resolvent.}}
Algebrans fundamentalsats ger   en faktorisering
\[ 
\Delta_A(\lambda)=
\prod_{\nu}^{\nu=n}\, (\lambda-\alpha_\nu)
\]
där produkten tas over polynomets nollställen och 
multipla  rötter
upprepas med sin multiplicitet.
Unionen  av  \emph{distinkta}  nollställen betecknas med $\sigma(A)$ och kallas  matrisens spektralmängd, eller kort dess spektrum.
\medskip

\noindent
För varje
$\lambda\in \bold C\setminus \sigma(A)$ är
$det(\lambda\cdot E_n-A)\neq 0$ vilket ger
en invers matris 
\[
R_A(\lambda)=(\lambda\cdot E_n-A)^{-1}
\]

\noindent
{\bf{Övning.}}
{\bf{1}}.
Visa att Cramers inversionsformel medför att
det existerar matriser $\mathcal C_0,\ldots,\mathcal C_{n-1}$
så att
\[
\Delta_A(\lambda)\cdot R_A(\lambda)=
 \mathcal C_0+\lambda\cdot \mathcal C_1+\ldots
\lambda^{n-1}\cdot 
 \mathcal C_{n-1}\tag{1}
 \]
{\bf{2}}.
Visa  att om $S$ är en inverterbar matrix
så gäller likheten
\[ 
\sigma(S^{-1}AS)= \sigma(A)\tag{2.1}
\]
och för tillhörande resolventmatriser gäller 
\[
S^{-1}R_A(\lambda)S= R_{S^{-1}AS}(\lambda)\tag{2.2}
\]
\medskip

\noindent
{\bf{3}}.
Till varje polynom
\[ 
q(\lambda)= \lambda^m+q_1\lambda^{m-1}+\ldots+q_m
\]
bildas matrisen
\[ 
q(A)= 
A^m+q_1A^{m-1}+\ldots+q_mE_n
\]
Visa likheten
\[
\sigma(q(A))= q(\sigma(A))\tag{3.1}
\]
\medskip







\centerline {\bf{Framställning av resolventen.}}
\medskip


\noindent
Från (1) i Övning 1 ger 
division med $\Delta_A(\lambda)$
att
de $\lambda$-beroende elementen i resolventmatrisen 
är \emph{rationella funktioner} av den komplexa variabeln
$\lambda$.
Detta  kan  tolkas så att 
den matrisvärda resolventen
är en rationell funktion av
$\lambda$.
Tillämpas   uppdelningen i partialbråk 
hos polynomet
$\Delta_A(\lambda)$ från § XX erhålles:
\medskip


\noindent
{\bf{Teorem.}}
\emph{Låt $\{\alpha_1,\ldots,\alpha_k\}$
vara de distinkta rötterna hos
$\Delta_A(\lambda)$  där
multipliciteten hos en enskild rot $\alpha_\nu$ betecknas med $e_\nu$.
Då kan resolventen skrivas på formen}
\[
R_A(\lambda)=\sum\sum\,
\frac{C_\nu,j}{(\lambda-\alpha_\nu)^j}\tag{*}
\]
där dubbelsumman tas över $1\leq \nu\leq k$ och för varje fixt $\nu$ förekommer
endast
nämnarnas potenser då $1\leq j\leq e_\nu$.
Matriserna $\{C_{\nu,j}\}$
kallas  $A$-matrisens polära
Cayley-matriser.

\medskip


\noindent
{\bf{Övning.}}
Visa  att 
för varje par av komplexa tal $\lambda,\mu$ utanför $\sigma(A)$  gäller
likheten:
\[
(\lambda-\mu)\cdot R(\lambda)\cdot R(\mu)=R(\mu)-R(\lambda)\tag{1}
\]
Tillämpa denna likhet då $\mu$ närmar sig $\lambda$ för att visa att den komplexa derivatan
\[
\frac{dR(\lambda)}{d\lambda}= -R(\lambda)^2\tag{2}
\]


\medskip


\centerline{\bf{Fallet då $\Delta_A(\lambda)$ har enkla nollställen.}}
\medskip


\noindent
Teorem XX ger då att 
\[ 
R_A(\lambda)=\frac{\mathcal C_1}{\lambda-\alpha_1}+\ldots+
\frac{\mathcal C_n}{\lambda-\alpha_n}\tag{1}
\]

\medskip


\noindent
{\bf{Övning.}}
Använd differentialekvationen från XX
för att visa att
varje matris $\mathcal C_\nu$ är  idempotent, dvs. satisfierar 
\[
\mathcal C_\nu^2=\mathcal  C_\nu
\]
\medskip


\noindent
{\bf{En serieutveckling.}}
Till matrisen $A$ hör dess spektralradie
\[
\rho(A)= \max_{\alpha\in \sigma(A)} \, |\alpha|
\]
Enligt resultatet från § XX
följer att
potensserien
\[
E_n+\sum_{\nu=1}^\infty\, \frac{A^\nu}{\lambda^v}\tag{*}
\]
konvergerar i området av det komplexa talplanet definierat
genom
\[
\{|\lambda|>\rho(A)\}
\]
Vi har dessutom att
\[ 
R_A(\lambda)=
\lambda^{-1}\cdot (E_n-\lambda^{-1}A)=
\frac{E_n}{\lambda}+
\sum_{\nu\geq 1}\,
\frac{A^\nu}{\lambda^{1+\nu}}\tag{**}
\]
dvs. den matrisvärda  resolventen
är framställd med en  Laurentserie i
det öppna komplementet  till den slutna cirkelskivan
med centrum i origo och radie lika med $\rho(A)$.


\bigskip

\centerline{\bf{Tillämpning av residykalkyl}}.


\medskip



\noindent
Vi påminner om att de komplexa linjeintegralerna
\[
\int_{\lambda|=M}\, \frac{d\lambda}{\lambda^k}
=i\cdot\int_0^{2{\pi}}\, e^{i(1-k)\lambda}\,d\lambda=0\quad\, k\geq 2
\] 
medan integralens värde för $k=1$ ger en icke försvinnande residy där
integralen antar  värdet $2\pi i$.
Tillämpas detta från serien (*) erhålles:
\medskip


\noindent
{\bf{Teorem.}} \emph{För varje $M>\rho(A)$ gäller
likheten}

\[
\frac{1}{2\pi i}\cdot \int_{|\lambda|=M}\,R_A(\lambda)\cdot d\lambda=E_n\tag{*}
\]
Mer allmännt, om $Q(\lambda)$ är ett godtyckligt polynom med komplexa koefficienter så gäller likheten
\[
\frac{1}{2\pi i}\cdot \int_{|\lambda|=M}\,Q(\lambda)\cdot R_A(\lambda)\cdot d\lambda=Q(A)\tag{*}
\]
\medskip


\noindent
{\bf{Övning.}}
Visa att Cauchys residyteorem tillsammans med (1) ovan samt
Teorem XX ger följande likhet mellan matriser:
\[
E_n= \mathcal C_1+\ldots+\mathcal C_n
\]
Betrakta sedan för varje $1\leq\nu\leq n$ polynomet
\[
Q_\nu(\lambda)= \prod_{j\neq\nu}\, (\lambda-\alpha_j)
\]
och visa likheten
\[ 
Q_\nu(A)= \frac{\mathcal C_\nu}{Q_\nu(\alpha_\nu)}\quad\, 1\leq\nu\leq n
\]
\medskip


\noindent
{\bf{Övning.}} Visa dessutom att

\[ 
\nu\neq j\implies\mathcal C_\nu\cdot \mathcal C_j
\]

Rang ett egenvektorer !!!


FÖLJ SEDAN UPP med

Tuffare analys hos multipla nollställen !!!




\newpage

\centerline{\bf{En Olikhet av Weierstrass och Schur}}
\medskip


\noindent
Låt $n\geq 2$ och $A$   en symmetrisk $(n,n)$-matris
vars operatornorm är 1.
Låt nu $p(z)$ vara ett polynom med - i allmännhet - komplexa koefficienter
där maximumnormen
\[
|p|_D= \max_{|z|\leq 1}\, |p(z)|=1
\]
Vi kan nu bilda matrisen $p(A)$
vars element i allmännhet är komplexa. Dess - hermiteska - operatornorm
definierades i § xx.
\medskip


\noindent
{\bf{Teorem.}}\emph{Operatornormen hos  $p(A)$ är $\leq 1$.}
\medskip


\noindent



Fallet då $g(z)\neq 0$ i hela $D$.
Då finns en analytisk funktion $\phi(z)$
i $D$  så att
\[ 
g(z)= e^{\phi(z)}
\]
Betrakta nu matrisen
\[
g(A)= e^{\phi(A)}
\]
Enligt antagende är
\[ 
\sigma(A)\subset D
\]
Likgeten
\[ 
\sigma(g(A))= g(\sigma(A))
\]
samt antagandet att $|g|_D\leq 1$
betyder att
\[
\sigma(g(A))\subset D
\]
Det följer nu från xxx att
\[
e^{\sigma(phi(A)}\subset D
\]
Speciellt gäller att
för varje $\lambda\in \sigma(\phi(A)$ blir
\[
|e^\lambda|\leq 1\implies  \mathfrak{Re}(\lambda)\leq 0
\]
Med andra ord
gäller inkusionen
\[
\sigma(\phi(A))\subset \mathfrak{Re}(\lambda)\leq 0
\]
\medskip


\noindent
Låt oss nu betrakta den adjungerade matrisen
$g(A)$ där vi påminner om att
\[
\sigma(g(A)^*=\overline{\sigma(g(A))}\quad\&\quad
e^{\phi(A)^*}= g(A)^*
\]
På samma vis som ovan
kan vi sluta att

\[
\sigma(\phi(A)^*)\subset \mathfrak{Re}(\lambda)\leq 0
\]
\medskip
\medskip


\noindent
Nu introduceras den hemiteska matrisen
\[
B= e^{\phi(A)+\phi(A)^*}=g(A)g(A)^*
\]


Har nu en diagonlieraing
\[
\phi(A)+\phi(AS)^*= U\cdot D\cdot U^*
\]
där
$D$:s diagonala element alla har realdel $\leq 0$.
och nu erhålles
\[ 
B=U\cdot e^D\cdot U^*
\]
där påersatprnpmmen hos $D$ är $\leq 1$.
varefter (II medför att
$g(A)$ har operatornorm $\leq 1$.


slutsats: A en matris där $\sigma(A9\subset D$.
Om nu
\[ 
g(z)= e^{\phi(z))}
\] 
har maxnorm $\leq 1$ i $D$ så operatotrnorm hos A blir mindre än ett.





































































\newpage

\centerline
{\bf{A. Matrices and determinants.}}
\bigskip


\noindent
Let  $A$ be a matrix whose 
 elements $\{a_{pq}\}$ are complex numbers.
The Hilbert-Schmidt norm is defined
by
\[
||A||=\sqrt{ \sum\sum\, |a_{pq}|^2}
\]
where the doube sum extends over all pairs
$1\leq p,q\leq n$.
The operator norm is defined by:
\[
\text{Norm}(A)=
\max_{z_1,\ldots z_n}\,\sqrt{
\sum_{p=1}^{p=n}\, \bigl|\sum_{q=1}^{q=n}\, a_{pq}z_q|^2\,}\tag{*}
\]
with the maximum taken over $n$-tuples of complex numbers
such that
$\sum\, |z_p|^2=1$.
Introduce the Hermitian inner product on
${\bf{C}}^n$
and identify  $A$ with the linear operator which
sends a basis vector $e_q$ into
\[ 
A(e_q)= \sum_{p=1}^{p=n}\, a_{pq}\cdot e_p
\]
If $z$ and $w$ is a pair of complex $n$-vectors one gets:

\[ 
\langle Az,w\rangle=
\sum\sum a_{pq}z_q\bar w_p
\]
The Cauchy-Schwarz inequality
gives
\[
\bigl|\langle Az,w\rangle\bigr|^2\leq
\bigl(\sum_{p=1}^{p=n}\,  \bigl|\sum_{q=1}^{q=n}\, a_{pq}z_q|^2\,\bigr)
\cdot 
\sum_{p=1}^{p=n}\, |w_p|^2\tag{1}
\]
So if both $z$ and $w$ have length $\leq 1$
The definition of the operator norm entails that
\[
\max_{z,w}\, |\langle Az,w\rangle\bigr|=\text{Norm}(A)\tag{**}
\]
where the maximum is taken over
vectors  $z$ and $w$ of unit length.
Next, another application of the Cauchy-Schwarz inequality
shows that if $z$ has unit length, then
\[
\sum_{p=1}^{p=n}\,  \bigl|\sum_{q=1}^{q=n}\, a_{pq}z_q|^2
\leq 
\sum_{p=1}^{p=n}\sum_{q=1}^{q=n}\, |a_{pq}|^2
\]
Then (1) and (**) give the inequality
\[
\text{Norm}(A)\leq ||A||\tag{***}
\]

\medskip

\noindent{\bf{Example.}}
Consider  a matrix $A$ whose elements are non-negative real numbers.
Then it is clear that(*) is maximixed when
the $z$-vector is real with non-negative components. Thus,
\[
\text{Norm}(A)=
\max_{x_1,\ldots x_n}\,\sqrt{
\sum_{p=1}^{p=n}\, \big(\sum_{q=1}^{q=n}\, a_{pq}x_q\bigr )^2\,}\tag{**}
\]
taken over real $n$-vectors
for which
$\sum\, x_p^2=1$ and every $x_p\geq 0$.
The $A$-norm  is found
via Lagrange's multiplier i.e. one employs Lagrange's criterion for extremals
of quadratic forms. The result is that  (**) is maximized by a real non-negative $n$-vector $x$
which satsfies a linear system of equations
\[
\lambda\cdot x^*_j=\sum_{p=1}^{p=n}\, a_{pj}\cdot 
\sum_{q=1}^{q=n}\, a_{pq}x^*_q\tag{1}
\]
Introducing the double indexed numbers
\[
\beta_{jq}= \sum_{p=1}^{p=n}\, a_{pj}a_{pq}\tag{2}
\]
Lagrange's equations corresponds to the system
\[ 
\lambda\cdot x^*_j=\sum_{q=1}^{q=n}\,\beta_{jq}\cdot x_q^*\tag{3}
\]
Notice that the $\beta$-mstrix is symmetric, i.e.
$\beta_{jq}=\beta_{qj}$ hold for ech pair.
So (3) amounts to find an eigenvector
to the symmetric $\beta$-matrix with an eigenvector $x^*$ for which
$x_j^*\geq 0$ hold for each $j$.
In "generic" cases the
$\{\beta_{jq}\}$
are strictly positive numbers, and for such special matrices
the largest eigenvalue was studied by Perron, with further extensions 
As a specific   example 
we consider an 
$n\times n$-matrix of the form
\[ 
T_s=
\begin{pmatrix}
1&s&s&\ldots &s \\
0&1&\ldots &s&s\\
\ldots&\ldots&\ldots&\ldots&\ldots\\
\ldots&\ldots&\ldots&\ldots\\
0&0&\ldots &1& s\\
0&0&\ldots &0& 1\\
\end{pmatrix}
\]
where $s$ is real and positive.
Thus, the diagonal elements are all units and
$T$ is upper triangular with $t_{ij}=s$ for pair $i<j$
while the elements below the diagonal are zero.
In spite of the explicit
expression for $T$ the computation of its
operator norm is rather  involved.
With $n=2$ we find the $\beta$-matrix
\[
B=\begin{pmatrix}
1&s \\
s&1+s^2\\
\end{pmatrix}
\]
and here one seeks the largest root of
its characteristic polynomial to find
the requested norm above. 
For a general $n\geq 2$ and
$s=2$ is of special interest
a classic result which goes back to Hankel and Frobenius is that
\[ 
\text{Norm}(T_2)= \cot\frac{\pi}{4n}\tag{4}
\]


\noindent{\bf{Exercise.}} Prove (4).
If necessary, consult the
literature.


\medskip

\noindent
The next sections discuss determinants. The reader may skip this
for a while
and turn  to § 1 where we treat
basic results about linear operators on finite dimensional vector spaces.

\newpage


HARD PROBLEMS START  



\centerline {\bf{0.A  The Sylvester-Franke theorem.}}
\medskip


\noindent
Let    $A$  be some
$n\times n$\vvv matrix with
elements $\{a\uuu{ik}\}$.
Put
\[
b\uuu{rs}=a\uuu {11}a\uuu {rs}\vvv a\uuu{r1}a\uuu{1s}
\quad\colon\quad 2\leq r,s\leq n
\]
These $b$\vvv numbers give an $(n\vvv 1)\times(n\vvv 1)$\vvv matrix
where $b\uuu{22}$ is put in position $(1,1)$ and so on.
The matrix is denoted by 
$\mathcal S^1(A)$ and called the first order
Sylvester matrix.
If $a\uuu{11}\neq 0$ one has
the equality
\[
a\uuu{11}^{n\vvv 2}\cdot \text{det}(A)=
\text{det}(\mathcal S^1(A))\tag{1}
\]
\medskip

\noindent
{\bf{Exercise.}} Prove (1) or consult a text\vvv book
which
apart from "soft abstract notions"  
does not ignore to
discuss  determinants. Personally I recommend 
Gerhard Kovalevski's text\vvv book
\emph{Determinantenheorie} from 1909 where 
many results about determinants
are proved in an elegant and  detailed fashion.
\medskip

\noindent
{\bf{Sylvester's equation.}}
For every
$1\leq h\leq n\vvv 1$ one constructs the
$(n\vvv h\times (n\vvv h)$\vvv matrix whose elements are

\[ b\uuu{rs}= \det\,
\begin{pmatrix}
a\uuu{11}&a\uuu{12}&\ldots &a\uuu{1h}& a\uuu{1s}\\
a\uuu{21}&a\uuu{22}&\ldots &a\uuu{2h}& a\uuu{2s}\\
\ldots&\ldots&\ldots&\ldots&\ldots\\
\ldots&\ldots&\ldots&\ldots&\ldots\\
a\uuu{h1}&a\uuu{h2}&\ldots &a\uuu{hh}& a\uuu{hs}\\
a\uuu{r1}&a\uuu{r2}&\ldots &a\uuu{rh}& a\uuu{rs}\\
\end{pmatrix}\quad\colon\quad h+1\leq r,s\leq n
\]
\medskip

\noindent
With these notation one has the Sylvester equation:

\[
\det
\begin{pmatrix}
b\uuu{h+1,h+1}&b\uuu{h+1,h+2}&\ldots &b\uuu{h+1,n}\\
b\uuu{h+2,h+1}&b\uuu{h+2,h+2}&\ldots &b\uuu{h+2,n}\\
\ldots&\ldots&\ldots&\ldots\\
\ldots&\ldots&\ldots&\ldots\\
b\uuu{n,h+1}&b\uuu{n,h+2}&\ldots &b\uuu{n,n}\\
\end{pmatrix}=
\bigl[\,\det\begin{pmatrix}
a\uuu{11}&a\uuu{12}&\ldots &a\uuu{1h}\\
a\uuu{21}&a\uuu{22}&\ldots &a\uuu{2h}\\
\ldots&\ldots&\ldots&\ldots\\
\ldots&\ldots&\ldots&\ldots\\
a\uuu{h1}&a\uuu{h2}&\ldots &a\uuu{hh}\\
\end{pmatrix}\,\bigr]^{n\vvv h\vvv 1}\cdot \det(A)\tag{*}
\]
\medskip


\noindent
For a proof of (*) we refer to original work by Sylvester or
[Kovalevski: page xx\vvv xx] which offers several different
proofs of (*).

\bigskip

\noindent
{\bf{The Sylvester\vvv Franke theorem.}}
Let  $n\geq 2$ and $A=\{a\uuu{ik}\}$ an
$n\times n$\vvv matrix.
Let $m<n$ and consider 
the family of minors of size $m$, i.e.
one picks $m$ columns and $m$ rows
which give  an $m\times m$\vvv matrix
whose determinant is called a minor of size $m$
of the given matrix $A$. The total number of
such minors is equal to
\[
N^2\quad\text{where}\quad N= \binom{n}{m}
\]
We have $N$ many strictly increasing sequences
$1\leq \gamma\uuu1<\ldots\gamma \uuu m\leq n$
where a $\gamma$\vvv sequence corresponds to preserved
columns when   a minor is constructed. Similarly we have
$N$ strictly increasing sequences which correspond to preserved rows.
With this in mind we get  for each pair $1\leq r,s\leq N$
a minor $\mathfrak{M}\uuu {rs}$
where the enumerated $r$:th $\gamma$\vvv  sequence preserve columns and similarly
$s$ corresponds to the enumerated sequence of rows.
Now we obtain the $N\times N$\vvv matrix

\[
\mathcal A\uuu m= \begin{pmatrix}
\mathfrak{M}\uuu{11}&\mathfrak{M}{12}&\ldots &\mathfrak{M}\uuu{1N}\\
\mathfrak{M}\uuu{21}&\mathfrak{M}{22}&\ldots &\mathfrak{M}\uuu{2N}\\
\ldots&\ldots&\ldots&\ldots\\
\ldots&\ldots&\ldots&\ldots\\
\mathfrak{M}\uuu{N1}&\mathfrak{M}{22}&\ldots &\mathfrak{M}\uuu{NN}\\
\end{pmatrix}
\]


\noindent
We refer to $\mathcal A\uuu m$ as the Franke\vvv Sylvester matrix of order
$m$. They  are defined for each $1\leq m\leq n\vvv 1$.

\medskip







\noindent
{\bf{0.A.1 Theorem.}}
\emph{For every $1\leq m<n$ one has
the equality}
\[
\mathcal A\uuu m= \text{det}(A)^{\binom{n\vvv 1}{m\vvv 1}}
\]


\medskip

\noindent
{\bf{Example.}} Consider the diagonal $3\times 3$\vvv matrix:

\[
A=\begin{pmatrix}
1&0&0\\
0&1&0\\
0&0&2\\
\end{pmatrix}
\]

\medskip

\noindent
With $m=2$
we have 9 minors of size 2 and the reader can recognize that
when they are arranged so that we begin to remove
the first column, respectively the first row, then 
the resulting $\mathfrak{M}$\vvv matrix becomes
\[
\begin{pmatrix}
2&0&0\\
0&2&0\\
0&0&1\\
\end{pmatrix}
\]
Its determinant is $4= 2^2$ which is in accordance with the general formula since
$n=3$ and $m=2$ give $\binom{n\vvv 1}{m\vvv 1}=2$.
For the proof of Theorem 0.A.1 the reader can consult 
[Kovalevski: page102\vvv 105].




\newpage



\centerline {\bf{0.B Hankel determinants.}}
\bigskip


\noindent
Let $\{c\uuu 0,c\uuu 1,\ldots\}$
be a sequence of complex numbers.
For each  integer $p\geq 0$ and  every $n\geq 0$
we obtain the 
$(p+1)\times (p+1)$\vvv matrix:


\[
\mathcal C\uuu n^{(p)}=
\begin{pmatrix}
c\uuu{n}&c\uuu{n+1}&\ldots&c\uuu{n+p}\\
c\uuu{n+1}&c\uuu{n+2}&\ldots&c\uuu{n+p}\\
\ldots&\ldots&\ldots&\ldots\\
c\uuu{n+p}&c\uuu{n+p+1}&\ldots&c\uuu{n+2p}\\
\end{pmatrix}
\]

\medskip


\noindent
Let $\mathcal D\uuu n^{(p)}$ denote the determinant. One
refers to $\{\mathcal D\uuu n^{(p)}\}$
 as the recursive Hankel determinants.
They  describe  various properties of the given
$c$\vvv sequence.
To begin with we define  the rank   $r^*$ 
of $\{c_n\}$
as follows:
To every non\vvv negative integer $n$
one has the  infinite vector
\[ 
\xi\uuu n=(c\uuu n,c\uuu{n+1},\ldots)
\]
We say that $\{c\uuu n\}$ has finite rank if
there exists a number $r^*$ such that
$r^*$ many $\xi$\vvv vectors
are linearly independent and the rest are linear combinations of these.
\medskip

\noindent
{\bf{Remark.}}
The sequence $\{c\uuu n\}$ gives the formal power series
\[
f(x)=\sum\uuu{\nu=0}^\infty\, c\uuu\nu x^\nu \tag{*}
\]
If $n\geq 1$ we set
\[
\phi\uuu n(x)= x^{\vvv n}\cdot(
f(x)\vvv \sum\uuu{\nu=0}^{n\vvv 1} c\uuu\nu x^\nu)=
\sum\uuu{\nu=0}^\infty c\uuu{n+\nu} x^\nu
\]
From this it is clear
that $\{c\uuu \nu\}$ has finite rank if and only if  the sequence
$\{\phi\uuu\nu(x)\}$
generates a finite dimensional complex subspace of the vector space 
${\bf{C}}[[x]]$ whose elements are formal power series.
If this dimension is finite we find a positive integer
$p$ and a 
non\vvv zero $(p+1)$\vvv tuple $(a\uuu 0,\ldots,a\uuu p)$ of complex numbers
such that the power series
\[ 
a\uuu 0\cdot  \phi\uuu 0(x)+\ldots+a\uuu p\cdot \phi\uuu p(x)=0
\]
Multiplying this equation with $x^p$ it follows that
\[
(a\uuu p+a\uuu{p\vvv 1} x+\ldots+a\uuu o x^p)\cdot f(x)=q(x)
\]
where $q(x)$ is a polynomial.
Hence the finite rank entails that the power series (*) 
represents a rational function.
\medskip


\noindent
{\bf{B.1 Exercise.}}
Conversely, assume that
\[
\sum\, c\uuu\nu x^\nu= \frac{q(x)}{g(x)}
\] 
for some pair of polynomials. Show that $\{c\uuu n\}$ has finite rank.
The next result is also left as an exercise to the reader.

\medskip


\noindent
{\bf{B.2 Proposition.}}
\emph{A sequence $\{c\uuu n\}$ has a finite rank if and only if
there exists an integer $p$ such that}
\[
\mathcal D\uuu 0^{(p)}\neq 0\quad
\text{and}\quad D\uuu 0^{(q)}=0\quad \colon\quad q>p\tag{4}
\]
\emph{Moreover, one has the equality $p=r^*$.}


\medskip

\noindent
{\bf{B.3 A specific example.}}
Suppose that the degree of $q$ is strictly less than that of $g$ in Exercise B.1 
and that the rational function $\frac{q}{g}$
is expressed by a sum of simple 
fractions which means that
\[ 
\sum\, c\uuu\nu x^\nu= \sum\uuu{k=1}^{k=p}\, \frac{d\uuu k}{1\vvv \alpha\uuu k x}
\] 
where $\alpha\uuu 1,\ldots,\alpha\uuu p$ are distinct and every $d\uuu k\neq 0$.
Then we see that
\[ 
c\uuu n=\sum\uuu{k=1}^{k=p}\, d\uuu k\cdot \alpha\uuu k^n\quad 
\text{where we have put}\quad
\alpha\uuu k^0=1\quad\text{ so that}\quad
c\uuu 0=\sum\, d\uuu k
\]
\medskip



\noindent
{\bf{B.4 The reduced rank.}}
Assume that $\{c\uuu n\}$ has finite rank. To each $k\geq 0$ we denote by $r\uuu k$
the dimension of the vector space generated by
$\xi\uuu k,\xi\uuu{k+1},\ldots$.
It is clear that $\{r\uuu k\}$ decrease and we find a non\vvv negative integer
$r\uuu *$ such that $r\uuu k=r\uuu *$ for large $k$ and
refer to $r\uuu *$ as the reduced rank. By the construction
$r\uuu *\leq r^*$. The relation between $r^*$ and $r\uuu *$
is related to the representation
\[
 f(x)= \frac{q(x)}{g(x)}
\]
where $q$ and $g$ are polynomials without common factor.
We shall not pursue this discussion any further but refer to the literature.
See in particular
the exercises
in [Polya\vvv Szegö : Chapter VII:problems 17\vvv 34].




\newpage


DELETE THIS  


\noindent
{\bf{B.5 Hankel's formula for Laurent series.}}
Consider a rational function of the form
\[
R(z)= \frac{q(z)}{z^p\vvv [c\uuu 1z^{p\vvv 1}+\ldots
+c\uuu{p\vvv 1}z+ c\uuu p]}
\]
where the polynomial $q$ has degree $\leq p\vvv 1$.
At $\infty$ we have a Laurent series

\[ 
R(z)= \frac{c\uuu 0}{z}+ 
\frac{c\uuu 1}{z^2}+\ldots
\]
Consider the $p\times p$\vvv matrix
\[
A=\begin{pmatrix}
0&0&&\ldots&0&c\uuu p\\
1&0&0&\ldots&0&c\uuu{p\vvv 1}\\
0&1&0&\ldots&\ldots&c\uuu{p\vvv 2}\\
\ldots&
\ldots&
\ldots&
\ldots&\ldots&\ldots
\\
0&0&0&\ldots&1&c\uuu 1\\
\end{pmatrix}
\]
\medskip

\noindent
Prove  the following  for every $n\geq 1$:

\[
\mathcal D\uuu n^{(p)}=  \mathcal D^{(p)}\uuu 0\cdot
\bigl[\text{det}(\,A\bigr)\bigr ) ^n
\]

\medskip

\noindent

\noindent
{\bf{B.6 The Hadamard-Kronecker identity.}}
For all pairs
of positive integers $p$ and $n$ one has the equality:
\[
\mathcal D\uuu n^{(p+1)}\cdot
\mathcal D\uuu {n+2}^{(p-2)}=
\mathcal D\uuu n^{(p+1)}
\mathcal D\uuu {n+2}^{(p\vvv 1)}-
\bigl[\mathcal D\uuu {n+1}^{(p)}\,\bigr]^2\tag{*}
\]

\medskip


\noindent
{\bf{Remark.}}
The equality (*) is s special case of a determinant formula for symmetric matrices
which is due to Sylvester.  Namely,
let $N\geq 2$ and consider a symmetric matrix

\[
S=\begin{pmatrix}
s\uuu{11}&s\uuu{12}&\ldots &s\uuu{1N}\\
s\uuu{21}&s\uuu{22}&\ldots &s\uuu{2N}\\
\ldots&\ldots&\ldots&\ldots\\
\ldots&\ldots&\ldots&\ldots\\
s\uuu{N1}&a\uuu{N2}&\ldots &s\uuu{NN}\\
\end{pmatrix}
\]
Now we construct three matrices as follows.
First
we get three $(N-1)\times (N-1)$-matrices

\[
S_1= 
\begin{pmatrix}
s\uuu{22}&s\uuu{23}&\ldots &s\uuu{2N}\\
s\uuu{32}&s\uuu{33}&\ldots &s\uuu{3N}\\
\ldots&\ldots&\ldots&\ldots\\
\ldots&\ldots&\ldots&\ldots\\
s\uuu{N2}&s\uuu{N3}&\ldots &s\uuu{NN}\\
\end{pmatrix}
\quad\colon\quad 
S_2= 
\begin{pmatrix}
s\uuu{12}&s\uuu{13}&\ldots &s\uuu{1N}\\
s\uuu{22}&s\uuu{23}&\ldots &s\uuu{2N}\\
\ldots&\ldots&\ldots&\ldots\\
\ldots&\ldots&\ldots&\ldots\\
s\uuu{N-1,2}&s\uuu{N-1,3}&\ldots &s\uuu{N-1,N}\\
\end{pmatrix}
\]
\medskip
\[
S_3= 
\begin{pmatrix}
s\uuu{11}&s\uuu{12}&\ldots &s\uuu{1,N-1}\\
s\uuu{21}&s\uuu{22}&\ldots &s\uuu{2,N-1}\\
\ldots&\ldots&\ldots&\ldots\\
\ldots&\ldots&\ldots&\ldots\\
s\uuu{N-1,1}&s\uuu{N-1,2}&\ldots &s\uuu{N-1,N-1}\\
\end{pmatrix}
\]
\medskip

\noindent
We have also the $(N-2)\times (N-2)$-matrix
when extremal rows and columns are removed:

\[ S_*=\begin{pmatrix}
s\uuu{22}&s\uuu{23}&\ldots &s\uuu{2,N-1}\\
s\uuu{32}&s\uuu{33}&\ldots &s\uuu{3,N-1}\\
\ldots&\ldots&\ldots&\ldots\\
\ldots&\ldots&\ldots&\ldots\\
s\uuu{2,N-1}&s\uuu{3,N-1}&\ldots &s\uuu{N-1,N-1}\\
\end{pmatrix}
\]
\medskip

\noindent
{\bf{B.7 Sylvester's identity.}}
\emph{One has the determinant formula:}
\[
\det(S)\cdot \det(S_*)=
\det S_1)\cdot \det S_3-\bigl(\det S_2\bigr)^2
\]
\medskip

\noindent{\bf{Exercise}}. Prove this result and deduce the 
Hadamard-Kronecker equation.














\newpage




\centerline{\bf{0.C The Gram\vvv Fredholm formula.}}
\medskip


\noindent
A result whose discrete version is due to Gram  was 
extended to integrals  by
Fredholm and goes as follows:
Let $\phi\uuu 1,\ldots,\phi\uuu p$
and $\psi\uuu 1,\ldots,\psi\uuu p$ be two
$p$\vvv tuples of continuous functions on
the unit interval.
We get the $p\times p$\vvv matrix with elements
\[
a\uuu {\nu k}= \int\uuu 0^1\, \phi\uuu \nu(x)\psi\uuu k(x)\cdot dx
\]
At the same time
we define the following  functions on $[0,1]^p$:

\[ 
\Phi(x\uuu 1,\ldots,x\uuu p)
=\text{det}
\begin{pmatrix}
\phi\uuu 1(x\uuu 1)&\cdots&\phi\uuu 1(x\uuu p)\\
\cdots &\cdots&\cdots \\
\cdots &\cdots&\cdots\\
\phi\uuu p(x\uuu 1)&\cdots &\phi\uuu 1(x\uuu p)\\
\end{pmatrix}\quad\colon\quad
\Psi(x\uuu 1,\ldots,x\uuu p)
=\text{det}
\begin{pmatrix}
\psi\uuu 1(x\uuu 1)&\cdots&\psi\uuu 1(x\uuu p)\\
\cdots &\cdots&\cdots \\
\cdots &\cdots&\cdots\\
\psi\uuu p(x\uuu 1)&\cdots &\psi\uuu 1(x\uuu p)\\
\end{pmatrix}
\]

\medskip

\noindent
Product rules for determinants give the Gram\vvv Fredholm  equation
\[
\text{det}(a\uuu{\nu k})=
\frac{1}{p\,!}\int\uuu{[0,1]^p}\, 
\Phi(x\uuu 1,\ldots,x\uuu p)\cdot
\Psi(x\uuu 1,\ldots,x\uuu p)\cdot
dx\uuu 1\ldots dx\uuu p\tag{*}
\]
\medskip

\noindent
{\bf{Exercise.}} Prove (*)
or consult the literature.
See in particular the classic
book [Bocher]
which contains a detailed account about Fredholm
determinants and
their role for solutions to integral equations.





 
\bigskip

\noindent
\centerline
{\bf{0.D Resolvents of integral operators.}}
\medskip

\noindent
Fredholm studied integral equations of the form
\[
\phi(x)\vvv \lambda\cdot 
\int\uuu\Omega\,
K(x,y)\cdot \phi(y)\cdot dy=
f(x)\tag{*}
\]
where $\Omega$ is a bounded domain in some euclidian space
and the kernel function $K$ is complex\vvv valued. In general no
symmetry condition is imposed.
Various regularity conditions can be imposed upon the kernel. The simplest is when
$K(x,y)$ is a continuous function in
$\Omega\times\Omega$.
The situation becomes more involved when singularities occur, for example when
$K$ is $+\infty$ on the diagonal, i.e. $|K(x,x)|=+\infty$.
This occurs for example when
$K$ is derived from Green's functions
which yield fundamental solutions to
elliptic PDE\vvv equations
where corresponding boundary value problems are solved
via integral equations. 
To obtain square integrable solutions in (*) for less regular kernel
functions, 
the original determinants used by Fredholm were modified by
Hilbert which avoid the singularities
and lead to quite general 
formulas for resolvents of the integral operator $\mathcal K$ defined by
\[
\mathcal K(\phi)(x)=\int\uuu\Omega\,
K(x,y)\cdot \phi(y)\cdot dy
\]
One studies foremost the case when $K$ is square integrable,  i.e.
when
\[
\iint\uuu{\Omega\times\Omega}\, |K(x,y)|^2\, dxdy<\infty\tag{*}
\]
An eigenvalue is a complex number
$\lambda\neq 0$ for which there exists a non\vvv zero function $\phi$ such that
\[
\mathcal K(\phi)= \lambda\cdot \phi
\]
It is not difficult to show that (*) entails  that 
the set of eigenvalues form a discrete set $\{\lambda\uuu n\}$.
In the article [Schur: 1909] Schur proved 
the inequality
\[
\sum\, \frac{1}{|\lambda\uuu n|^2}\leq
\iint\uuu{\Omega\times\Omega}\, |K(x,y)|^2\, dxdy<\infty\tag{**}
\]

\noindent
Notice that  one does not assume that
the kernel function is symmetric, i.e. in general $K(x,y)\neq K(y,x)$.

\bigskip

\noindent
{\bf{0.D.1 Hilbert's determinants.}}
Let $K$ be a kernel function for which the integral (*) is finite.
A typical case is that
$K$ is singular on the diagonal subset
of $\Omega\times\Omega$.
To each positive integer $m$ one associates a pair of matrices of size
$(m+1)\times m(+1)$ whose elements depend upon a pair 
$(\xi,\eta)
\in\Omega\times\Omega$ and an $m$\vvv tuple of distinct points
$x\uuu 1,\ldots,x\uuu m$ in $\Omega$: 

\[
C^*\uuu m=
\begin{pmatrix}
0&K(\xi,x\uuu 1)&K(\xi,x\uuu 2)&\ldots&\ldots &K(\xi, x\uuu m)\\
K(x\uuu 1,\eta)&0&K(x\uuu 1,x\uuu 2)&\ldots&\ldots &K(x\uuu 1,x\uuu m)\\
K(x\uuu 2,\eta)&K(x\uuu 2,x\uuu 1)&0
&\ldots&\ldots&0\\
\ldots&
\ldots&
\ldots&
\ldots&\ldots&\ldots
\\
\ldots&
\ldots&
\ldots&
\ldots&\ldots&\ldots
\\
K(x\uuu m,\eta)&K(x\uuu m,x\uuu 1)&K(x\uuu m,x\uuu 2)&\ldots &\ldots&0\\
\end{pmatrix}
\]

\bigskip


\[
C\uuu m=\begin{pmatrix}
0&K(x\uuu 1,x\uuu 2)&&\ldots&0&K(x\uuu 1, x\uuu m)\\
K(x\uuu 2,x\uuu 3)&0&K(x\uuu 2,x\uuu 3)
&\ldots&&K(x\uuu 2,x\uuu m)\\
\ldots&\ldots &\ldots&\ldots&\ldots&\ldots\\
\ldots&
\ldots&
\ldots&
\ldots&\ldots&\ldots
\\
K(x\uuu m,x\uuu 1)&K(x\uuu m,x\uuu 2)
&K(x\uuu m,x\uuu 3) &\ldots&K(x\uuu m,x\uuu{m\vvv 1})&0\\
\end{pmatrix}
\]


\noindent
Put:
\[
D^*\uuu m(\xi,\eta)=
\int\uuu{\Omega^m}\, 
C^*\uuu m(\xi,\eta: x\uuu 1,\ldots,x\uuu m)\cdot dx\uuu 1\cdots dx\uuu m\tag{i}
\]
\[
D\uuu m=
\int\uuu{\Omega^m}\, 
C\uuu m(x\uuu 1,\ldots,x\uuu m)\cdot dx\uuu 1\cdots dx\uuu m\tag{ii}
\]
Thus, we take the integral over the $m$\vvv fold product of
$\Omega$.
Next, let $\lambda$ be a new complex parameter and set
\[ 
\mathcal D^*(\xi,\eta,\lambda)=
\sum\uuu{m=1}^\infty\, \frac{(\vvv\lambda)^m}{m!}
\cdot D^{**}\uuu m(\xi,\eta)
\]

\[ 
\mathcal D(\lambda)=1+
\sum\uuu{m=1}^\infty\, \frac{(\vvv\lambda)^m}{m!}
\cdot D_m
\]

\noindent


\medskip

\centerline{\bf{Some results by Carleman}}
\medskip


\noindent
Using the Fredholm-Hilbert determinants 
some
conclusive facts about integral operators
were established
by Carleman in the article \emph{Zur Theorie der Integralgleichungen}
from 1921  when the kernel
$K$ is of the Hilbert-Schmidt type, i.e. below we assume that
\[ 
\iint\, |K(x,y]|^2\, dxdy<\infty\tag{*}
\]


\medskip

\noindent
{\bf{D.2.1 Theorem.}} \emph{The kernel of the resolvent associated to the integral operator
$\mathcal K$ is for each complex $\lambda$ outside the spectrum 
given by}
\[
\Gamma(\xi,\eta;\lambda)=
K(\xi,\eta)+\frac{\mathcal D^*(\xi,\eta,\lambda)}{
\mathcal D(\lambda)}
\]


\noindent
{\bf{D.2.2  Remark.}}
Let  $\{\lambda\uuu\nu\}$ be the discrete spectrum of $\mathcal K$ 
where multiple
eigenvalues are repeated when the corresponding 
eigenspaces have dimension $\geq 2$. 
This spectrum constitutes the zeros of the entire function
$\mathcal D(\lambda)$. So when
$\lambda$ is outside this zero set
the inverse operator $(\lambda\cdot E-\mathcal K)^{-1}$
is the integral operator
defined by
\[
f\mapsto \int_\Omega\, \Gamma(\xi,\eta;\lambda)\cdot f(\eta)\, d\eta
\]
where $\xi$ and $\eta$ denote variable points in
$\Omega$.
Using
inequalities of Fredholm\vvv Hadamard type for determinants, it
is also proved in [ibid] that:
\[
\int\uuu{\Omega}\, \Gamma (\xi,\xi; \lambda)\cdot d\xi=
\vvv \lambda\cdot \sum\uuu{\nu=1}^\infty\,
\frac{1}{\lambda\uuu\nu(\lambda\vvv\lambda\uuu\nu)}\tag{D.2.3}
\]
\medskip


\noindent
Another major result in [ibid]
deals with
the function $\mathcal D(\lambda)$.

\medskip

\noindent
{\bf{D.2.4 Theorem.}}
\emph{$\mathcal D(\lambda)$ is an entire function of the complex parameter
$\lambda$ given by a Hadamard product}
\[
\mathcal D(\lambda)=
\prod\,(1-\frac{\lambda}{\lambda_n})\cdot e^{\frac{\lambda}{\lambda_n}}\tag{1}
\]
\emph{where $\{\lambda_n\}$ satisfy}
\[
\sum\, |\lambda_n|^{-2}<\infty\tag{2}
\]
\medskip


\noindent
{\bf{Remark.}}
Prior to this Schur had proved a
representation 
for $\mathcal D(\lambda)$ as above adding
a factor $e^{b\lambda^2}$.
So the novelty
in Carleman's
work is that
$b=0$ always hold.
Apart from Schur's result that (2) above is convergent, a
crucial
step in Carleman's  proof of (1) 
was to use  an  inequality for
determinants which goes as follows:
Let $q>p\geq 1$ be a pair of integers
and
$\{a\uuu{k,\nu}\}$ a doubly\vvv indexed sequence of complex numbers
which appear as elements in a $p+q$\vvv matrix of the form:






\[
\begin{pmatrix}
0&\ldots&0&a\uuu{1,p+1}&\ldots &a\uuu{1,p+q}\\
0&\ldots&0&a\uuu{2,p+1}&\ldots &a\uuu{1,p+q}\\
\ldots&
\ldots&\ldots&\ldots&\ldots&\ldots \\
0&\ldots&0&a\uuu{p,p+1}&\ldots &a\uuu{p,p+q}\\
a\uuu{p+1,1}&\ldots &a\uuu{p+1,p}&a\uuu{p+1,p+1}&\ldots&
a\uuu{p+1,p+q}\\
\ldots&
\ldots&\ldots&\ldots&\ldots&\ldots
\\
a\uuu{p+q,1}&\ldots &a\uuu{p+q,p}&a\uuu{p+q,p+1}&\ldots
&a\uuu{p+q,p+q}
\\
\end{pmatrix}\tag{*}
\]
\medskip



\noindent
For each pair $1\leq m\leq p$ we put
\[
L\uuu m=\sum\uuu{\nu=1}^{\nu=q}\, |a\uuu{m,p+\nu}|^2
\quad\colon\quad 
S\uuu m=\sum\uuu{\nu=1}^{\nu=q}\, |a\uuu{p+\nu,m}|^2
\quad\colon\quad 
N=\sum\uuu{j=1}^{j=q} \sum\uuu{\nu=1}^{\nu=q}\, |a\uuu{p+j,p+\nu}|^2
\]





\noindent
{\bf{D.2.5 Theorem.}}\emph{
Let $D$ be the determinant of the matrix  (*). Then}
\[
|D|\leq (L\uuu 1\cdots L\uuu p) ^ {\frac{1}{p}}\cdot
\sqrt{M\uuu 1\cdots M\uuu p}\cdot
\frac{N^{\frac{q\vvv p}{2}}}{(q\vvv p)^{\frac{q\vvv p}{2}}}
\]


\noindent
\emph{Proof.}
After unitary transformations of the last $q$ rows and 
the last $q$ columns respectively, the proof is reduced to the case
when $a\uuu{jk}=0$ for pairs $(j.k)$ with  $j\leq p$ and $k>p+j$ or with
$k\leq p$ and $j>p+k$.
Here $L\uuu m, S\uuu m$ and $N$  are unchanged and we get
\[ 
D=(\vvv 1)^p\cdot \prod\uuu{j=1}^{j=p}\,
a\uuu{j,p+j}\cdot \prod\uuu{k=1}^{k=p}\,a\uuu {p+k,k}
\cdot 
\det \begin{pmatrix}
a\uuu{p+1,2p+1}&\ldots &a\uuu{p+1,p+q}\\
\ldots&
\ldots&\ldots
\\
a\uuu{p+q,p+1}&\ldots &a\uuu{p+q,p+q}\\
\end{pmatrix}
\]
\medskip


\noindent
The absolute value of the last determinant is
majorized by   Hadamard's inequality in § F.XX   and 
the requested inequality in Theorem D.2.5 follows.




\newpage





\centerline{\bf{1. Wedderburn's theorem.}}

\bigskip


\noindent
A finite dimensional ${\bf{C}}$-algebra $\mathcal A$
is  an associative  ring
which contain
${\bf{C}}$ as a central subfield,  i.e.
$\lambda\cdot a=a\cdot \lambda$ for pairs $a\in\mathcal A$
and complex numbers $\lambda$.
The ring product gives  the family of
left ideals. They consist of complex subspaces $L$
which are stable under left multiplication, i.e.
$aL\subset L$ hold for every element $a$ in 
$\mathcal  A$.
One may also regard two-sided ideals $J$  
where one requires that btoh $aJ$ and $Ja$ are contained in $J$
for every $a\in\mathcal A$.
One says that  $\mathcal A$ is called a simple algebra if the sole 2-sided ideals are 
$\mathcal A$ and the trivial zero ideal.
Examples of  finite dimensional ${\bf{C}}$-algebras
are given by the matrix-algebras $\{M_n({\bf{C}}\,\colon\, n\geq 1$.
It turns out that they give the sole simple algebras.
\bigskip



\noindent
{\bf{1.1 Theorem.}}
\emph{Let $A$ be a finite dimensional and simple 
${\bf{C}}$-algebra.
Then there exists an integer $n$ such that}
\[
A\simeq M_n({\bf{C}})
\]
\medskip


\noindent
The proof requires several steps.
Let us first show that
the matrix algebras are simple.
With $n\geq 2$ we  put 
$\mathcal A=M_n({\bf{C}})$ and identify every  matrix
with a linear operator
on ${\bf{C}}^n$.
it contains special matrices
$e_1,\ldots,e_n$ where the elements of $e_p$ are zero except fro
$a_{pp}$ which is equal to one.
product rukes for matrices show that
\[
e_pe-q=0\quad|colon\, p\neq q
\]
At thews same time we notice that
the identity element is $e_1+\ldots+e_n$ and that
$e_p=e_p^2$. it can  be expressed by saying that
$\{e_p\}$ are pairwise  orthogonal idempotent elements.
Wirh $p$ fixed we have the left ideal 
\[
L_p=Ae_p
\]
The reader can  check that it consists of all matrices whose columns of degree $q\neq p$
are zero.
The left ideal $L_p$ is minimal. For let
$\xi$ be a non-zero matrix in $L_p$ which means that there exists
at least some $\nu$ where the matrix element $\xi_{\nu p}\neq 0$.
mutlplying with a sclar we can assume that
$\xi_{\nu p}=1$ and then
\[
e_\nu\cdot \xi)=e_p
\]
It follows that the principal left ideal generated by $\xi$ is equal to $L_p$.
Thus, every non-zero element in $L_p$ generates $L_p$ whuich shows that
this left ideal is minimal.
Next, let $\xi= \{a_{qp}\}$
be a non-zero matrix.
and choose $p$ so that $a_{qp}\neq 0$ for at least one $\xi$.
Then $\xi\cdot e_p$ is a non-zero element in $L_p$ so the
2-sided ideal generated by $\xi$ contains the minmal left ideal $L_p$.
If we consider another integer $q$
we take the matrix $\xi$ 
with a single non-zero element  placed at $(p,q)$
which is equal to one. Then we see that $e_p\cdot \xi=e_q$
and hence the 2-sided ideal contains $L_q=Le_q$, Since this hold for every
$1\leq q\leq n$
the rader mat conclude that the 2-sided ideal generated by $\xi$ is the whole ring
$A$. This proves that $A$ is simple.
\medskip

\noindent
Next, let $A$ be a non-zero matrix which commutes with all other matrices.
To prove that $A$ is a complex multiple of the identity matrix
one argues as follows:
The matrix elements of $A$ are $\{a_{\nu k}\}$. Fir a given $p$
the product $A\cdot e_p $ gives a matrix with a single
non-zero column put in place $p$ with elements $\{a_{\nu p}\}$.
At thes ame time $e_pA$ is a matrix with a single non-zero row
placed in degree $p$. So the equality $e_pA=Ae_p$ entails that
\[
a_{qp}=0\quad\colon q\neq p
\]
If $A$ commnutes with
all the $e$-matrices we conckude that
$A$ is a diagonal matrix,  i.e. the elements outside
the diagonal are all zero.
There remain to see that the diagonal elenets are all equal.
Suppose for example that $a_{11}\neq a_{22}$.
Now there exists the matrix $B$ where $b_{12}=b_{21}=1$
and all other elements are zero.
Then we see that
\[
B\cdot A= a_{11}e_2+a_{22}e_1\quad\colon\quad
A\cdot B= a_{11}\cdot e_1+a_{22}\cdot e_2
\]
Hence the equality $AB=BA$ entails that
$a_{11}=a_{22}$. In  the same way one proves that
all diagoal elements are equal. Hence the center of the matrix algebra is reduced to
complex multiples of the identity.

\medskip








\noindent
{\bf{B. Exercise.}} Set 
$\mathcal A=M_n({\bf{C}})$ and identify every  matrix
with a ${\bf{C}}$-linear operator on
${\bf{C}}^n$.
Toeach left iodeal $L$ we assign the null space
\[
L^\perp=\{ v\in {\bf{C}}^n\,\colon\, L(v)=0\} 
\]
This one takes the intersection of the null spaces of operators from $L$.
Show that $L^\perp$ determines $L$ in the sense
that a matrix $Q$ belongs to $L$ if and only if its null-space contains
$L^\perp$. Concclude that
if $p$ is the dimension  of the vector space
$L^\perp$ then
the dimension of $L$ regared as a vector space is equal to
$n(n-p)$. Moreover, by $L\mapsto L^\perp$ one gets a bijective map
between
the family of left ideals in the matrix algebra and
subspaces of ${\bf{C}}^n$.

\bigskip







\medskip

\noindent
\centerline {\bf{Proof of 1.1 Theorem.}}
\medskip

\noindent
Denote by $\mathcal L_*$ the family of non-zero
left ideals $L$ in $A$ which are minimal in the sense that
every
non-zero left ideal $L_1\subset L$ is equal to $L$.
Since $A$ is a finite dimensional vector space  it is clear that
there exists at least one minimal left ideal $L$.
Identfying $L$ with a complex vector space of some dimension 
$k$, we get the ${\bf{C}}$-algebra
\[
\mathcal M=\text{Hom}_{{\bf{C}}}(L,L)
\]
Choosing a basis in the complex vector space $L$
one has 
\[
\mathcal M\simeq M_k({\bf{C}})
\]
We shall prove that $\mathcal M\simeq A$ which by gives Wedderburn's theorem.
To attain this we take $a\in A$ which by left multiplication
gives a map
\[
a^*\colon x\mapsto ax\quad\colon\quad x\in L
\]
Since ${\bf{C}}$ by assumption is a central subfield  of
$A$ these maps are complex linear and hence
$a^*$ is an element in $\mathcal M$.
If $b$ is another element in $A$
we get $b^*$ and the composed linear operator
$b^*\circ a^*$, defined by
\[
x\mapsto bax=(ba)^*(x)
\]
Hence 
\[
a\mapsto a^*\tag{i}
\]
is an algebra homomorphism from
$A$ into $\mathcal M$.
We claim  that this map is injective.
For if $a^*$ is the zero map we use that $L$ is a left ideal which gives
\[
ax\xi=0
\]
for all $x\in A$ and $\xi\in L$.
This gives $a^*\circ x^*=0$
and since it is obvious that $x^*\circ a^*=0$ also holds, we conclude that
the kernel of the map (xi is a 2-sided ideal in $A$. 
Since $A$ is simple 
this kernel is zero which proves that
(i) is injective.
\medskip

\noindent
\emph{Proof of surjectivity.}
First we notice that if $x\in A$ is such that
$Lx\neq 0$ then this is a left ideal and since
$L$ is minimal the reader can check that we also have $Lx\in \mathcal L_*$.
By assumption  the 2-sided ideal of $L$
is the whole ring
$A$. Hence there exists a finite set of $A$-elements $\{x_\nu\}$ such that
\[
A=Lx_1+\ldots+Lx_m\tag{i}
\]
Above we can choose $m$ minimal which 
gives 
a direct sum
\[
A=Lx_1\oplus\ldots\oplus Lx_m\tag{ii}
\]
For suppose that 
\[
\xi_1x_1+\ldots+\xi_mx_m=0\quad\colon\, \xi_\nu\in L
\]
where $\xi_kx_k\neq 0$ for some $k$.
Since $Lx_k$ is minimal the resder can chek that
$Lx_K$ now can be deleted in (i) which contraidicts the minmal chose of $m$.
Hence one has the direct sum in (ii).
Next, for every $1\leq k\leq m$ the map from $L$ into $Lx_k$ defined by 
\[
x\to x\cdot x_k
\] 
is surjetive and since $L$ was minmal the reader can check that it is also
injective. It means that the vector spaces
$L$ and $Lx_k$ are isomorphic. Counting dimensions we conclude that
\[
\dim_{{\bf{C}}}(A= m\cdot k
\]
Since the map from $A$ into $\mathcal M$  was injective
and $B$ has dimension $k^2$ 
we have $m\leq k$ and there
remains onoy to prove the opposite inequality
\[
k\leq m\tag{*}
\]
To get (*)  we take the identity element $1$ in $A$ and via 
(ii) one gets an $m$-tuple $\{\xi_\nu\}$ in $L$ so that
\[
1=\xi_1x_1+\dots+\xi_kx_m\tag{1}
\]
Put $e_\nu=\xi_\nu\cdot x-\nu$.
Mupltilying to themleft by some $e_k$ in (1) we get
\[
e_k= e_ke-1+\ldots+e_ke_m
\]
The direct sum in (xx) entials that
\[
e_ke_\nu=0\,\colon \nu\neq k\quad \&\quad e_ke_k=e_k
\]
This can be exrepssed by saying that $\{e_\nu\}$
are pairwise  orthogonal idempotent elements  in $A$.
For a fixed $k$ the equality $e_k=e_k^2$ entails  that
$e_k\cdot A\cdot e_k$ is a subalgebra of $A$.
If $x=e_k\cdot x\cdot e_k$ is an element in this subalgebra
then right mulitipkication
by $x$ on the left ideal $Ae_k$ is left $A$-linear, i.e.
one has a map
\[
e_k\cdot A\cdot e_k\to \text{Hom}_A(Ae_k,Ae_k)
\]
Now we use that $Ae_k$ is a minimal left ideal, i.e. as a left $A$-module it is simple.
This implies that
the right hand side in (xx) is a divsion ring, i.e. every non-zero element is invertible,
Since the complex field is algebraically closed
this division ring is equal to ${\bf{C}}$. 
Moreover, if $\xi=e_kxe_k$ is such that
its image in (xx) is zero, then
\[
e_k\xi= e_k^2xe_k=e_kxe_k=\xi=0
\]
So (xx) is injective and hence
\[
e_kAe_k={\bf{C}}
\]
\medskip


\noindent
Let us now take some $j\neq k$ and consider the
space
\[
\text{Hom}_A(Ae_j,Ae_k)
\]
Every left $A$-linear map from
$Ae_j$ into $Ae_k$ is induced by
right multiication with an element $\xi$ and since
$e_j$ and $e_k$ are idenpotens one has
\[
\xi=e_j\xi e_k
\]
Conversly we notice that for every $x\in A$, one 
gets a $\xi$-element $a_kxe_j$. Hence the vector space 8xx) above can be identfiied with
the subset of $A$ given by
\[
e_jAe_k
\]
We have already seen that the left $A$-modukes generated by $e_k$ and $e_j$ are isomorphic
and then (xx) entails that
\[
\dim_{\bf{C}}(e_jAe_k)=1
\]
Now (*) follows because with $L=Ae_1$
one has
\[
L=\sum_{j=1}^{j=m}\, e_jAe_1
\]
which proves thst the $k$-dimensional vector space $L$ has dimension
$m$ at most which gives (*) and finishes the proof
of Wedderburn's theorem.












\newpage
















we construct the principal left ideal
\[
Aa=\{ x\cdot a\,\colon x\in A\}
\]
and since $L$ is minimal we have $Aa=L$.
Let $a$ be an element as above.
Suppose that $x\in A$ is such that
$ax\neq 0$.






 left ideal $L$  in $A$ is  minimal if
there does not exist any non-zero left ideal which is strictly smaller than
$L$. Denote by $\mathcal L_*$ the family of all minimal left
ideals. Notice that if $0\neq x\in L$ for some minimal ideal then
we must have $A\cdot x=L$, i.e. the single element $x$ generates $L$.
Moreover it is clear
that a left principal
ideal $A\cdot x$ belongs to $\mathcal L_*$ if and only if the left annihilator:
\[ 
\ell(x)=\{a\in A\quad\colon\, ax=0\}
\]
is a maximal left ideal. So when
$A\cdot x\in\mathcal L_*$ and  $a\in A$ is such that
$xa\neq 0$, then $xa$ 
also generates a minimal left ideal and
the maximality of $\ell(x)$ gives the equality
\[
\ell(x)=\ell(xa)
\]


\noindent
It follows that the  left $A$-modules $Ax$ and $Axa$ are isomorphic.
Next,
every
left ideal is in particular  a complex subspace. 
If $N$ is the dimension of the complex vector space
$A$ then every  increasing sequence of complex subspaces 
has at most $N$ strict inclusions. This shows  that there exist
minimal left ideals.
Choose some $a_0\neq 0$ where $Aa_0$ is a minimal left ideal.
By left multiplication every $a\in A$ gives a ${\bf{C}}$-linear operator
on $Aa_0$ 
defined by
\[
a^*(xa_0)= a\cdot x\cdot a_0\quad\colon\quad x\in A\tag{1}
\]
If $a$ and $b$ is a pair of elements in $A$
the composed ${\bf{C}}$-linear operator $b^*\circ a^*$ is given by
\[
b^*\circ a^*(xa)=
b^*(axa_0)= baxa_0=(ba)\cdot x\cdot a_0= (ba)^*(xa_0)\tag{2}
\]
Hence $a\mapsto a^*$ is a 
homomorphism from
$A$ into the algebra 
$\mathcal M=\text{Hom}_{{\bf{C}}}(Aa_0,Aa_0)$

\medskip

\noindent
\emph{Sublemma}. \emph{The map $a\mapsto a^*$ is injective.}
\medskip

\noindent
\emph{Proof.}
To say that $a^*=0$ means that
\[
axa_0=0\quad\text{for all}\quad x\in A
\]
Hence the \emph{two-sided} ideal generated by $a$ is contained in
$\ell(a_0)$. So if $a\neq 0$ this two-sided ideal would be the whole ring
and then
$1\cdot a_0=a_0=0$ which is a contradiction.

\medskip


\noindent
\emph{Proof continued}. 
Let $k$ be the dimension of the complex vector space $Aa_0$
which after a chosen basis identifies
$\mathcal M$ with  the algebra of 
$k\times k$-matrices.
Wedderburn's Theorem  follows
from the  Sublemma  if prove that the map (1)
is surjective.
Counting dimensions this amounts to show the equality
\[ 
\text{dim}_{{\bf{C}}}\, A= k^2\tag{3}
\]



\noindent
To prove (3) we consider the
the two-sided ideal generated
by the family
$\{ Aa_0x\,\colon\, x\in A\}$. Since $A$ is simple it gives
the whole ring and we find
a finite set $\{x_1,\ldots,x_m\}$ such that
\[ 
A= Aa_0x_1+\ldots+\ldots+Aa_0x_m\tag{4}
\]


\noindent 
Here we can choose $m$ to be minimal which gives a direct sum in (4), i.e.
 now
\[ 
 A=Aa_0x_1+\oplus\ldots\oplus\,Aa_0x_m\tag{5}
\]

\noindent
By previous observations the left ideal $Aa_0x_i\simeq Aa_0$
for each $i$. So the direct sum decomposition (5) entails that
\[
\text{dim}_{{\bf{C}}}\, A= m\cdot k\tag{6}
\]

\noindent
Hence (3) follows if we can show the equality $m=k$.
To attain this 
we consider the unit element $1_A$
in the ring $A$ which by 
(5) has an expression:
\[ 
1_A=\xi_1+\ldots+\xi_m\quad\colon\,\xi_i=b_ia_0x_i\tag{7}
\]
for some $m$-tuple $b_1,\ldots,b_m$.
Since (5) is a direct sum it is easily seen that
the $\xi$-elements satisfy:
\[
\xi_i^2=\xi_i\quad\text{and}\quad  \xi_i\cdot \xi_k\,\,\colon\,\, i\neq k\tag{8}
\]

\noindent
Thus, $\{\xi_i\}$ are mutually orthogonal idempotents.
Moreover, from the previous observations we 
have the isomorphism
of left $A$-modules
\[
A\xi_i\simeq Aa_0\quad\colon\quad 1\leq i\leq m\tag{9}
\]

\noindent
Next, since $\xi$ is an idempotent we notice that
$\xi_iA\xi_i$ is a ${\bf{C}}$-algebra which is naturally
identified
with
the Hom-space
\[ 
\text{Hom}_A(A\xi_i,A\xi_i)\tag{10}
\]
Since $A\xi_i$ is a simple left $A$-module this Hom-algebra is a division
ring and since
${\bf{C}}$ is algebraically closed it follows that
\[
\xi_iA\xi_i\simeq {\bf{C}}\quad\colon\, 1\leq i\leq m\tag{11}
\] 

\medskip

\noindent
Next, for each pair $i,j$
consider the Hom-space:
\[
E_{ij}=\text{Hom}_A(A\xi_i,A\xi_j)\tag{12}
\]
By the isomorphisms in (9) and the equality (11) it follows that
$E_{ij}$ are one-dimensional complex vector spaces for all pairs $i,j$.
Moreover, the reader may verify the following equality:
\[ 
E_{ij}=\{ \xi_ix\xi_j\quad\colon x\in A\}\tag{13}
\]

\noindent
Next, consider $\xi_1$. From the expression of $1_A$ in (7) we obtain
\[ 
A\xi_1=\sum_{i=1}^{i=m}\, \xi_i\cdot A\cdot \xi_1=\sum\, E_{i1}
\]
Since $\{E_{i1}\}$ are 1-dimensional we get the inequality
\[ 
k=\dim_{{\bf{C}}}(A\xi_1)\leq m\tag{*}
\]


\noindent
At this stage we are done
since the injectivity in the Sublemma
and (6) give
\[ 
m\cdot k\leq k^2\implies m\leq k
\] 
Together with (*) it follows that $m=k$ and the requested equality (3)
follows.




\newpage


\centerline{\bf{2. Resolvents}}
\bigskip

\noindent
Let $A$ be some matrix in $M_n({\bf{C}})$.
Its characteristic polynomial is defined by
\[ 
P_A(\lambda)=
\text{det}(\lambda\cdot E_n-A)\tag{*}
\]
By the fundamental theorem of
algebra $P\uuu A$
has $n$ roots
$\alpha_1,\ldots,\alpha_n$ where eventual
multiple roots are repeated.
The 
union of  distinct roots is denoted by
$\sigma(A)$ and called the spectrum of $A$.
Since matrices with non-zero determinants are invertible
we obtain a matrix valued function defined in
${\bf{C}}\setminus\sigma(A)$ by:
\bigskip
\[ 
R_A(\lambda)=
(\lambda\cdot E_n-A)^{-1}\quad\colon\quad
\lambda\in{\bf{C}}\setminus\sigma(A) \tag{**}
\]


\noindent
One refers to $R_A(\lambda)$ as the resolvent of $A$.
The map
\[ 
\lambda\mapsto R_A(\lambda)
\] 
yields a matrix-valued analytic function defined in
${\bf{C}}\setminus \sigma(A)$.
To see this we take some
$\lambda_*\in {\bf{C}}\setminus \sigma(A)$ and set
\[
R_*=(\lambda_*\cdot E_n-A)^{-1}
\]
Since
$R_*$ is a 2-sided inverse we have
the equality
\[
E_n=R_*(\lambda_*\cdot E_n-A)=
(\lambda_*\cdot E_n-A)\cdot R_*\implies
R_*A=AR_*
\] 
Hence the resolvent $R_*$ commutes with $A$.
Next,
construct the matrix-valued power series
\[
\sum_{\nu=1}^\infty (-1)^\nu\cdot \zeta^\nu\cdot (R_*A)^\nu\tag{1}
\]
which is convergent when $|\zeta|$ are small enough.
\medskip


\noindent
{\bf{2.1 Exercise.}}
Prove  the equality
\[
R_A(\lambda_*+\zeta)=R_*+\sum_{\nu=1}^\infty
(-1)^\nu\cdot \zeta^\nu\cdot R_*\cdot (R_*A)^\nu
\]
The local series expansion () above therefore  shows that
the resolvents yield a matrix-valued analytic function
in ${\bf{C}}\setminus\sigma(A)$.

\medskip

\medskip

\noindent
We are going to use 
analytic function theory 
to establish results which after can be extended
to
an operational calculus for  linear operators on infinite dimensional 
vector spaces.
The  analytic constructions are     also useful to investigate
dependence  upon parameters. Here is 
an example.
Let
$A$ be an $n\times n$-matrix whose
characteristic polynomial $P_A(\lambda)$ has $n$ simple roots
$\alpha_1,\ldots,\alpha_n$. When
$\lambda$ is outside the spectrum $\sigma(A)$.
residue calculus  gives
the following   expression for
the resolvents:
\[
(\lambda\cdot E_n-A)^{-1}=
\sum_{k=1}^{k=n}
 \frac{1}{\lambda-\alpha_k}\cdot \mathcal C_k(A)\tag{*}
 \]
where each matrix $\mathcal C_k(A)$ is a polynomial in $A$ given by:
\[
 \mathcal C_k(A)=
 \frac{1}{\prod_{\nu\neq k}\, (\alpha_k-\alpha_\nu)}
\cdot\prod_{\nu\neq k}\,(A-\alpha_\nu E_n)
\]
The  formula (*)   goes back to work
by
Sylvester, Hamilton and Cayley.
The resolvent $R_A(\lambda)$
is also used to construct  the Cayley-Hamilton polynomial of $A$
which 
by definition this is the unique monic polynomial $P\uuu *(\lambda]$ in the 
polynomial ring ${\bf{C}}[\lambda]$
of smallest possible degree such that the associated
matrix
$p_*(A)=0$.
It is found as follows:
Let $\alpha_1,\ldots,\alpha_k$
be the distinct roots of $P_A(\lambda)$
so that
\[
P_A(\lambda)=
\prod_{\nu=1}^{\nu=k}\, 
(\lambda-\alpha_\nu)^{e_\nu}
\]
where $e_1+\ldots+e_k=n$.
Now the meromorphic and matrix-valued resolvent
$R_A(\lambda)$
has  poles at $\alpha_1,\ldots,\alpha_k$. If
the order of a pole at root $\alpha_j$ is denoted by
$\rho_j$ one has the inequality
$\rho_j\leq e(\alpha_j)$
which in general can be strict. The Cayley\vvv Hamilton polynomial
becomes:
\[
P_*(\lambda)=\prod_{\nu=1}^{\nu=k}\, 
(\lambda-\alpha_\nu)^{\rho_\nu}\tag{**}
\]
\medskip




\noindent
Now we begin to prove results in more detail.
To begin with one has the Neumann series expansion:

\medskip

\noindent
{\bf{Exercise.}}
Show that if $|\lambda|$ is 
strictly larger than the absolute values of the roots
of $P_A(\lambda)$, then the resolvent is given by the  series
\[ 
R_A(\lambda)=\frac{E_n}{\lambda}+ \sum_{\nu=1}^\infty
\,\lambda^{-\nu-1}\cdot A^\nu\tag{*}
\]

\noindent
{\bf{A differential equation.}}
Taking the complex derivative of $\lambda\cdot R_A(\lambda)$ 
in (*) we get
\[ \frac{d}{d\lambda}(\lambda R_A(\lambda))
=-\sum_{\nu=1}^\infty\,\nu\cdot\lambda^{-\nu-1}\cdot A^\nu\tag{1}
\]


\noindent
{\bf{Exercise.}}
Use (1) to prove that
if $|\lambda|$ is large then
$R_A(\lambda)$ satisfies the differential equation:
\[ 
\frac{d}{d\lambda}(\lambda R_A(\lambda))
+A[\lambda^2R_A(\lambda)-E_n-\lambda A]=0\tag{2}
\]


\noindent
Now
(2) and  the analyticity of the resolvent 
outside the spectrum of $A$ give:


\medskip

\noindent
{\bf 2.3 Theorem} \emph{Outside the spectrum
$\sigma(A)$
$R(\lambda)$
satisfies the differential equation}
\[
\lambda\cdot R_A'(\lambda)+R_A(\lambda)+\lambda^2\cdot
A\cdot R_A(\lambda)=
A+\lambda\cdot A^2
\]

\bigskip

\noindent
{\bf{2.4 Residue formulas.}}
Since the resolvent is analytic we can construct complex line integrals and
apply results in complex residue calculus.
Start from the Neumann series (*) above 
and  perform integrals over circles 
$|\lambda|=w$ where $w$ is large.
\medskip

\noindent
{\bf{2.5 Exercise.}}
Show that when $w$ is strictly 
larger than
the absolute value of every
root of
$P_A(\lambda)$ then
\[ A^k=\frac{1}{2\pi i}\int_{|\lambda|=w}\,
\lambda^k\cdot R_A(\lambda)\cdot d\lambda
\quad\colon\quad k=1,2,\ldots
\]
It follows that when $Q(\lambda)$ is an arbitrary polynomial then
\[ Q(A)=\frac{1}{2\pi i}\int_{|\lambda|=w}\,
Q(\lambda)\cdot R_A(\lambda)\cdot d\lambda\tag{*}
\]
In particular we take the identity $Q(\lambda)=1$ and obtain

\[ E_n=\frac{1}{2\pi i}\cdot\int_{|\lambda|=w}\,
R_A(\lambda)\cdot d\lambda\tag{**}
\]
Finally, show  that if $Q(\lambda)$ is a  polynomial
which has a zero of order
$\geq e(\alpha_\nu)$ at every root then
\[ 
Q(A)=0\tag{***}
\]

\bigskip

\noindent
{\bf{2.6 Residue matrices.}}
Let $\alpha_1,\ldots,\alpha_k$ be the distinct zeros
of $P_A(\lambda)$.
For a given root, say $\alpha_ 1$ of  multiplicity
$p\geq 1$ we have a local Laurent series expansion
\[
R_A(\alpha_1+\zeta)=
\frac{G_p}{\zeta^p}+\ldots+\frac{G_1}{\zeta}+
B_0+\zeta\cdot B_1+\ldots\tag{i}
\]
We refer to $G_1,\ldots,G_p$ as the residue matrices at $\alpha_1$.
Choose a polynomial $Q(\lambda)$ in ${\bf{C}}[\lambda]$
which vanishes up to the multiplicity at all the
the remaining roots
$\alpha_2,\ldots,\alpha_k$ while it has a zero of order $p-1$ at $\alpha_1$, i.e.
locally
\[
Q(\alpha_1+\zeta)= \zeta^{p-1}(1+q_1\zeta+\ldots)\tag{i}
\]

\noindent
{\bf{2.7 Exercise.}}
Use residue calculus and  (*) from Exercise 2.5 to show that:
\[ 
Q(A)=
\frac{1}{2\pi}\int_{|\lambda-\alpha_1|=\epsilon}\,
Q(\lambda)\cdot R_A(\lambda)\cdot d\lambda=
G_p\tag{*}
\]
Hence the matrix $G_p$ is a polynomial of $A$. In a similar way one proves that
every $G$-matrix in the Laurent series (i) is a polynomial in $A$.
\medskip


\noindent
{\bf{2.7 Some idempotent matrices.}}
Consider  a zero $\alpha_j$ and choose a polynomial
$Q_j$ such that
$Q_j(\lambda)-1$ has a zero of order $e(\alpha_j)$ at
$\alpha_j$ while $Q_j$ has a zero of order
$e(\alpha_\nu)$ at the remaining roots.
Set
\[ 
E_A(\alpha_j)=\frac{1}{2\pi i}\int_{|\lambda|=w}\,
Q_j(\lambda)\cdot R_A(\lambda)\cdot d\lambda\tag{1}
\]
where $w$ is large as in 2.5.
Since 
the polynomial 
$S=Q_j-Q_j^2$ vanishes up to the multiplicities
at all the roots of $P_A(\lambda$ we have $S(A)=0$ from (***) in 2.5
which entails that

\[
E_A(\alpha_j)=E_A(\alpha_j)\cdot E_A(\alpha_j)\tag{*}
\]
In other words, we have constructed an idempotent matrix.



 














\bigskip



\noindent
{\bf{2.8 The Cayley-Hamilton decomposition.}}
Recall the equality


\[ E_n=\frac{1}{2\pi i}\cdot\int_{|\lambda|=w}\,
R_A(\lambda)\cdot d\lambda
\]
where the radius $w$ is so large that the disc $D_w$ contains the zeros of 
$P_A(\lambda)$.
The previous construction of the $E$-matrices at
the roots of $P_A(\lambda)$  entail that
\[
 E_n=E_A(\alpha_1)+\ldots+E_A(\alpha_k)
\]


\noindent
Identifying $A$ with a ${\bf{C}}$-linear operator on
${\bf{C}}^n$ we obtain a direct sum decomposition
\[ 
{\bf{C}}^n= V_1\oplus\ldots\oplus V_k\tag{*}
\] 
where each $V_\nu$ is an $A$-invariant subspace given by 
the image of $E_A(\alpha_\nu)$.
Here $A-\alpha_\nu$ restricts to a \emph{nilpotent} linear operator on
$V_\nu$ and the dimension of this vector space is equal to the multiplicity
of the root $\alpha_\nu$ of the characteristic polynomial.
One refers to (*) as the \emph{Cayley-Hamilton decomposition} of
${\bf{C}}^n$.
\bigskip

\noindent
{\bf{2.9 The vanishing of $P_A(A)$}}.
Consider the characteristic polynomial $P_A(\lambda)$. By definition it vanishes up to the order
of multiplicity at every point in $\sigma(A)$ and hence (***) in 2.5
gives
$P_A(A)=0$.
Let us write:
\[
 P_A(\lambda)= 
 \lambda^n+c_{n-1}\lambda^{n-1}+\ldots+ c_1\lambda+c_0
\]

\noindent
Notice that
$c_0=(-1)^n\cdot\text{det}(A)$.
So if the determinant of $A$ is $\neq 0$
we get
\[ 
A\cdot [A^{n-1}+c_{n-1}A^{n-2}+\ldots+ c_1\bigr]=
(-1)^{n-1}
\text{det}(A)
\cdot E_n
\]



\noindent
Hence 
the inverse $A^{-1}$ is  expressed as a polynomial in $A$.
Concerning the equation
\[ 
P_A(A)=0
\]
it is in general not the minimal 
equation for $A$, i.e. it can occur that $A$ satisfies an equation of degree $<n$.
More precisely , if $\alpha_\nu$ is a  root of some multiplicity
$k\geq 2$ there exists a  Jordan decomposition which gives an 
integer $k_*(\alpha_\nu)$ for the largest Jordan block
attached to the nilpotent operator $A-\alpha_\nu$ on $V_{\alpha_\nu}$.
The \emph{reduced} polynomial $P_*(\lambda)$ is  the product
where the factor $(\lambda-\alpha_\nu)^{k_\nu}$ is replaced by
$(\lambda-\alpha_\nu)^{k_*(\alpha_\nu)}$
for every  $\alpha_\nu$ where $k_\nu<k_*(\alpha_\nu)$ occurs. 
Then $P_*$ is the polynomial of smallest possible degree such that
$P_*(A)=0$.
One  refers to $P_*$ as the \emph{Hamilton polynomial} attached to $A$.
This   result relies upon Jordan's result in § 3.

\medskip





\noindent
{\bf{2.10 Similarity of matrices.}}
Recall
that
the determinant of  a matrix $A$ does not  change when it is
replaced by $SAS^{-1}$
where $S$ is an arbitrary invertible matrix.
This implies  that
the coefficients of the
characterstic polynomial  $P_A(\lambda)$
are intrinsically defined via the
associated linear operator, i.e. if another basis is chosen in
${\bf{C}}^n$ the given $A$-linear operator is expressed by a matrix
$SAS^{-1}$ where $S$ effects the change of the basis.
Let us now draw an interesting consequence
of the previous operational calculus.
Let us give the following:
\medskip


\noindent {\bf 2.11 Definition.}
\emph{A pair of $n\times n$-matrices $A,B$ are
\emph{similar} if there exists some invertible matrix $S$ such that}
\[ 
B=SAS^{-1}
\]


\noindent
Since the product of two invertible matrices is invertible this yield
an equivalence relation on $M_n({\bf{C}})$ and from 2.2 above we conclude that
$P_A(\lambda)$ only depends on its equivalence class.
The question arises if  to matrices $A$ and $B$ whose characteristic 
polynomials are equal
also are similar in the sense of Definition 2.6.
This is not true in general.
More precisely,
\emph{Jordan normal form}
determines the eventual similarity between
a pair of matrices with the same characteristic polynomial.

\bigskip




\centerline {\bf\large 3.  Jordan's normal form}
\bigskip

\noindent
{\bf Introduction.}
Theorem 3.1 below is due to 
Camille Jordan. It plays
an important role when we discuss multi-valued analytic functions in punctured discs
and is also  used  in ODE-theory.
Jordan's theorem   says
that every equivalence class in $M_n({\bf{C}})$
contains a 
matrix which is built up by Jordan blocks which are defined below.
The proof employs 
the Cayley-Hamilton decomposition from 2.7.
which shows that an arbitrary $n\times n$-matrix $A$
has a similar matrix
$B=S^{-1}AS$  represented in a block form. More precisely, to every root
$\alpha_\nu$ of $P_A(\lambda)$
of some multiplicity
$e_\nu$
there occurs a square matrix $B_\nu$
of size
$e_\nu$ and $\alpha_\nu$ is the only root of
$P_{B_\nu}(\lambda)$.
It follows that for every fixed $\nu$ one has
\[
B_\nu= \alpha\cdot E_{k\uuu \nu}+
S\uuu\nu
\]
where $E\uuu {k\uuu \nu}$ is an identity matrix of size
$k\uuu\nu$ and $S\uuu\nu$ is nilpotent, i.e. there exists an integer $m$ such that
$S\uuu\nu^m=0$.
Jordan's theorem gives a further decomposition
of these nilpotent $S$-matrices.
\medskip

\noindent
{\bf{3.0 Jordan blocks.}}
An \emph{elementary} Jordan matrix of size $4$
is  matrix of the form
\[
\begin{pmatrix}
\lambda&0&0&0\\
1&\lambda&0&0\\
0&1&\lambda&0\\
0&0&1&\lambda\\
\end{pmatrix}
\]
where $\lambda$ is the eigenvalue. For $k\geq 5$ one has similar 
expressions. In general several elementary Jordan block matrices
build up a matrix which is said to be in Jordan's normal form.

\medskip

\noindent {\bf 3.1 Theorem}. \emph{For every matrix $A$ there
exists an invertible matrix $u$ such that
$UAU^{\vvv 1}$ is in Jordan's normal form.}



\bigskip

\noindent 
\emph{Proof.} By the remark after Proposition 2.12
it suffices to prove Jordan's result when $A$ has a single eigenvalue 
$\alpha$. Replacing
$A$ by $A-\alpha$
there remains only to consider the nilpotent case, i.e when
$P_A(\lambda)=\lambda^n$ so that $A^n=0$ and then
we must find a basis
where $A$ is represented in Jordan's normal form.

\bigskip



\noindent {\bf 3.2 Nilpotent operators.}
Let $S$ be a nilpotent ${\bf{C}}$-linear operator on some $n$-dimensional 
complex vector space $V$. 
So for  each non-zero vector in $v\in V$ there exists a unique integer
$m$ such that
\[ 
S^m(v)=0\quad\text{and}\quad
S^{m-1}(v)\neq 0
\]
The unique integer $m$ is denoted by
$\text{ord}(S,v)$. The case $m=1$ occurs if  
$S(v)=0$. If $m\geq 2$ the reader can check  
that the vectors $v,S(v),\ldots,S^{m-1}(v)$ are linearly independent
The vector space generated by this $m$-tuple is denoted by
$\mathcal C(v)$ and  called a \emph{cyclic} subspace of $V$
generated by $v$.
With these notations Jordan's theorem amounts to prove the following:
\bigskip

\noindent {\bf 3.3 Proposition}
\emph{Let $S$ be a nilpotent linear operator. Then $V$ is a direct sum of
cyclic subspaces.}
\medskip

\noindent\emph
{Proof.}
Set
\[ m^*=
\max_{v\in V}\, \text{ord}(S,v)
\]
Choose $v^*\in V$ such that
$\text{ord}(S,v^*)=m^*$ and
construct the quotient space
$W=\frac{V}{\mathcal C(v^*)}$ on which $S$ induces a linear operator
denoted by $\bar S$.
By  induction over  $\text{dim}(V)$ we may assume that
$W$ is a direct sum of cyclic subspaces. Hence
we can pick
a finite set of vectors $\{v_\alpha\}$ in $V$ such that if
$\{\bar v_\alpha\}$ are the images in $W$, then
\[
W=\oplus\,\mathcal C(\bar v_\alpha)\tag{1}
\]
For each
$v_\alpha$
we have  a postive  integer
\[
k_\alpha=\text{ord}(\bar S,\bar v_\alpha)
\]
The construction of a quotient space  means that
\[
S^{k\uuu\alpha}(v_\alpha)\in\mathcal C(v^*)\tag{2}
\]
Hence there exists  some
$m^*$-tuple $c_0,\ldots,c_{m-1}$ in ${\bf{C}}$
such that 

\[ 
S^{k\uuu\alpha}(v_\alpha)=c_0\cdot v^*+c_1\cdot S(v^*)+ \ldots+
c_{m^*-1}\cdot S^{m^*-1}(v^*)\tag{3}
\]
Next, put
\[
k^*\uuu\alpha= 
\text{ord}(S,v_\alpha)\tag{4}
\]
It is obvious that
$k^*\uuu\alpha\geq k\uuu \alpha$ and (3) gives
\[ 
0=S^{k^*\uuu\alpha}(v_\alpha)=
\sum\, 
c\uuu\nu\cdot
S^{k^*\uuu\alpha\vvv k\uuu\alpha+\nu}(v^*)
\]
The maximal choice of $m^*$ entails that $k^*\uuu\alpha\leq m^*$
and since the vectors
$v^*,S(v^*),\ldots S^{m^*-1}(v^*)$ are linearly independent
it follows that
\[
c\uuu 0=\ldots=c\uuu{k\uuu\alpha\vvv 1}=0\tag{5}
\]
Hence (3) enable us to find
$w\uuu\alpha\in \mathcal C(v^*)$
such that
\[
S^{k\uuu\alpha}(v\uuu \alpha)= S^{k\uuu\alpha}(w\uuu \alpha)\tag{6}
\]
The images of $v_\alpha$ and $v_alpha-w_\alpha$
are equal in $\mathcal C(v^*)$. So if
$\{v_\alpha\}$ are replaced by the vectors $\{\xi_\alpha= v_\alpha- w_\alpha\}$
one still has
\[
W=\oplus\,\mathcal C(\bar \xi_\alpha)\tag{7}
\]
Moreover, the construction of the $\xi$-vectors entail that
\[
\text{ord}(\bar S,\bar\xi_\alpha)=\text{ord}(S,v_\alpha)\tag{8}
\]
hold for each $\alpha$. At this stage
an obvious counting of dimensions give the requested
direct sum decomposition
\[
V=\mathcal C(v^*)\,\oplus \mathcal C(\xi_\alpha)
\]

\noindent
{\bf Remark.}
The proof was  bit cumbersome. The reason  is
that the direct sum decomposition  
in Jordan's Theorem is not unique.
Only the   individual \emph{dimensions}
of the cyclic subspaces which appear in a direct sum decomposition are unique.
It is instructive to perform Jordan decompositions 
of specific matrices using
an implemented program which for example
can be found in
\emph{Mathematica}.


\newpage

\centerline{\bf {4. Hermitian and Normal operators.}}

\bigskip

\noindent
The $n$-dimensional vector space
${\bf{C}}^n$ is equipped with
the hermitian inner product:
\[ 
\langle x,y\rangle= x_1\bar y_1+\ldots+x_n\bar y_n
\]
A basis $e_1,\ldots,e_n$ is orthonormal if
$\langle e_i,e_k\rangle=\text{Kronecker's delta function}$.
A linear operator
$U$ is  unitary if
it preserves the inner product:
\[
\langle U(x),U(y)\rangle=
\langle x,y\rangle
\]
for all $x$ and $y$.
It is clear that a unitary operator $U$ 
sends an orthonormal basis to another 
orthonormal basis and the reader may verify
that a linear operator $U$ is unitary if and only if
\[
U^{-1}=U^*
\]

\medskip

\noindent
{\bf{4.0.1 Adjoint operators.}}
Let $A$ be a linear operator. Its adjoint $A^*$ is the linear operator for which
\[
\langle A(x),y\rangle=
\langle x,A^*(y)\rangle
\] 

\noindent
{\bf{4.0.2 Exercise.}}
Show that if  $e_1,\ldots,e_n$ is an arbitrary
orthonormal basis in the inner product space
${\bf{C}}^n$
where $ A$ is represented by a matrix with elements
$\{a_{p,q}\}$, then $A^*$ is represented by the matrix whose elements are
\[
 a^*_{pq}=\bar a_{qp}
\]

\medskip

\noindent
{\bf{4.0.3 Hermitian operators.}}
A linear operator $A$ is called Hermitian if
\[
\langle A(x),y\rangle=
\langle x,A(y)\rangle
\]
holds for all $x$ and $y$.
An equivalent condition is that $A$ is equal to its adjoint $A^*$. 
Therefore one also
refers to a self-adjoint operator, i.e the notion of a hermitian respectively
self-adjoint  matrix  is the same.
\medskip


\noindent
{\bf{4.0.4 Self-adjoint projections.}}
Let $V$ be a subspace of ${\bf{C}}^n$ of some dimension
$1\leq k\leq n-1$.
Its orthogonal complement is denoted by $V^\perp$ and we have 
the direct sum decomposition
\[
{\bf{C}}^n=V\oplus V^\perp
\]
To $V$ we associate the linear operator $E$ whose kernel is
$V^\perp$ while it restricts to the identity on $V$.
Here
\[
E=E^2\quad\text{and}\quad E=E^*
\]
One refers to $E$ as a self-adjoint projection.
\medskip


\noindent
{\bf{4.0.5 Exercise.}}
Show  that if $E$ is some $n\times n$-matrix
which is idempotent in $M_n({\bf{C}})$ and Hermitian
in the sense of 4.0.3 then $E$
is the self-adjoint projection attached to the subspace
$V=E({\bf{C}}^n)$.
\medskip


\noindent
{\bf{4.0.6 Orthonormal bases.}}
Let $V_1\subset V_2\subset \ldots V_n={\bf{C}}^n$
be a strictly increasing sequence of subspaces. So here each $V_k$
has dimension $k$.
The \emph{Gram-Schmidt orthogonalisation}
yields  an orthonormal basis $\xi_1,\ldots,\xi_n$
such that
\[
V_k={\bf{C}}\cdot\xi_1+\ldots+
{\bf{C}}\cdot\xi_k
\] 
hold for every $k$.
The verification of this wellknown construction is left to the reader.
Next, if $A$ is an arbitrary $n\times n$-matrix the fundamental theorem of algebra implies
that there exists a sequence
$\{V_k\}$ as above such that
every $V_k$ is $A$-invariant, i.e.
\[
 A(V_k)\subset V_k
\] 
hold for each $k$.
We find the orthonormal basis $\{\xi_k\}$
and construct the unitary operator $U$ which sends the standard basis in
${\bf{C}}^n$ onto this $\xi$-basis.
In this $\xi$-basis we see  that the linear operator $A$ is represented by an upper
triangular matrix. Hence we have

\medskip

\noindent
{\bf{4.0.7 Theorem.}}
\emph{For every $n\times n$-matrix $A$ there exists a unitary matrix
$U$ such that
$U^*AU$ is upper triangular.}


\newpage








\centerline {\bf{4.1 The spectral theorem.}}
\medskip

\noindent
This important result asserts the following:

\medskip

\noindent
{\bf{Theorem.}}
\emph{If $A$ is Hermitian  there exists
an orthonormal basis $e_1,\dots,e_n$ 
in ${\bf{C}}^n$ where each $e_k$ is an eigenvector 
to $A$ whose eigenvalue is a real number. Thus,
$A$ can be diagonalised in an
orthonormal basis and  expressed  by matrices this means that 
there exists
a unitary matrix $U$ such that}
\[ 
U^*AU= S\tag{*}
\] 
\emph{where $S$ is a diagonal  matrix and  every $s_{ii}$
is a real number. In particular the roots of the characteristic polynomial
$\text{det}(P_A(\lambda))$ are all real.}

\medskip

\noindent
\emph{Proof.}
Since $A$ is self-adjoint we have
a real-valued function on ${\bf{C}}^n$ defined by
\[ 
x\mapsto \langle Ax,x\rangle\tag{1}
\]
Let $m^*$ be the  maximum 
of (1) as $x$ varies over the compact unit sphere of unit vectors in
${\bf{C}}^n$.
The maximum is attained by some
complex vector $x_*$ of unit length. 
Suppose $y$ is a unit vector where
 that $y\perp x_*$ and let $\lambda$ be a complex number.
Since $A$ is self-adjoint we have:
\[
 \langle A(x_*+\lambda y),x_*+\lambda y\rangle=
 m^*+2\cdot\mathfrak{Re}
 \bigl(\lambda\cdot
\langle Ax_*,y\rangle\bigr)+|\lambda|^2\cdot
\langle Ay,y\rangle\tag{2}
\]
Now $x+\lambda y$ has norm $\sqrt{1+\lambda|^2}$ 
 and the maximality gives:
 \[
 m^*+2\cdot\mathfrak{Re}
 \bigl(\lambda\cdot
\langle Ax_*,y\rangle\bigr)+|\lambda|^2\cdot
\langle Ay,y\rangle\leq\sqrt{1+|\lambda|^2}\cdot m^*\tag{3}
\]
Suppose now that
$\langle Ax_*,y\rangle\neq 0$
and set
\[
\langle Ax_*,y\rangle= s\cdot e^{i\theta}\quad\colon\quad   s>0
\]
With $\delta>0$ we take
 $\lambda= \delta\cdot e^{-i\theta}$ and (3) entails
that
\[
2s\cdot\delta\leq (\sqrt{1+\delta^2}-1)\cdot m^*-\langle Ay,y\rangle\cdot\delta^2\tag{4}
\]
Next, by calculus one has
$2\cdot \sqrt{1+\delta^2}-1)\leq\delta^2$ so after division with $\delta$
we get
\[
2s\leq\delta\cdot\bigl(\frac{m_*}{2}- \langle Ay,y\rangle\bigr)\tag{5}
\]
But this is impossible for arbitrary small $\delta$
and hence we have proved that
\[ 
y\perp x_*\implies \langle Ax_*,y\rangle=0\tag{6}
\]

\noindent
This means that  $x_*^\perp$
is an invariant subspace for $A$
and the restricted operator remains self-adjoint. At this stage
the reader can finish the proof
to get a unitary matrix $U$ such that (*) holds.



\bigskip


\centerline {\bf{4.2 Normal operators.}}
\medskip

\noindent
An $n\times n$-matrix
$A$ is  normal if it commutes with its adjoint, i.e. 
\[ 
A^*A=AA^*\quad\text{holds in}\quad M_n({\bf{C}})\tag{*}
\]

\medskip

\noindent
{\bf{4.2.0 Exercise.}}
Let $A$ be a normal matrix. Show that every
equivalent
matrix is normal, i.e. if $S$ is invertible then
$SAS^{-1}$ is also normal. The hint is to use that
\[ 
(S^{-1})^*=(S^*)^{-1}
\] 
holds
for every invertible matrix.
Conclude from this that we can refer to normal linear operators
on ${\bf{C}}^n$.
\medskip

\noindent
{\bf{4.2.1 Exercise.}}
Let $A$ and $B$ be two Hermitian matrices which commute, 
i.e. $AB=BA$. Show that the matrix 
$A+iB$ is normal.





\medskip

\noindent
Next, let $R$ be normal and assume that
the its characteristic polynomial has simple roots.
This means that there exists  a basis $\xi_1,\ldots,\xi_n$
formed by eigenvectors to $R$ with eigenvalues
$\lambda_1,\ldots,\lambda_n$.
Thus:
\[ 
R(\xi_\nu)=\lambda_\nu\cdot \xi_\nu\quad\colon\quad 1\leq\nu\leq n\tag{*}
\] 
Notice that $R$ is invertible if and only if
al the eigenvalues are $\neq 0$. It turns out that the normality
gives a stronger conclusion.

\medskip

\noindent
{\bf{4.3 Proposition.}} \emph{Assume that  the eigenvalues
are $\neq 0$. Then
the $\xi$-vectors in (*) are orthogonal.}
\medskip

\noindent
\emph{Proof.}
Consider some  eigenvector, say $\xi_1$.
Now we get
\[ 
R(R^*(\xi_1))=
R^*(R(\xi_1))=\lambda_1\cdot 
R^*(\xi_1)\tag{i}
\]
Hence $R^*(\xi_1)$ is an eigenvector to $R$ with eigenvalue $\lambda_1$. By 
hypothesis this eigenspace is 1-dimensional which gives
\[ 
R^*(\xi_1)=\mu\cdot \xi_1\implies
\] 
\[
\lambda_1\cdot \langle \xi_1,\xi_1\rangle=
\langle R(\xi_1),\xi_1\rangle=
\langle \xi_1),R^*(\xi_1)\rangle=\bar\mu\cdot
\langle \xi_1,\xi_1\rangle
\]


\noindent
Hence  $\mu=\bar\lambda_1$ which shows that
the eigenvalues of $R^*$ are the complex conjugates of
the eigenvalues  of $R$. There remains to show that
the $\xi$-vectors are orthogonal.
Consider two eigenvectors, say $\xi_1,\xi_2$. Then
we obtain:
\[ 
\bar\lambda_2\lambda_1\cdot\langle \xi_1,\xi_2\rangle=
\langle R\xi_1,R\xi_2\rangle
=\langle \xi_1,R^*R\xi_2\rangle
\langle \xi_1,RR^*\xi_2\rangle=
\]
\[
\langle R^*\xi_1,R^*\xi_2\rangle=\bar\lambda_1\cdot \lambda_2\cdot
\langle \xi_1,\xi_2\rangle\implies
(\bar\lambda_2\lambda_1-\lambda_2\bar\lambda_1)\cdot\langle \xi_1,\xi_2\rangle=0\tag{ii}
\]

\noindent
By assumption   $\lambda_1\neq\lambda_2$ and both are $\neq 0$. It follows
that 
$\bar\lambda_2\lambda_1-\lambda_2\bar\lambda_1\neq 0$ and then (ii) gives
$\langle \xi_1,\xi_2\rangle=0$ as required.
\medskip

\noindent
{\bf{4.4 Remark.}}
Proposition 4.3  shows that if $R$ is an invertible normal operator with
$n$ distinct eigenvalues then
there exists a unitary matrix
$U$ such that
$U^*RU$ is a diagonal matrix. But in contrast to the Hermitian
case the eigenvalues can be  complex.
\medskip


\noindent
{\bf{4.5 Exercise.}}
Let $R$ as above be an invertible normal operator
with distinct eigenvalues.
Show that $R$ is a Hermitian matrix if and only if
the eigenvalues are real numbers.

\medskip


\noindent
{\bf{4.6 Theorem.}} \emph{Let $R$  be an invertible normal operator
with distinct eigenvalues. Then there exists a unique pair of 
Hermitian operators $A,B$ such that
$AB=BA$ and}
\[
 R=A+iB
\]


\noindent{\bf{4.7 Exercise.}} Prove Theorem 4.6.

\medskip

\noindent
{\bf{4.8 The operator $R^*R$}}.
Let $R$ as above be  an invertible  normal operator
with eigenvalues $\lambda_1,\ldots,\lambda_n$.
From Remark 4.4 it is clear that
$R^*R$ is a Hermitian operator whose eigenvalues all are
given by the positive numbers
$\{|\lambda_\nu|^2\}$ and if $A.B$ are the Hermitian operators in
Theorem 4.6 then we have
\[ 
R^*R= A^2+B^2
\] 
Thus, $R^*R$ is represented as a sum of squares of two pairwise commuting
Hermitian operators.
\medskip

\noindent{\bf{4.9 The normal operator
$(A+iE_n)^{-1}$}}.
Let $A$ be a arbitrary  Hermitian $n\times n$-matrix. 
We have already seen that
its eigenvalues are real . Let us denote them
by $r_1,\ldots,r_n$. The spectral theorem gives  
a unitary matrix $U$ such that
$U^*AU$ is diagonal
with elements $\{r_\nu\}$. It follows that the matrix
$A+iE_n$ is invertible and its inverse
\[
R=
(A+iE_N)^{-1}
\]
is a normal operator with eigenvalues $\{\frac{1}{r_\nu+i}\}$.

\bigskip

\centerline {\bf{4.10 The case of multiple roots}}
\medskip

\noindent
The assumption that the eigenvalues of a normal operator are all distinct can be 
relaxed. Thus, for every normal and invertible operator $R$ there  exists
a unitary operator $U$ such that $U^*RU$ is diagonal.


\medskip

\noindent{\bf{4.11 Exercise.}}
Prove the assertion above.
The hint is to establish the following
which has independent interest:
\medskip

\noindent {\bf{4.12 Proposition.}}
\emph{Let  $R$ be  normal and nilpotent. Then $R=0$}
\medskip

\noindent
\emph{Proof.}
By Jordan's Theorem it suffices to prove this when
$R$ is a single Jordan block
represented by a special $S$-matrix whose elements
below the diagonal, are 1 while all the other elements are zero.
If $n=2$ we have for example
\[
S=
\begin{matrix}
0&0\\1&0\end{matrix}\implies S^*=
\begin{matrix}
0&1\\0&0\end{matrix}
\]
The reader verifies that $S^*S\neq SS^*$ and a similar calculation gives
Proposition 4.12 for every $n\geq 3$.

\medskip

\noindent
{\bf{4.13 Remark.}} The  result above  means that if $R$ is normal
then there never   
appear Jordan blocks of size $>1$
and hence there exists an invertible matrix $S$ such that
$SRS^{-1}$ is diagonal.




\bigskip

\centerline{\bf {5. Fundamental solutions to ODE:s.}}

\bigskip

\noindent Recall from
Calculus that every ordinary
differential equation can be expressed as a system of first
order equations. The fundamental issue is therefore to consider
a matrix valued function $A(t)$,  i.e. an $n\times n$-matrix whose
elements $\{a_{ik}(t)\}$ are functions of $t$.
Given $A(t)$ there exists  at least locally close to $t=0$, a unique
$n\times n$-matrix $\Phi(t)$ such that
\[ 
\frac{d\Phi}{dt}=A(t)\cdot \Phi(t)
\]
with the initial condition  $\Phi(0)=E_n$.
One refers to $\Phi$ as a fundamental solution.
The columns of the $\Phi$-matrix give
solutions to the homogenous system defined by $A(t)$.
Moreover, the determinant of
$\Phi(t)$ is $\neq 0$ for every $t$.
In fact his follows from the equality (*) below:
\medskip

\noindent
{\bf{Exercise.}}
The trace function of $A$ is defined by:
\[ 
\text{Tr}(A)(t)= a_{11}(t)+\ldots+a_{nn}(t)
\]
Show that  the function 
$t\mapsto \text{det}(\,\Phi(t))$ satisfies the ODE.equation
\[
\frac{d}{dt}( \text{det}\,\Phi(t))=
\text{det}\,\Phi(t)\cdot 
\text{Tr}(A)(t)
\]
Hence we have the formula
\[ 
\text{det}\,\Phi(t)=e^{\int_0^t\, \text{Tr}(A)(s)\cdot ds}\quad\colon t\geq 0\tag{*}
\]
For example, if the trace function  is identically zero then
$\text{det}\,\Phi(t)=1$ for all $t$.
\bigskip


\noindent
{\bf{5.1 Inhomogeneous equations.}}
From (*) it follows that the matrix $\Phi(t)$ is invertible for all $t$.
This gives a formula to solve
a inhomogeneous
equation:
\[
\frac{d{\bf{x}}}{dt}=
A(t)({\bf{x}}(t))+{\bf{u}}(t)\tag{1}
\] 
Here
${\bf{u}}(t)=(u_1(t),\ldots,u_n(t)$
is a given vector-valued function and one seeks
a vector-valued function ${\bf{x}}(t)=(x_1(t),\ldots,x_n(t)$
such that (1) holds and in addition
satisfies the initial condition:
\[
{\bf{x}}(0)={\bf{b}}\quad\text{where} \,\,\,
{\bf{b}}\,\,\,\text{is some vector }\tag{2}
\] 


\noindent
{\bf{Exercise.}}
Show that the unique solution to (1) is given by
\[ 
{\bf{x}}(t)=\Phi(t)({\bf{b}})+
\Phi(t)\bigr(\int_0^t\, \Phi^{-1}(s)\bigl({\bf{u}}(s)\bigr)\cdot ds\bigl)\tag{**}
\]
\medskip

\noindent
In other words, for every $t$ we first evaluate
the matrix $\Phi(t)$ on the $n$-vector ${\bf{b}}$
which gives the first time dependent vector in the right
hand side.
In the second term the inverse matrix
$\Phi^{-1}(s)$ is applied to ${\bf{u}}(s)$ for every
$0\leq s\leq t$. After integration over
$[0,t]$ we get a time-dependent  $n$-vector on which
$\Phi(t)$ is applied.









 
 
 


\newpage

\centerline{\bf\large{6. Carleman's inequality}}
\medskip

\noindent

\noindent
{\bf{Introduction }} Theorem 6.1 below was proved by
Carleman in the article
\emph{Sur le genre du denominateur $D(\lambda)$
de Fredholm}
from 1917. At that time the result was used 
to study non-singular integral equations of the Fredholm type.
For more recent applications of Theorem 6.1
we refer to Chapter XI
in [Dunford-Schwartz]. 
\medskip

\noindent
{\bf{The Hilbert-Schmidt norm. }} It is defined 
for an  
$n\times n$-matrix $A=\{a_{ik}\}$  by:
\[
||A||= \sqrt{ \sum\sum\, |a_{ik}|^2}
\]


\noindent
where the double sum extends over all pairs
$1\leq i,k\leq n$.
Notice that this norm is the same as
\[
||A||^2= \sum_{i=1}^{i=n}\, ||A(e_i)||^2
\] 
where $e_1,\ldots,e_n$ can be taken as an arbitrary orthogonal basis in
${\bf{C}}^n$.
Next, 
for a linear operator $S$
on ${\bf{C}}^n$
its \emph{operator norm} is defined by
\[
\text{Norm}[S]=
\max_x\, ||S(x)||\quad\text{with the maximum taken over unit vectors.}
\] 

\noindent
{\bf{6.1 Theorem.}}
\emph{Let $\lambda_1,\ldots,\lambda_n$ be the roots of 
$P_A(\lambda)$ and  $\lambda\neq 0$ is outside $\sigma(A)$. Then
one has the inequality:}
\[ 
\bigl|\prod_{i=1}^{i=n}\, 
\bigl[1-\frac{\lambda_i}{\lambda}\bigr ]
e^{\lambda_i/\lambda}\bigr|\cdot \text{Norm}\bigl[R_A(\lambda)\bigr]
\leq |\lambda|\cdot \text{exp}
\bigl(\frac{1}{2}+ \frac{||A||^2}{2\cdot |\lambda|^2}\bigr)
\]
\medskip

\noindent
The proof requires some preliminary results.
First we  need  inequality due to  Hadamard which
goes as follows:
\medskip

\noindent
{\bf {6.2 Hadamard's inequality.}}
\emph{For every matrix $A$ with a non-zero determinant one has the inequality}
\[
\bigl|\text{det}(A)\bigr|\cdot \text{Norm}(A^{-1})\leq
\frac{||A||^{n-1}}{(n-1)^{n-1)/2}}
\]


\noindent
{\bf{Exercise.}} Prove this  result. 
The hint is to  use
expansions of certain determinants while one
considers 
$\text {det}(A)\cdot \langle A^{-1}(x),y\rangle$ 
for all pairs of unit vectors $x$ and $y$.

\bigskip

\noindent
{\bf{6.3 Traceless matrices.}}
Let $A$ be an  $n\times n$-matrix. The trace is by definition given by:
\[
\text{Tr}(A)= b_{11}+\ldots+b_{nn}\tag{i}
\]
Recall  that 
$-\text{Tr}(A)$ is
equal to the sum of the roots of $P_A(\lambda)$.
In particular the trace of two equivalent matrices are equal.
This will be used to prove the following:
\medskip

\noindent
{\bf{6.4 Theorem.}}
\emph{Let $A$ be an $n\times n$-matrix whose trace is zero. Then there
exists a unitary matrix
$U$ such that  the diagonal elements of $U^*AU$ all are zero.}
\medskip


\noindent
\emph{Proof}. Consider
first consider the case $n=2$. By Theorem 4.0.7
it suffices to consider the case when the $2\times 2$-matrix $A$
is upper diagonal and since the trace is zero it has the form
\[ 
A=
\bigl(\,\begin{matrix} a&b\\0&-a
\end{matrix}\,\bigr)
\]
where $a,b$ is a pair of complex numbers.
If $a=0$ then the two diagonal elements are zero and wee can take $U=E_2$ to be the identity in Lemma 6.5. If $a\neq 0$ we consider a vector $\phi=(1,z)$ in ${\bf{C}}^2$.
Then $A(\phi)$ is the vector $(a+bz,-az)$ and hence the inner product becomes:
\[
\langle A(\phi),\phi\rangle=a+bz-a|z|^2\tag{i}
\]
We can write
\[
\frac{b}{a}= re^{i\theta}
\]
where $r>0$ and then (i) is zero if
\[
|z|^2=1+se^{i\theta}\cdot z\tag{ii}
\]
With $z=se^{-i\theta}$
it amounts to find a positive real number $s$ such that
$s^2=1+s$ which clearly exists.
Now we get the vector
\[ 
\phi_*=\frac{1}{1+s^2}(1,se^{-i\theta})
\]
which has unit length and 
\[
\langle A(\phi_*),\phi_*\rangle=0\tag{ii}
\]
By 4.0.6 we find another unit vector $\psi_*$ so that
$\phi_*,\psi_*$ is an orthonormal base in
${\bf{C}}^2$ and hence there exists a unitary matrix
$U$ such that $U(e_1)=\phi_*$ and $U(e_2)= \psi_*$.
If $B=U^*AB$ the vanishing in (ii) gives $b_{11}=0$. At the same time
the trace is unchanged, i.e. $\text{tr}(B)=0$ holds and hence
we also get
$b_{22}=0$. This means  that  the diagonal elements of $U^*AU$ 
are both zero as required.
\medskip

\noindent{\bf{The case $n\geq 3$}}.
For the
induction
the following is needed:
\medskip

\noindent
\emph{Sublemma.} \emph{Let $n\geq 3$
and assume as above that
$\text{Tr}(A)=0$. Then there exists some
non-zero vector $\phi\in{\bf{C}}^n$ such that}
\[
\langle A(\phi),\phi\rangle=0\tag{*}
\]

\noindent
\emph{Proof.}
If (*) does not hold we
get the positive number
\[
m_*=\min_\phi\, \bigl|\langle A(\phi),\phi\rangle\bigr|
\]
where the minimum is taken over unit vectors in
${\bf{C}}^n$.
The minimum is achieved by some unit vector $\phi_*$. Let
$\phi_*^\perp$ be its orthonormal complement
and $E$ the self-adjoint projection from
${\bf{C}}^n$ onto $\phi_*^\perp$.
On the $(n-1)$-dimensional inner product space
$\phi_*^\perp$ we get the linear operator
$B=EA$, i.e. 
\[ 
B(\xi)= E(A(\xi))\quad\colon\quad \xi\in \phi_*^\perp\tag{i}
\]
If $\psi_1,\ldots,\psi_{n-1}$ is an orthonormal basis in
$\phi_*^\perp$ then the $n$-tuple $\phi_*,\psi_1,\ldots,\psi_{n-1}$
is an orthonormal basis in ${\bf{C}}^n$ and since the trace of $A$ is zero we get
\[
0=\langle A(\phi_*),\phi_*\rangle+
\sum_{\nu=1}^{\nu=n-1}\, \langle A(\psi_\nu),\psi_\nu\rangle=
m+\sum_{\nu=1}^{\nu=n-1}\, \langle B(\psi_\nu),\psi_\nu\rangle\tag{ii}
\]
where we used that $E(\psi_\nu)=\psi_\nu$ for each $\nu$
and that $E$ is self-adjoint so that

\[
\langle A(\psi_\nu),\psi_\nu\rangle=
\langle A(\psi_\nu),E(\psi_\nu)\rangle=\langle E(A(\psi_\nu)),\psi_\nu\rangle
=\langle B(\psi_\nu),\psi_\nu\rangle
\]
Now (ii)  gives
\[ 
\text{Tr}(B)=-m
\]
Hence the $(n-1)\times(n-1)$-matrix which represents
$B+\frac{m}{n-1}\cdot E$
has trace zero. By an induction over $n$ we find a unit vector
$\psi\in \phi_*^\perp$
such that
\[
\langle B(\psi_*),\psi_*\rangle=-\frac{m}{n-1}
\]
Finally, since $E$ is self-adjoint we have already seen that
\[
\langle A(\psi_*),\psi_*\rangle=\langle B(\psi_*),\psi_*\rangle\implies
\bigl|\langle A(\psi_*),\psi_*\rangle\bigr |=\bigl|\frac{m}{n-1}\bigr |=
\frac{m_*}{n-1}
\]
Since $n\geq 3$ the last number is $<m_*$ which contradicts the minimal choice 
of $m_*$.
Hence we must have $m_*=0$ which proves lemma 6.5
\bigskip

\noindent
{\emph{Final part of the proof.}
Let $n\geq 3$. The Sublemma  gives unit vector $\phi$
such that
$\langle A(\phi),\phi\rangle=0$.
Consider the hyperplane
$\phi^\perp$ and the operator $B$ from the Sublemma  which now has trace
zero on this $(n-1)$-dimensional space. So by an induction over
$n$
there exists an orthonormal basis $\psi_1,\ldots,\psi_{n-1}$ in
$\phi^\perp$ such that
$\langle B(\psi_\nu),\psi_nu\rangle=0$ for every $\nu$.
Now $\phi,\psi_1,\ldots,\psi_{n-1}$
is an orthonormal basis in ${\bf{C}}^n$ and if $U$
is the unitary matrix which has this $n$-tuple as column vectors
it follows that the diagonal elements of $U^*AU$ all vanish.
This finishes the proof of Theorem 6.4.


 
\newpage

\centerline{\bf{Proof Theorem 6.1}}.



\noindent
Set $B=\lambda^{-1}A$ so that $\sigma(B)=\{ \lambda_i/\lambda\}$ and
$\text{Tr}(B)=\sum\,\frac{\lambda_i}{\lambda}$.
We also have 

\[
||B||^2=\frac{||A||^2||}{|\lambda|^2}\quad\text{and}\quad
\bigl |\lambda\bigr |\cdot \text{Norm}[R_A(\lambda)]=\text{Norm}[(E-B)^{-1}]
\]


\noindent Hence Theorem 6.1 follows if we prove the inequality
\[
\bigl |e^{\text{Tr}(B)}\bigr|\cdot
\bigl|\prod_{i=1}^{i=n}\, 
\bigl[1-\frac{\lambda}{\lambda_i}\bigr ]
\cdot \text{Norm}
\bigl [E-B)^{-1}\bigr]
\leq \text{exp}\bigl[\frac{1+ ||B||^2}{2}\bigr]\tag{*}
\]


\noindent
To prove (*) we choose an arbitrary integer $N$ such that
$N>\bigl |\text{Tr}(B)\bigr|$ and for each such $N$ we define the linear operator
$B_N$ on the $n+N$-dimensional complex space with points 
denoted by $(x,y)$ with  $y\in{\bf{C}}^N$
as follows:
\[
B_N(x,y)= (Bx\, , \, -\frac{\text{Tr}(B)}{N}\cdot y)\tag{**}
\]


\noindent
The
eigenvalues of the linear operator $E-B_N$ is the union of the $n$-tuple 
$\{1-\frac{\lambda_i}{\lambda}\}$ and 
the $N$-tuple of equal eigenvalues given by 
$1+\frac{\text{Tr}(B)}{N}$.
This gives the determinant formula
\[
\text{det}(E-B_N)=
\bigl(1+\frac{\text{Tr}(B)}{N}\bigr)^N
\cdot \prod_{i=1}^{i=n}\, (1-\frac{\lambda_i}{\lambda}\bigr)\tag{1}
\]
The choice of $N$ implies that (1) is $\neq 0$ so 
the inverse $(E-B_N)^{-1}$ exists.
Moreover, the construction of $B_N$ gives
for any pair $(x,y)$ in ${\bf{C}}^{N+n}$:
\medskip
\[
(E-B_N)^{-1}(x,y)=
\bigl (E-B)^{-1}(x), \frac{y}{
1+\frac{1}{N}\cdot \text{Tr}(B)}\bigr)
\]


\noindent
It follows that 
\[
\text{Norm}\bigl[(E-B)^{-1})\bigr]\leq
\text{Norm}\bigl[
(E-B_N)^{-1}\bigr]\implies
\]
\[
\bigl|\text{det}(E-B_N)\bigr|\cdot \text{Norm}\bigl[(E-B)^{-1})\bigr]\leq
\bigl|\text{det}(E-B_N)\bigr|\cdot
\text{Norm}\bigl[
(E-B_N)^{-1}\bigr]\tag{2}
\]


\noindent 
Hadarmard's inequality
estimates the
hand side in (2) by:
\[
\frac{||E-B_N||^{N+n-1}}{(N+n-1)^{N+n-1)/2}}\tag{3}
\]
\medskip

\noindent
Next, the construction of $B_N$ implies that its trace is zero.
So  by the result in 6.3 we can find
an orthonormal basis $\xi_1,\ldots,\xi_{n+N}$
in ${\bf{C}}^{n+N}$ such that
\[ 
\langle B_N(\xi_k),\xi_k\rangle=0\quad\colon 1\leq k\leq n+N
\]


\noindent
Relative to this basis the matrix of $E-B_N$ 
has 1 along the diagonal and the negative of the
elements of $B_N$ elsewhere. It follows that the Hilbert-Schmidt norm
satisfies the equality:
\[
||E-B_N||^2=
N+n+||B_N||^2=N+n+||B||^2+ N^{-1}\cdot
|\text{Tr}(B)|^2\tag{4}
\]

\medskip

\noindent Hence, (1) and the inequalities from (2-3) give:
\medskip

\[
\bigl(1+\frac{\text{Tr}(B)}{N}\bigr)^N
\cdot \prod_{i=1}^{i=n}\, (1-\frac{\lambda_i}{\lambda}\bigr)\cdot
\text{Norm}\bigl[
(E-B)^{-1}\bigr]\leq
\] 

\[
\frac{\bigl(N+n+||B||^2+
N^{-1}\cdot
|\text{Tr}(B)|^2\bigr)^{(N+n-1)(2}}{
\bigl(N+n-1\bigr)^{N+n-1/2}}=
\frac{\bigl(1+\frac{||B||^2}{N+n}+
\frac{|\text{Tr}(B)|^2}{N(N+n)}\bigr)^{(N+n-1)/2}}{
(1-\frac{1}{N+n}\bigr)^{N+n-1/2}}
\]
\bigskip

\noindent
This inequality holds for
arbitrary large $N$.
Passing to the limit as $N\to\infty$  the definition of Neper's constant $e$
give

\[
\lim_{N\to\infty}\, \bigl(1+\frac{\text{Tr}(B)}{N}\bigr)^N=
e^{\text{Tr}(B)}
\]
and the reader may also verify that
the limit of the last term above is equal to
$\text{exp}\bigl[\frac{1+ ||B||^2}{2}\bigr]$ which finishes the proof of
(*) above and hence also of Theorem 6.1.

\newpage

\centerline {\bf{0.C.2 Hadamard's inequality.}}
\medskip

\noindent
The following result is due
Hadamard whose proof is left as an exercise.

\medskip

\noindent
{\bf{0.C.3  Theorem.}}
\emph{Let $A=\{a\uuu{\nu k}\}$ be some $p\times p$\vvv matrix whose elements are
complex numbers. To each $1\leq k\leq p$ we set}
\[
\ell\uuu p= \sqrt{|a\uuu{1k}|^2+\ldots+|a\uuu{p k}|^2}
\]
\emph{Then}
\[ 
\bigl |\text{det}(A)\bigr |\leq
\ell\uuu 1\cdots \ell\uuu p
\]
\newpage


\centerline{\bf{7. Hadamard's radius theorem.}}

\bigskip

\noindent
Hadamard's thesis \emph{Essais sur l'études des fonctions donnés par leur
dévelopment d Taylor} contains many interesting results.
Here we expose material from Section 2 in [ibid].
Consider a power series
\[
 f(z)=\sum\, c\uuu nz^n
\]
whose radius is a positive  number
$\rho$.
So $f$ is analytic in the open disc $\{|z|<\rho\}$
and has at least one singular point on the circle
$\{|z|=\rho\}$.
Hadamard found a condition in order that
these singularities consists of a finite set of poles only so that
$f$ extends to be meromorphic in some disc $\{|z|<\rho\uuu *\}$ with
$\rho\uuu * >\rho$. The condition is expressed via properties of
the  Hankel determinants 
$\{\mathcal D\uuu n^{(p)}\}$ from § 0.B.
For each $p\geq 1$ we set 
\[
\delta(p)=\, 
\limsup\uuu{n\to \infty}\, 
[\mathcal D\uuu n^{(p)}]^{\frac{1}{n}}
\]

\noindent
In the special case $p=0$ we have $\{\mathcal D\uuu n^{(0)}\}=\{c\uuu n\}$
and hence 
\[
\delta(0)= \frac{1}{\rho}=\limsup\uuu{n\to \infty}\, |c\uuu n|^{\frac{1}{n}}
\]
This entails that for every  $\epsilon>0$  there exists a constant $C\uuu\epsilon$ 
such that
\[ 
|c\uuu n|\leq C\cdot (\rho \vvv  \epsilon)^{\vvv n}\quad\text{
hold for every}\quad  n
\]
It follows trivially that
\[
|\mathcal D\uuu n^{(p)}|\leq (p+1) !\cdot C^{p+1}(\rho\vvv \epsilon)^{\vvv (p+1)n}
\]
Passing to limes superior where  high $n$:th roots are taken
we conclude that:
\[
\delta(p)= \limsup\uuu{n\to \infty}\, 
\bigl[\mathcal D\uuu n^{(p)}\bigr]^{\frac{1}{n}}\leq \rho^{\vvv (p+1)}\tag{1}
\]

\medskip


\noindent
Suppose   there exists some $p\geq 1$ 
where a strict inequality occurs:
\[
\delta(p)<\rho^{\vvv(p+1)}\tag{2}
\]
Let $p$ be the smallest integer $\geq 1$ where the strict
inequality holds. This gives  
a number $\rho\uuu *>\rho$ such that
\[
\delta(p)=\rho\uuu *^{\vvv 1}\cdot
\rho ^{\vvv p}\tag{3}
\]


\medskip

\noindent
{\bf{7.1 Theorem.}} \emph{With $p$ chosen  minimal as above,
it follows that $f(z)$ extends to a meromorphic function in the disc
of radius $\rho\uuu *$ where the number of poles counted with multiplicity
is at most  $p$.}
\bigskip


\noindent
The proof requires several steps. To begin with one has

\medskip

\noindent
{\bf{7.2 Lemma. }}\emph{When $p$ as above is minimal one has
the unrestricted limit formula:}
\[
\lim\uuu{n\to \infty}\, 
\bigl[\mathcal D\uuu n^{(p\vvv 1)}\bigr]^{\frac{1}{n}}=
\rho ^{\vvv p}\tag{*}
\]

\bigskip

TO BE GIVEN: Exercise power series+ Sylvesters equation.

\bigskip


\noindent
{\bf{7.3 The meromorphic extension
to $\{|z|<\rho\uuu *\}$.}} Lemma 7.2 entails that if $n$ is large
$\{\mathcal D\uuu n^{(p\vvv 1)}\}$
are  $\neq 0$.
So there exists some $n_*$ such that every  $n\geq n_*$
gives a    unique $p$\vvv vector
$(A\uuu n^{(1)},\ldots, A\uuu n^{(p)})$
which solves the inhomogeneous system
\[
\sum\uuu{k=0}^{k=p\vvv 1}
\, c\uuu {n+k+j}\cdot A\uuu n^{(p\vvv k)}
=\vvv c\uuu {n+p+j}\quad\colon\quad 0\leq j\leq p\vvv 1
\]
Or expressed in matrix notation:
\[
\begin{pmatrix}
c\uuu{n}&c\uuu{n+1}&\ldots&c\uuu{n+p-1}\\
c\uuu{n+1}&c\uuu{n+2}&\ldots&c\uuu{n+p}\\
\ldots&\ldots&\ldots&\ldots\\
c\uuu{n+p-1}&c\uuu{n+p}&\ldots&c\uuu{n+2p-2}\\
\end{pmatrix}\,
\begin{pmatrix}A_n^{(p)}\\\ldots\\\ldots\\\ldots\\
A_n^{(1)}\end{pmatrix}=-
\begin{pmatrix}c_{n+p} \\\ldots\\\ldots\\\ldots\\
c_{n+2p-1}\end{pmatrix}\tag{*}
\]

\medskip


\noindent
{\bf{7.4 Exercise.}}
Put
\[
H\uuu n=
c\uuu{n+2p}+ A\uuu n^{(1)}\cdot  c\uuu{n+2p\vvv 1}+
\ldots+ A\uuu n^{[(p)} \cdot c\uuu{n+p}
\]
Show that the evaluation of  $\mathcal D\uuu n^{(p)}$ 
via an expansion of the last column gives the equality:
\[ 
H\uuu n=
\frac{\mathcal D\uuu n^{(p)}}{\mathcal D\uuu n^{(p\vvv 1)}}\tag{i}
\]
\medskip

\noindent

\noindent
Next,  the  limit formula   (3) above Theorem 7.1 together with
Lemma 7.2  
give for every $\epsilon>0$
a constant $C\uuu\epsilon$ 
such that the following hold for all sufficiently large $n$:
\[ 
|H\uuu n|\leq C\uuu\epsilon \cdot 
\bigl(\frac{\rho+\epsilon}{\rho\uuu *\vvv \epsilon}\bigr)^n\tag{ii}
\]

\noindent
Next, 
put
\[ 
\delta\uuu n^{k}=A\uuu {n+1}^{(k)}\vvv A\uuu n^{(k)}
\quad\colon\quad 1\leq k\leq p\tag{iii}
\]

\medskip

\noindent
Solving (*) above for $n$ and $n+1$ a computation shows that
the $\delta$\vvv numbers satisfy the system
\[
\sum\uuu{k=0}^{k=p\vvv 1}
\, 
c\uuu {n+j+k+1}\cdot \delta \uuu n^{(p\vvv k)}=0
\quad\colon\quad 0\leq j\leq p\vvv 2
\]
\[
\sum\uuu{k=0}^{k=p\vvv 1}
\, 
c\uuu {n+p+k}\cdot \delta \uuu n^{(p\vvv k)}=
\vvv (c\uuu{n+2p}+ A\uuu n^{(1)}\cdot  c\uuu{n+2p\vvv 1}+
\ldots+ A\uuu n^{[(p)} \cdot c\uuu{n+p})
\tag{iv}
\]
\medskip



\noindent
The $\delta$\vvv numbers in the linear system ( iv)
are found  via Cramer's rule. 
The minors of degree $p\vvv 1$ in the Hankel matrices 
$\mathcal C\uuu {n+1}^{(p\vvv 1)}$ have elements from
the given
$c$\vvv sequence and  (7.0)  implies
that every such minor has an absolute value majorized by
\[
C\cdot (\rho\vvv \epsilon)^{\vvv (p\vvv 1)n}
\] 
where $C$ is a constant 
which is independent of $n$.
We conclude that the $\delta$\vvv numbers satisfy
\[
|\delta \uuu n^{(k)}|\leq |\mathcal D\uuu n^{(p\vvv 1)}|^{\vvv 1}\cdot 
C\cdot (\rho\vvv \epsilon)^{\vvv (p\vvv 1)n}\cdot |H\uuu n|\tag{v}
\]


\noindent
The unrestricted limit in Lemma 7.2
give  upper bounds for
 $|\mathcal D\uuu n^{(p\vvv 1)}|^{\vvv 1}$ so that  (iii) and (v) give:
 
\medskip
 
 \noindent
{\bf{7.5 Lemma}}
 \emph{To each $\epsilon>0$ there is a constant
 $C\uuu\epsilon$ such that}
 \[
|\delta \uuu n^{(k)}|\leq 
C\uuu\epsilon\cdot
 \bigl(\frac{\rho+\epsilon}{\rho\uuu *\vvv \epsilon}\bigr)^n
 \quad\colon\quad 1\leq k\leq p
\]
\medskip


\noindent
{\bf{7.6 The polynomial $Q(z)$}}.
Lemma 7.5  and (iii) entail that
the sequence $\{A\uuu n^{(k)}\,\colon\, n=1,2,\ldots\}$
converges for every $k$ and  we set
\[
A\uuu *^{(k)}=\lim_{n\to\infty}\, A\uuu n^{(k)}\,
\]
 Notice   that Lemma 7.5 after summations of geometric series gives
a constant $C_1$ such that
\[
|A\uuu *^{(k)}\vvv A\uuu n^{(k)}|\leq C_1\cdot 
\bigl(\frac{\rho+\epsilon}{\rho\uuu *\vvv \epsilon}\bigr)^n\tag{7.6.i}
\]
hold for every $1\leq k\leq p$ and every $n$.

\noindent
Now we consider the sequence
\[
b\uuu n=
 c\uuu{n+p}+ A\uuu *^{(1)}\cdot c\uuu {n+p\vvv 1}+
\ldots A\uuu *^{(p)}\cdot c\uuu n\tag{7.6.ii}
 \]
Equation (*) applied to   $j=0$  gives
\[
b\uuu n= 
(A\uuu *^{(1)}\vvv A\uuu n ^{(1)})
\cdot c\uuu {n+p\vvv 1}+
\ldots +(A\uuu *^{(p)}\vvv A\uuu n^{(p)})\cdot c\uuu n\tag{7.6.iii}
\]

\medskip

\noindent
Next, we have already seen that $|c\uuu n|\leq C\cdot(\rho\vvv \epsilon)^{\vvv n}$
hold for some constant $C$ which
together with (7.6.i)  gives:
\medskip

\noindent
{\bf{7.7 Lemma.}}
\emph{For every $\epsilon>0$ there exists a constant $C$ such that}
\[
|b\uuu n|\leq C\cdot \bigl(\frac{1+\epsilon}{\rho\uuu *}\bigr)^n
\]
\medskip

\noindent
Finally, consider the polynomial
\[
Q(z)=  1+ A\uuu *^{(1)}\cdot z+
\ldots A\uuu *^{(p)}\cdot z^p
\]

\noindent
Set $g(z)= Q(z)f(z)$ which has a power series
$\sum\, d_\nu z^\nu$
where 
\[
b_n=
c_n\cdot   A_*^{(p)}+\ldots
c_{n+p-1}A_*^{(1)} +c_{n+p}=d_{n+p}
\]
\medskip


\noindent
Above $p$ is fixed so Lemma 7.7 and the trivial spectral radius formula 
show that
$g(z)$ is analytic in the disc $|z|<\rho_*$. This
proves that $f$ extends and the poles are contained in
the zeros of the polynomial $Q$ which occur  in the annulus
$\rho\leq |z|<\rho_*$.







 
 














\newpage


\centerline{\bf{8. On positive definite quadratic forms}}
\bigskip


\noindent
In many situations one is asking 
when  a given a bi\vvv linear form is positive definite.
We  prove a result 
which has a geometric interpretation.
Let $m\geq 2$ and denote $m$\vvv vectors in
${\bf{R}}^m$
with  capital letters, i.e. $X=(x\uuu 1,\ldots,x\uuu m)$.
Let $N\geq 2 $ be some positive integer 
and $X\uuu 1,\ldots,X\uuu N$
an $N$\vvv tuple of real $m$\vvv vectors.
To each pair $j\neq k$
we set
\[
b\uuu{ij}= ||X\uuu j||+X\uuu k||\vvv 
||X\uuu j\vvv X\uuu k||
\]
where $||\cdot ||$ is 
the usual euclidian length in ${\bf{R}}^m$.
We get the symmetric $N\times N$\vvv matrix with elements
$\{b\uuu{ij}\}$ and the associated
quadratic form

\[ 
H(\xi\uuu 1,\ldots,\xi\uuu N)=
\sum\sum\, b\uuu{ij}\cdot \xi\uuu i\cdot \xi\uuu j
\]
\medskip

\noindent
{\bf{8.1 Theorem.}}
\emph{If the
$X$\vvv vectors are all different then
$H$ is positive definite.}
\medskip

\noindent
The proof relies upon a useful formula to express
the length of a vector in ${\bf{R}}^m$.
\medskip

\noindent
{\bf{8.2 Lemma }}There exists a constant
$C\uuu m$
such that
for every $m$\vvv vector $X$  one has
\[
||X||=C\uuu m\cdot  
\int\uuu{{\bf{R}}^m}\,
\frac{1\vvv \cos\,\langle X,Y\rangle }{||Y||^{m+1}}\cdot dY\tag{*}
\]
\medskip

\noindent
\emph{Proof.}
We use polar
coordinates and denote by $dA$  the area measure on the unit sphere
$S^{m\vvv 1}$ and
$\omega=(\omega\uuu 1,\ldots,\omega\uuu m)$
denote points on the unit sphere
$S^{m\vvv 1}$.
Notice that the integrals 
\[
\int\uuu{S^{m\vvv 1}}\, 
(1\vvv \cos\,\langle X,\omega\rangle)\cdot dA
\]
only depend upon $||X||$. Hence it suffices to prove Lemma 8.2 when
$X=(R,\ldots,0)$ where $R=||X||$ and here  the integral in (*) becomes:
\[
\int\uuu 0^\infty\, \bigl[\,
\int\uuu{S^{m\vvv 2}}\, (1\vvv\cos Rr\omega\uuu 1)\cdot dA\uuu{m\vvv 1}
\,\bigr]\cdot \frac{dr}{r^2}
\] 
where $dA\uuu{m\vvv 1}$ is the area measure on $S^{m\vvv 2}$.
Set 
\[
B(R,\omega\uuu 1)=
\int\uuu 0^\infty\, (1\vvv\cos Rr\omega\uuu 1)\cdot \frac{dr}{r^2}
\]
for each   $\vvv 1<\omega\uuu 1<1$. The variable substitution $r\to s/R$
gives
\[
B(R,\omega\uuu 1)=
R\cdot 
\int\uuu 0^\infty\, \frac{1\vvv\cos s\omega\uuu 1}{s^2}\cdot ds
=R\cdot B\uuu *(\omega\uuu 1)
\]
With these notations the integral in (*) becomes
\[ 
R\cdot\int\uuu{S^{m\vvv 2}}\,B\uuu *(\omega\uuu 1)\cdot dA\uuu{m\vvv 2}\tag{1}
\]
Hence Lemma 8.2 follows where $C\uuu m^{\vvv 1}$ is equal to (1) above.



\medskip

\noindent
\emph{Proof of Theorem 8.1.}
For a given pair $i,j$ the addition formula for the cosine\vvv function gives:
\[
1\vvv\cos\, \langle X\uuu i,Y\rangle+
1\vvv\cos\, \langle X\uuu j,Y\rangle+
\cos\, \langle (X\uuu i\vvv X\uuu j ),Y\rangle=
\]
\[
(1\vvv\cos\, \langle X\uuu i,Y\rangle)\cdot 
(1\vvv\cos\, \langle X\uuu i,Y\rangle)+\sin\,\langle X\uuu i,Y\rangle\cdot
\sin\,\langle X\uuu j,Y\rangle\tag{1}
\]
It follows that the matrix element $b\uuu{ij}$
is given by

\[
C\uuu m\cdot \int\uuu{{\bf{R}}^m}\,
\frac{(1\vvv\cos\, \langle X\uuu i,Y\rangle)\cdot 
(1\vvv\cos\, \langle X\uuu j,Y\rangle)+\sin\,\langle X\uuu i,Y\rangle\cdot
\sin\,\langle X\uuu j,Y\rangle}{||Y||^{m+1}}\cdot dY
\]
From this we see that

\[
H(\xi)= C\uuu m\cdot
\int\uuu{{\bf{R}}^m}\,\bigl([\sum\, (\xi\uuu k\cdot(1\vvv \cos\langle X\uuu k,Y\rangle) \,]^2
+[\sum\, (\xi\uuu k\cdot(\sin\langle X\uuu k,Y\rangle) \,]^2\bigr) \cdot 
\frac{dY}{||Y||^{m+1}}
\]
This shows that $H$ is positive definite as requested.

\bigskip

\noindent
{\bf{8.3 Exercise.}}
Prove more generally that for every $1<p<2$
a similar result as above holds when the elements of the matrix are:
\[
b\uuu{ij}= ||X\uuu j||^p+X\uuu k||^p\vvv 
||X\uuu j\vvv X\uuu k||^p
\]
\emph{Hint.} Employ a similar formula as in (*) where a
new constant $C\uuu{p,m}$ appears and $||Y||^{m+1}$
is replaced by $||Y||^{m+p}$.
\bigskip

\medskip

\noindent
{\bf{8.4  A class of Hermitian matrices.}}
\emph{Let $z\uuu 1,\ldots,z\uuu N$ be an $n$\vvv tuple of distinct and non\vvv
zero complex numbers. Set}
\[
b\uuu {ij}= \{\frac{z\uuu i}{z\uuu j}\}
\]
\emph{Then the matrix $B=\{b\uuu{ij}\}$ is Hermitian and positive definite.}
\medskip

\noindent
Again the proof is left as an exercise to the reader.
\medskip

\noindent
{\bf{8.5  Remark.}}
Theorem 8.1 has several applications. For example, Beurling used it to
prove the existence of certain spectral measures which arise in
ergodic processes.
Another application  from [Beurling: Notes  Uppsala
1935] goes as follows: Let $f$ and $g$
be a pair of continuous and absolutely integrable functions on
the real line. Define the function on the real  $t$\vvv line by

\[
\phi(t)=\int\uuu{\vvv \infty}^\infty\, 
[f(t+s)\vvv g(s)|\cdot ds
\]

\medskip

\noindent
{\bf{8.6 Theorem.}}
\emph{There exists a measure $\mu$ 
on the $\xi$\vvv line of total variation 
$\leq 2\sqrt{||f||\uuu 1\cdot ||g||\uuu1}$
such that}

\[ 
\phi(t)=||f||\uuu1+||g||\uuu 1+\int\uuu{\vvv \infty}^\infty\,
e^{i\xi t}\cdot d\mu(\xi)
\]
\bigskip


\noindent
The reader is invited to try to prove this theorem 
using Theorem 8.1 and the observation that
the a similar result as above holds for $L^2$\vvv functions
$f$ and $g$, i.e. this time  we set

\[
\psi(t)=\int\uuu{\vvv \infty}^\infty\, 
[f(t+s)\vvv g(s)|^2\cdot ds
\]
and one shows that there exists a measure $\gamma$ whose total variation is
$\leq 2\sqrt{||f||\uuu 2\cdot ||g||\uuu2}$ and
\[
\psi(t)=||f||\uuu2+||g||\uuu 2+\int\uuu{\vvv \infty}^\infty\,
e^{i\xi t}\cdot d\gamma(\xi)
\]
\bigskip






\newpage


\centerline{\bf{9. The Davies-Simon inequality.}}
\bigskip

\noindent
{\bf{Introduction.}}
Every
$n\times n$-matrix $A$ can be regared as a ${\bf{C}}$-linear operator on
the hermitian complex $n$-space which yields
the  operator norm $\text{Norm}(A)$.
Just  as in Theorem 6.1 we shall exhibit an inequality for the
operator norm but this time  another feature appears.
Namely, Theorem 9.1  yields
an upper bound
expressed by the euclidian  distance from $\lambda$ to $\sigma(A)$
which is better than the product which appears in the left hand side of
Theorem 6.1. On the other hand, the inequality below is restricted
to special $\lambda$-values whose absolute values are  larger than
the operator norm of $A$. Hence 
the results in 6.1 and 9.1 supplement each other.

\medskip



\noindent
{\bf{9.1 Theorem.}}
\emph{For every $n\times n$-matrix $A$
whose operator norm is $\leq 1$
the inequality below holds for every
$0\leq\theta\leq 2\pi$ outside $\sigma(A)$ }
\[
 \text{Norm}(R_A(e^{i\theta}))\leq \cot\,\frac{\pi}{4n}\cdot
 \text{dist}(e^{i\theta},\sigma(A))^{-1}
 \]



\noindent
\emph{Proof.} Schur's result in Theorem
4.0.7 reduces the proof to the case when
$A$ is upper triangular and
replacing $A$ by $e^{i\theta}A$ we may take $\theta=0$.
Set
$B= (E-A)^{-1}$ and let $B^*$ be the adjoint  operator.
The equations  $B-BA=E$ and $A^*B^*-B^*=-E$ give
\[
B(E-AA^*)B^*=BB^*-(B-E)A^*B^*
=BB^*-(B-E)(B^*-E)=B+B^*-E
\]
Set $C=B+B^*-E$ and notice that the diagonal elements
\[
c_{kk}=\frac{1}{1-\lambda_k i}+\frac{1}{1-\bar \lambda_k i}-1
= \frac{1-|\lambda_k|^2}{|1-\lambda_k|^2}\tag{1}
\]
where
$\{\lambda_k\}$ are the diagonal elements of $A$
which give points in $\sigma(A)$.
Now we shall we prove the inequality:
\[ 
|b_{ij}|^2\leq\frac{(1-|\lambda_i|^2)\cdot (1-|\lambda_j|^2)}
{(1-\lambda_i|^2\cdot |1-\lambda_j|^2}\tag{2}
\]
To get (2) we consider a vector $x$ and obtain
\[ 
\langle Cx,x\rangle=\langle B(E-AA^*)B^*x,x\rangle=
 \langle (E-AA^*)B^*x,B^*x\rangle\geq 0\tag{3}
 \] 
 where the last equality holds 
 since
 the self-adjoint
 matrix $E-AA^*$ is non-negative
 because $A$ by assumption has operator norm
 $\leq 1$.
 From (3) and the Cauchy-Schwarz inequality applied to the symmetric matrix
 we get
 \[
 |c_{ij}|^2\leq |c_{ii}|\cdot |c_{jj}|\quad\colon\quad i<j\tag{4}
\] 
for each pair $i\neq j$.
Since $c_{ij}= b_{ij}$ when $i<j$ we get (2).
Next, put $\delta=\text{dist}(1,\sigma(A))$ which means that
$|1-\lambda_i|\geq\delta$ for every $i$. From this it is clear that
(2) and the triangle inequality give
\[
 |b_{ij}|^2\leq \frac{4}{\delta^2}\quad\colon\quad i<j\tag{5}
\]
At the same time the diagonal elements satisfy:
\[
|b_{ii}|^2= \frac{1}{|1-\lambda_i|^2}\leq \frac{1}{\delta^2}\tag{6}
\]
Let $T$ be the upper triangular matrix where
$t_{ij}=2$ when $i<j$ and $t_{ii}=1$ for each $i$.
Then the elements in 
$\frac{1}{\delta}\cdot T$  majorize the absolute values of the
$B$-matrix.
The observation from § xx implies that 
\[ 
\text{Norm}(B)\leq \frac{1}{\delta}\cdot\text{Norm}(T)
\]
Now Theorem 9.1 follows from
the formula in § xx for the operator norm of $T$.


\newpage



\centerline{\bf{10. von Neumann's inequality.}}

\bigskip

\noindent
Let $A$ be an $n\times n$-matrix with operator norm
$\leq 1$, i.e., $A$ is a contraction.
For each
polynomisl $p(z)= a_0+a_1z+\ldots+a_Nz^N$
with complex coefficients we get the matrix $p(A)$.

\medskip

\noindent
{\bf{10.1 Theorem.}} \emph{One has the inequality}
\[
\text{Norm}(p(A))\leq \max_{z\in D}|, |p(z)|
\]
\medskip

\noindent
To prove this we first
establish a general  inequality which goes back to Schur.
Let  $g(z)$ be an analytic function
in the unit disc which extends continuously to the boundary
and  $A$ is  some  $n\times n$-matrix whose spectrum is 
contained in the open unit disc.
If $g(z)$ has the series expansion $\sum\, c_kz^k$
we know from § xx that the matrix-valued series
$\sum\, c_kA^k$ converges and gives a matrix $g(A)$.
There exists also the exponential matrix
\[
B= e^{g(A)}
\]
If $g^*(z)=\sum\, \overline{c_k}z^k$ then
\[
B^*=e^{g^*(A^*)}
\]
Put
\[
C=e^{g^*(A^*+g(A)}=B^*B
\]
The result in § xx gives
\medskip

\noindent
{\bf{10.2 The Schur-Weierstrass inequality.}}
\emph{For each pair $A$ and $g$ as above one has}
\[
\text{Norm}(e^{g(A)})=
\max_{\lambda\in\sigma(A)}\, e^{\mathfrak{Re}(g(\lambda))}\tag{*}
\]
\medskip

\noindent
Notice that (*) holds under the sole assumption that
$\sigma(A)\subset D$, i.e.  $A$ need not be a contraction.

\medskip

\noindent
{\bf{10.3.  Another norm inequality.}}
Let $\alpha$ be a point in the open unit disc and suppose that
$A$ is a contraction. It follows that
\[
(1-|\alpha|^2)\cdot \langle (E-A^*A)(y),y\rangle\geq 0
\]
hold for every vector $y$. Expanding this inequality we get
\[
||\alpha E-A)(y)||^2\leq ||y-\bar\alpha\cdot A(y)||^2\tag{i}
\]
Next, there
exists the matrix
\[
g_\alpha(A)= (\alpha E-A)\cdot (E-\bar\alpha A)^{-1}
\]
Consider a vector $x$ of unit norm and set
\[
y=(E-\bar\alpha A)^{-1}(x)
\]
Then
\[ 
||g_\alpha(A)(x)||^2=||\alpha E-A)(y)||^2\leq ||y-\bar\alpha\cdot A(y)||^2\tag{ii}
\]
where the last inequality used (i).
Now
$y-\bar\alpha\cdot A(y)=x$ and hence (ii) gives
\[
||g_\alpha(A)(x)||^2||\leq||x||^2
\]
Since the vector $x$ was arbitrary we conclude that
\[ 
\text{Norm}(g_\alpha(A))\leq 1\tag{3.1}
\]
\bigskip

\noindent
\emph{Proof of Theorem 1.}}
By scaling we can assume that the maximum norm
$|p|_D=1$.
We construct the Blaschke product taken over the zeros of $p$ in the open unit disc
and get a factorisation
\[
p(z)=B(z)\cdot e^{g(z)}
\]
where the zero-free analytic function $e^{g(z)}$ has maximum norm one which gives
$\mathfrak{Re}(g)(z)\leq 0$ for all $z\in D$.
Now
\[
p(A)= B(A)\cdot e^{g(A)}
\]
Here $B(A)$ is the product of operators of the form $-g_\alpha(A)$
where $\alpha$ are zeros of $p$ in $D$. By (3.1)
each of these operators have norm $\leq 1$ and (*) in (2) entails that
the same holds for $e^{g(A)}$.
So $p(A)$ is the product of operators of norm $\leq 1$ and
Theorem 1 follows.
\medskip


\noindent
{\bf{Remark.}}
The proof given by von Neumann in [1951]
is carried out for operators on Hobert spaces and we remark only that
the present version for matrices easilly extends to contractions on Hilbert spaces.
Abve we empoyed Blaschke's factorisation which was used by Schur, while 
the proof by von Neumann in [1951] avoid 
Blasche products
via  certain constructions of unitary operators 
arising from   contractions. The interested
reader should consult the text-book
[Davies] for this proof as well as further extensions 
of Theorem 10.1 which
appear in [ibid: Chapter 10].


































\newpage



\centerline{\bf{11. An application to integral equations.}}



\bigskip


\noindent
Let $k(x,y)$ be a complex-valued continuous function on the unit square
$\{0\leq x,y\leq 1\}$. We do not assume that $k$ is symmetric,  i.e,
in general $k(x,y)\neq k(y,x)$.
 Let $f(x)$ be another  continuousfunction  on $[0,1]$.
Assume that the maximum norms of $k$ and $f$ both are $<1$.
By induction over $n$ starting with $f\uuu 0(x)= f(x)$
we get a sequence $\{f\uuu n\}$ where
\[
f\uuu n(x)=\int\uuu 0^1\, k(x,y)\cdot f\uuu{n\vvv 1}(y)\cdot dy
\quad \colon\quad n\geq 1
\]
The hypothesis entails that each $f\uuu n$ has maximum norm
$<1$ and hence there exists  a power series:
\[
u\uuu\lambda(x)= \sum\uuu{n=0}^\infty\, f\uuu n(x)\cdot \lambda^n
\] 
which converges for every  $|\lambda|<1$ and yields a continuous function
$u\uuu\lambda(x)$  on $[0,1]$.

\medskip

\noindent
{\bf{11.1 Theorem.}}
\emph{The function $\lambda\mapsto u\uuu\lambda(x)$ with values in the Banach space
$B=C^0[0,1]$ extends to a meromorphic $B$\vvv valued 
function in the whole
$\lambda$\vvv plane.}
\bigskip

\noindent
To prove this we introduce the recursive Hankel determinants for
each $0\leq x\leq 1$:
\[
\mathcal D_n^{(p)}(x)=
\det
\begin{pmatrix}
f_{n+1}(x)
&f_{n+2}(x)
&\ldots&\ldots
& f_{n+p}(x)\\
f_{n+2}(x)
&f_{n+3}(x)
&\ldots&\ldots
& f_{n+p+1}(x)\\

\ldots &\ldots &\ldots&\ldots \\
\ldots &\ldots&\ldots&\ldots\\
f_{n+p}(x)
&f_{n+p+1}(x)
&\ldots&\ldots
& f_{n+2p-1}(x)\\
\end{pmatrix}
\]
\medskip


\noindent
{\bf{Proposition 11.2}} \emph{For every $p\geq 2$ and $0\leq x\leq 1$ one has
the inequality}

\[
\bigl |\,  \mathcal D\uuu n^{(p)}(x))\,\bigr|\leq 
(p\, !)^{\vvv n}\cdot \bigl( p^{\frac{p}{2}}) ^n\cdot \frac{p^p}{p\,!}
\]
\medskip

\noindent
{\bf{11.3 Conclusion.}}
The inequality above  entails that
\[ 
\limsup\uuu{n\to \infty}\, \bigl| \mathcal D\uuu n^{(p)}(x))\,\bigr |^{1/n}
\leq 
\frac{p^{p/2}}{p\,!}
\]
Next, Stirling's formula gives:
\[
\lim\uuu{p\to \infty}\bigl[\frac{p^{1/2}}{p\,!}\,\bigr]^{\vvv 1/p}=0
\]
Hence  Hadamard's theorem gives
Theorem 11.1



\bigskip

\centerline{\emph{Proof of Proposition 11.2}}
\bigskip


\noindent
The proof requires several steps. 
First, 
we get the sequence $\{k^{(m)}(x)\}$
which starts with $k=k^{(1)}$ and:
\[
k^{(m)}(x)= \int\uuu 0^1\, k^{(m\vvv 1)}(x,s)\ddot k(s)\cdot ds
\quad\colon\quad m\geq 2
\]
It is easily seen that
\[
f\uuu{n+m}(x)= \int\uuu 0^1\, k^{m)}(x,s)\cdot f\uuu n(s)\cdot ds
\]
hold for all pairs $m\geq 1$ and $n\geq 0$.
\medskip

\noindent
{\bf{11.4 Determinant formulas.}}
Let $\phi\uuu 1(x),\ldots,\phi\uuu p(x)$ and
 $\psi\uuu x),\ldots,\psi\uuu p(x)$
be a pair of $p$\vvv tuples of continuous functions on
$[0,1]$.
For each point $(x\uuu 1,\ldots,x\uuu p)$ in
$[0,1]^p$ we put

\[ 
D_{\phi_1,\ldots,\phi_p}(x\uuu 1,\ldots,x\uuu p)
=\text{det}
\begin{pmatrix}
\phi\uuu 1(x\uuu 1)&\cdots&\phi\uuu 1(x\uuu p)\\
\cdots &\cdots&\cdots \\
\cdots &\cdots&\cdots\\
\phi\uuu p(x\uuu 1)&\cdots &\phi\uuu 1(x\uuu p)\\
\end{pmatrix}\quad\colon\quad
\]
In the same way we define $D_{\psi_1,\ldots,\psi_p}(x\uuu 1,\ldots,x\uuu p)$.
Next, define the  $p\times p$\vvv matrix with elements

\[
a\uuu{jk}= \int\uuu 0^1\, \phi\uuu j(s)\cdot \psi\uuu k(s)\, ds
\]

\medskip

\noindent {\bf{11.5 Lemma.}} \emph{One has the equality}
\[
\text{det}(a\uuu{jk})=
\frac{1}{p\,!}\int\uuu {[0,1] ^p}\,
\Phi(s\uuu 1,\ldots,s\uuu p)\cdot 
\Psi(s\uuu 1,\ldots,s\uuu p)\cdot ds\uuu 1\cdots ds\uuu p
\]

\medskip

\noindent
{\bf{11.6 Exercise.}} Prove this result using standard formulas for
determinants.

\medskip

\noindent
Next, for each $0\leq x\leq 1$ and every pair $n,p$ of
positive integers we consider the $p\times p$-matrix

\[
\begin{pmatrix}
\int_0^1\, k(x,s)f_n(s)\,ds
&\int_0^1\, k^{(2)}(x,s)f_n(s)\,ds
&\ldots&\ldots
&\int_0^1\, k^{(p)}(x,s)f_n(s)\\

\int_0^1\, k^{(2)}(x,s)f_{n+1}(s)\,ds
&\int_0^1\, k^{(2)}(x,s)f_{n+1}(s)\,ds
&\ldots&\ldots
&\int_0^1\, k^{(2)}(x,s)f_{n+1}(s)\\
\ldots &\ldots &\ldots&\ldots \\
\ldots &\ldots&\ldots&\ldots\\
\int_0^1\, k^{(p)}(x,s)f_{n+p-1}(s)\,ds
&\int_0^1\, k^{(p)}(x,s)f_{n+p-1}(s)\,ds
&\ldots&\ldots
&\int_0^1\, k^{(p)}(x,s)f_{n+p-1}(s)\\
\end{pmatrix}\quad\colon\quad
\]
\medskip

\noindent 
We also get the two determinant functions
\[
\mathcal K^{(p)}(x,s_1,\ldots,s_p)=
\det
\begin{pmatrix}
k^{(1)}(x,s_1)
&k^{(1)}(x,s_2)
&\ldots&\ldots
& k^{(1)}(x,s_p)\\
k^{(2)}(x,s_1)
&k^{(2)}(x,s_2)
&\ldots&\ldots
& k^{(2)}(x,s_p)\\
\ldots &\ldots &\ldots&\ldots \\
\ldots &\ldots&\ldots&\ldots\\
k^{(p)}(x,s_1)
&k^{(p)}(x,s_2)
&\ldots&\ldots
& k^{(p)}(x,s_p)\\
\end{pmatrix}
\]

\bigskip

\[
\mathcal F_n^{(p)}(s_1,\ldots,s_p)=
\det
\begin{pmatrix}
f_n(s_1)
&f_n(s_2)
&\ldots&\ldots
& f_n(s_p)\\
f_{n+1}(s_1)
&f_{n+1}(s_2)
&\ldots&\ldots
& f_{n+1}(s_p)\\

\ldots &\ldots &\ldots&\ldots \\
\ldots &\ldots&\ldots&\ldots\\
f_{n+p-1}(s_1)
&f_{n+p-1}(s_2)
&\ldots&\ldots
& f_{n+p-1}(s_p)\\
\end{pmatrix}
\]

\medskip


\noindent{\bf{11.7 Lemma}}.
Let 
$\mathcal D_n^{(p)}(x)$
denote the determinant of the matrix (x). Then
one has the equation





\[
\mathcal D_n^{(p)}(x)=
\frac{1}{p !}\cdot\int_{[0,1]^p}\,
\mathcal K^{(p)}(x,s_1\ldots,s_p)\cdot \mathcal F^{(p)}_n(s_1,\ldots,s_p)
\, ds_1\cdots ds_p
\]


PROOF: Apply previous lemma ....


\bigskip

\noindent
Next, using (xx) we have the equality





\bigskip

\noindent
{\bf{Exercise.}}
Use the formulas above to conclude that
the requested intequality in Proposition 11.2 holds.




\end {document}






 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
